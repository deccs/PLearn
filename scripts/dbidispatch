#!/usr/bin/env python
import sys,os,re,time,datetime
from plearn.utilities.toolkit import search_file

ScriptName="launchdbi.py"
ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.

%(ShortHelp)s

common options:
  The -h, --help print the long help(this)
  The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
  The --dbilog (--nodbilog) tells dbi to generate (or not) an additional log
  The '--test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
  The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.

dbidispatch --test --file=tests


bqtools, cluster, local and ssh options:
  --nb_proc=nb_process, specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly.
    --local=X is the same as --local --nb_proc=X
    --cluster=X is the same as --cluster --nb_proc=X
    --bqtools=X is the same as --bqtools --nb_proc=X
    --ssh=X is the same as --ssh --nb_proc=X

bqtools and cluster option:
  The '--duree' option specifies the maximum duration of the jobs. The syntax depends on where the job is dispatched. For the cluster syntax, see 'cluster --help'. For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 13 minutes and 15 seconds.

bqtools only options:
  The '--micro[=nb_batch]' option can be used with BqTools when launching many jobs that
  have a very short duration. This may prevent some queue crashes. The nb_batch value
  is the number of experience to group together in a batch.(default 20)

  The '--long' option must be used with BqTools to launch jobs whose duration
  is more than 5 days. The maximum duration of a job will be either the
  BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
  set, and 1200:00:00 (50 days) otherwise.
  Since long jobs are launched on a different queue with few nodes, please make
  sure you are not using too many nodes at once.
  If this option is not set, the maximum duration of each job will be 120 hours
  (5 days).

cluster and condor options:
  The '--3264', '--32' or '--64' specify which type of cpu the node must have to execute the commands.

cluster only options:
  The '--wait' is transfered to cluster. This must be enabled if there is not nb_process available nodes. Otherwise when there are no nodes available, the launch of that command fails.
  The '--nowait' means the --wait option is not given to the cluster command, as in the default.
  The '--mem=X' speficify the number of meg the program need to execute.
  The '--os=X' speficify the os of the server: fc4 or fc7. Default: fc4
  
condor only options:
  The '--req=\"CONDOR_REQUIREMENT\"' option makes dbidispatch send additional option to DBI that will be used to generate additional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writen in the following way:

  dbidispatch \"--req=Machine==\\\\\\\"computer.example.com\\\\\\\"\"
     or
  dbidispatch '--req=Machine==\\\"computer.example.com\\\"' 


where <command-template> is interpreted as follows: the first argument
is the <command> above, and the rest are interpreted as <arguments>.
The arguments may contain segments of the form {{a,b,c,d}}, which trigger
parallel dispatch: a separate 'cluster --execute' command is issued for
the rest of the command template, the first time with value a, the second
time with value b, etc.  For example, the command (NOTE: THERE MUST NOT
BE ANY SPACES WITHIN THE 'numhidden={{5,10,25}}' part and the quotes are
important to avoid shell misinterpretation) :

  dbidispatch aplearn myscript.plearn 'numhidden={{5,10,25}}'

is equivalent to launching three jobs in parallel on the cluster:

  aplearn myscript.plearn numhidden=5
  aplearn myscript.plearn numhidden=10
  aplearn myscript.plearn numhidden=25

If several arguments contain {{ }} forms, all combinations of arguments
are taken, and the jobs are all launched in parallel.  For instance

  dbidispatch aplearn myscript.plearn 'numhidden={{10,25}}' 'wd={{0.01,0.001}}'

is equivalent to:

  aplearn myscript.plearn numhidden=10 wd=0.01
  aplearn myscript.plearn numhidden=10 wd=0.001
  aplearn myscript.plearn numhidden=25 wd=0.01
  aplearn myscript.plearn numhidden=25 wd=0.001

In the file of the parameter --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
"""%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}

if len(sys.argv) == 1:
    print ShortHelp
    sys.exit(1)
FILE = ""
dbi_param={}


PATH=os.getenv('PATH')
if search_file('condor_submit',PATH):
    launch_cmd = 'Condor'
elif search_file('bqsubmit',PATH):
    launch_cmd = 'bqtools'
elif search_file('cluster',PATH):
    launch_cmd = 'Cluster'
else:
    launch_cmd = 'Local'

command_argv = sys.argv[1:]
for argv in sys.argv[1:]:

    if argv == "--help" or argv == "-h":
        print LongHelp
        sys.exit(0)
    elif argv == "--nodbilog":
        dbi_param["dolog"]=False
    elif argv == "--dbilog":
        dbi_param["dolog"]=True
    elif argv.startswith("--bqtools"):
        launch_cmd = "bqtools"
        if len(argv)>9:
            assert(argv[9]=="=")
            dbi_param["nb_proc"]=argv[10:]
    elif argv.startswith("--cluster"):
        launch_cmd = "Cluster"
        if len(argv)>9:
            assert(argv[9]=="=")
            dbi_param["nb_proc"]=argv[10:]
    elif argv == "--condor":
        launch_cmd = "Condor"
    elif argv.startswith("--duree="):
        dbi_param["duree"]=argv[8:]
    elif argv.startswith("--mem="):
        dbi_param["mem"]=argv[6:]
    elif argv.startswith("--os="):
        dbi_param["os"]=argv[5:]
    elif argv.startswith("--local"):
        launch_cmd = "Local"
        if len(argv)>7:
            assert(argv[7]=="=")
            dbi_param["nb_proc"]=argv[8:]
    elif argv.startswith("--nb_proc="):
        dbi_param["nb_proc"]=argv[10:]
    elif argv.startswith("--ssh"):
        launch_cmd = "Ssh"
        if len(argv)>5:
            assert(argv[5]=="=")
            dbi_param["nb_proc"]=argv[6:]
        dbi_param["file_redirect_stdout"]=False
        dbi_param["file_redirect_stderr"]=False
    elif argv == "--test":
        dbi_param["test"]=True
    elif argv.startswith("--file="):
        FILE = argv[7:]
    elif argv == "--32"  or argv == "--64" or argv == "--3264":
        dbi_param["arch"]=argv[2:]
    elif argv == "--wait":
        dbi_param["cluster_wait"]=True
    elif argv == "--nowait":
        dbi_param["cluster_wait"]=False
    elif argv[0:6] == "--req=":
        dbi_param["requirements"]="\"%s\""%argv[6:]
    elif argv == "--no_clean_up":
        dbi_param["clean_up"]=False
    elif argv == "long":
        dbi_param["long"] = True
    elif argv.startswith("--micro"):
        dbi_param["micro"]=20
        if len(argv)>7:
            assert(argv[7]=="=")
            dbi_param["micro"]=argv[8:]
    elif argv[0:1] == '-':
	print "Unknow parameter (%s)",argv
	print ShortHelp
        sys.exit(1)
    else:
        break
    command_argv.remove(argv)

print "\n\nThe jobs will be launched on the system:", launch_cmd,"\n\n"

if len(command_argv) == 0 and FILE == "":
    print ShortHelp
    sys.exit(1)


def generate_combination(repl):
    if repl == []:
        return []
    else:
        res = []
        x = repl[0]
        res1 = generate_combination(repl[1:])
        for y in x:
            if res1 == []:
                res.append(y)
            else:
                for r in res1:
                    res.append(y+" "+r)
        return res

def generate_commands(sp):
### Find replacement lists in the arguments
    repl = []
    for arg in sp:
        p = re.compile('\{\{\S*\}\}')
        reg = p.search(arg)
        if reg:
#            print "reg:",reg.group()[2:-2]
            curargs = reg.group()[2:-2].split(",")# if arg =~ /{{(.*)}}/
#            print "curargs:",curargs
            newcurargs = []
            for curarg in curargs:
                new = p.sub(curarg,arg)
#                print "new:",new
                newcurargs.append(new)
            repl.append(newcurargs)
        else:
            repl.append([arg])
#    print "repl: ",repl
    argscombination = generate_combination(repl)
    return argscombination

#generate the command
if FILE != "":
    FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
    commands=[]
    for line in FD.readlines():
        line = line.rstrip()
	sp = line.split(" ")
        commands+=generate_commands(sp)
    FD.close
else:
    commands=generate_commands(command_argv)

if FILE == "":    
    t = [x for x in sys.argv[1:] if not x[:2]=="--"]
    t[0]=os.path.split(t[0])[1]
    tmp="_".join(t)
    tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
    ### We need to remove the symbols "," as this cause trouble with bqtools
    tmp=re.sub( ',', '-', tmp )
    tmp=tmp[:200]
    tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
    dbi_param["log_dir"]=os.path.join("LOGS",tmp)
    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
else:
    dbi_param["log_dir"]=os.path.join("LOGS",FILE)
    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')


SCRIPT=open(os.getenv("HOME")+"/.dbidispatch.launched",'a');
SCRIPT.write("["+time.ctime()+"] "+str(sys.argv)+"\n")
SCRIPT.close()

if "test" in dbi_param:
    print "We generated %s command in the file"% len(commands)
    print "The script %s was not launched"% ScriptName
    SCRIPT=open(ScriptName,'w');
    SCRIPT.write(
"""#! /usr/bin/env python
#%s
from plearn.parallel.dbi import DBI
jobs = DBI([
"""% " ".join(sys.argv))
    for arg in commands:
        cmdstr = "".join(arg);
        SCRIPT.write("   '%s',\n"%cmdstr)
    SCRIPT.write("   ],'%s'"%(launch_cmd))
    for key in dbi_param.keys():
        if isinstance(dbi_param[key],str):
            SCRIPT.write(","+str(key)+"='"+str(dbi_param[key])+"'")
        else:
            SCRIPT.write(","+str(key)+"="+str(dbi_param[key]))
    SCRIPT.write(
""")
jobs.run()
jobs.wait()
# There is %d command in the script"""%(len(commands)))
    if "test" in dbi_param:
        print "[DBI dispatch] In test mode, we do not make dbi errase temp file"
    else:
        SCRIPT.write("jobs.clean()") 
    SCRIPT.close()
    os.system("chmod +x %s"%(ScriptName));

else:
    print "We generate the DBI object with %s command"%(len(commands))
    from plearn.parallel.dbi import *
    print time.ctime()
    t1=time.time()
    jobs = DBI(commands,launch_cmd,**dbi_param)
    t2=time.time()
    print "it took %f s to create the DBI objects"%(t2-t1)
    jobs.run()
    t3=time.time()
    jobs.wait()
    if "test" in dbi_param:
        print "[DBI dispatch] In test mode, we do not make dbi errase temp file"
    else:
        jobs.clean()
    print "it took %f s to launch all the commands"%(t3-t2)

