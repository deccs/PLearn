#!/usr/bin/env python
import sys,os,re,time,datetime
from plearn.utilities.toolkit import search_file

ScriptName="launchdbi.py"
ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|**--nodbilog] [--condor|--bqtools[=nb_proc]|--cluster[=nb_proc]|--local[=nb_proc]|--ssh[=nb_proc]] [--nb_proc=nb_proc] [--mem=X] [--os=X] [--test|*--no_test] [--long|**--no_long] [--micro[=nb_batch]] [--duree=X] [**--cwait|--no_cwait] [--req="CONDOR_REQUIREMENT"] [--force|**--no_force] [--nice|**--no_nice] [--interruptible|**--no_interruptible] [--cpu=nb_cpu_per_node] [**--getenv|--no_getenv] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value in dbidispatch. An ** before -- signal a default option value in dbi'
LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.

%(ShortHelp)s

common options:
  The -h, --help print the long help(this)
  The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
  The --dbilog (--nodbilog) tells dbi to generate (or not) an additional log
  The '--test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
  The '--no_test' option cancel the '--test' option.
  The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.

dbidispatch --test --file=tests


bqtools, cluster, local and ssh options:
  --nb_proc=nb_proc, specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly.
    --local=X is the same as --local --nb_proc=X
    --cluster=X is the same as --cluster --nb_proc=X
    --bqtools=X is the same as --bqtools --nb_proc=X
    --ssh=X is the same as --ssh --nb_proc=X

bqtools and cluster option:
  The '--duree' option specifies the maximum duration of the jobs. The syntax depends on where the job is dispatched. For the cluster syntax, see 'cluster --help'. For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 13 minutes and 15 seconds.

bqtools only options:
  The '--micro[=nb_batch]' option can be used with BqTools when launching many jobs that
  have a very short duration. This may prevent some queue crashes. The nb_batch value
  is the number of experience to group together in a batch.(default 20)

  The '--long' option must be used with BqTools to launch jobs whose duration
  is more than 5 days. The maximum duration of a job will be either the
  BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
  set, and 1200:00:00 (50 days) otherwise.
  Since long jobs are launched on a different queue with few nodes, please make
  sure you are not using too many nodes at once.
  If this option is not set, the maximum duration of each job will be 120 hours
  (5 days).

cluster and condor options:
  The '--3264', '--32' or '--64' specify which type of cpu the node must have to execute the commands.

cluster only options:
  The '--cwait' is transfered to cluster. This must be enabled if there is not nb_proc available nodes. Otherwise when there are no nodes available, the launch of that command fails.
  The '--no_cwait' means the --cwait option is not given to the cluster command, as in the default.
  The '--mem=X' speficify the number of meg the program need to execute.
  The '--os=X' speficify the os of the server: fc4 or fc7. Default: fc4
  The '--force' option is passed to cluster
  The '--interruptible' option is passed to cluster
  The '--cpu=nb_cpu_per_node' option is passed to cluster

condor only options:
  The '--getenv'('--no_getenv') option is forwarded to condor. If True, the current environnement variable will be forwarded to the execution node.
  The '--req=\"CONDOR_REQUIREMENT\"' option makes dbidispatch send additional option to DBI that will be used to generate additional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writen in the following way:

  dbidispatch \"--req=Machine==\\\"computer.example.com\\\"\"
     or
  dbidispatch '--req=Machine=="computer.example.com"' 

  The '--server'(--no_server) option add the requirement that the executing host must be a server dedicated to computing. This is equivalent to: dbidispatch '--req=SERVER==True'(SERVER==False)
  The '--machine=full_host_name' option add the requirement that the executing host is full_host_name
     dbidispatch --machine=computer.example.com
        witch is equivalent to
     dbidispatch '--req=Machine=="computer.example.com"' 
  The '--machines=regexp' option add the requirement that the executing host name must be match the regexp
     dbidispatch '--machines=computer00*'
        witch is equivalent to
     dbidispatch '--req=regexp("computer0*", target.Machine)'
  The '--nice'('--no_nice') option set the nice_user option to condor. If nice, the job(s) will have the lowest possible priority.

where <command-template> is interpreted as follows: the first argument
is the <command> above, and the rest are interpreted as <arguments>.
The arguments may contain segments of the form {{a,b,c,d}}, which trigger
parallel dispatch: a separate 'cluster --execute' command is issued for
the rest of the command template, the first time with value a, the second
time with value b, etc.  For example, the command (NOTE: THERE MUST NOT
BE ANY SPACES WITHIN THE 'numhidden={{5,10,25}}' part and the quotes are
important to avoid shell misinterpretation) :

  dbidispatch aplearn myscript.plearn 'numhidden={{5,10,25}}'

is equivalent to launching three jobs in parallel on the cluster:

  aplearn myscript.plearn numhidden=5
  aplearn myscript.plearn numhidden=10
  aplearn myscript.plearn numhidden=25

If several arguments contain {{ }} forms, all combinations of arguments
are taken, and the jobs are all launched in parallel.  For instance

  dbidispatch aplearn myscript.plearn 'numhidden={{10,25}}' 'wd={{0.01,0.001}}'

is equivalent to:

  aplearn myscript.plearn numhidden=10 wd=0.01
  aplearn myscript.plearn numhidden=10 wd=0.001
  aplearn myscript.plearn numhidden=25 wd=0.01
  aplearn myscript.plearn numhidden=25 wd=0.001

In the file of the option --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.

The '--no_clean_up' set the DBI option clean_up to false

The environnement variable DBIDISPATCH_DEFAULT_OPTION can contain default option that you always want to pass to dbidispatch. You can override them on the command line.
"""%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}

if len(sys.argv) == 1:
    print ShortHelp
    sys.exit(1)
FILE = ""
dbi_param={}


PATH=os.getenv('PATH')
if search_file('condor_submit',PATH):
    launch_cmd = 'Condor'
elif search_file('bqsubmit',PATH):
    launch_cmd = 'bqtools'
elif search_file('cluster',PATH):
    launch_cmd = 'Cluster'
else:
    launch_cmd = 'Local'
to_parse=[]
env=os.getenv("DBIDISPATCH_DEFAULT_OPTION")
if env:
    to_parse=env.split()
to_parse.extend(sys.argv[1:])
command_argv = to_parse[:]
for argv in to_parse:

    if argv == "--help" or argv == "-h":
        print LongHelp
        sys.exit(0)
    elif argv == "--nodbilog":
        dbi_param["dolog"]=False
    elif argv == "--dbilog":
        dbi_param["dolog"]=True
    elif argv.split('=')[0] in ["--bqtools","--cluster","--local","--condor"]:
        launch_cmd = argv[2].upper()+argv.split('=')[0][3:]
        if len(argv.split('='))>1:
            dbi_param["nb_proc"]=argv.split('=')[1]
        if argv.startswith("--ssh"):
            dbi_param["file_redirect_stdout"]=False
            dbi_param["file_redirect_stderr"]=False
    elif argv.startswith("--file="):
        FILE = argv[7:]
    elif argv in ["--32","--64","--3264"]:
        dbi_param["arch"]=argv[2:]
    elif argv.startswith("--micro"):
        dbi_param["micro"]=20
        if len(argv)>7:
            assert(argv[7]=="=")
            dbi_param["micro"]=argv[8:]
    elif argv in  ["--force", "--interruptible", "--long", "--test",
                   "--getenv", "--cwait", "--nice"]:
        dbi_param[argv[2:]]=True
    elif argv in ["--no_force", "--no_interruptible", "--no_long", "--no_test",
                  "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice"]:
        dbi_param[argv[5:]]=False
    elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os",
                                "--nb_proc","--req"]:
        dbi_param[argv.split('=')[0][2:]]=argv.split('=')[1]
    elif argv.startswith('--machine=') or argv.startswith('--machines='):
        if argv.split('=')[0] == "--machine":
            new='Machine=="'+argv.split('=')[1]+'"'
        elif argv.split('=')[0] == "--machines":
            new=dbi_param["req"]='regexp("'+argv.split('=')[1]+'", target.Machine)'
        s=dbi_param.get('req','')
        if s:
            s+=' && '
        dbi_param["req"]=s+new
    elif argv=="--server" or argv=="--no_server":
        if 'req' in dbi_param:
            dbi_param["req"]+=' && '
        else:
            dbi_param['req']=''
        if argv=="--server":
            dbi_param["req"]+='SERVER==True'
        else:
            dbi_param["req"]+='SERVER==False'
    elif argv[0:1] == '-':
	print "Unknow option (%s)"%argv
	print ShortHelp
        sys.exit(1)
    else:
        break
    command_argv.remove(argv)

if len(command_argv) == 0 and FILE == "":
    print ShortHelp
    sys.exit(1)

valid_dbi_param=["clean_up", "test", "dolog"]
if launch_cmd=="Ssh":
    valid_dbi_param+=["nb_proc"]
elif launch_cmd=="Cluster":
    valid_dbi_param +=["cwait","force","nb_proc","arch","interruptible",
                       "duree","cpu","mem","os"]
elif launch_cmd=="Condor":
    valid_dbi_param +=["req", "arch", "getenv", "nice"]
elif launch_cmd=="Bqtools":
    valid_dbi_param +=["micro", "long","nb_proc","duree"]
elif launch_cmd=="Local":
    valid_dbi_param +=["nb_proc"]

from socket import gethostname
if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
    if not os.path.abspath(os.path.curdir).startswith("/home/fringant2/"):
        raise Exception("You must be in a subfolder of /home/fringant2/")
    f=os.getenv("CONDOR_LOCAL_SOURCE")
    if f and not f.startswith("/home/fringant2/"):
        dbi_param['copy_local_source_file']=True

print "\n\nThe jobs will be launched on the system:", launch_cmd
print "With options: ",dbi_param
for i in dbi_param:
    if i not in valid_dbi_param:
        print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end"
print "With the command to be expended:"," ".join(command_argv),"\n\n"

def generate_combination(repl):
    if repl == []:
        return []
    else:
        res = []
        x = repl[0]
        res1 = generate_combination(repl[1:])
        for y in x:
            if res1 == []:
                res.append(y)
            else:
                for r in res1:
                    res.append(y+" "+r)
        return res

def generate_commands(sp):
### Find replacement lists in the arguments
    repl = []
    for arg in sp:
        p = re.compile('\{\{\S*\}\}')
        reg = p.search(arg)
        if reg:
            curargs = reg.group()[2:-2].split(",")# if arg =~ /{{(.*)}}/
            newcurargs = []
            for curarg in curargs:
                new = p.sub(curarg,arg)
                newcurargs.append(new)
            repl.append(newcurargs)
        else:
            repl.append([arg])
    argscombination = generate_combination(repl)
    return argscombination

#generate the command
if FILE != "":
    FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
    commands=[]
    for line in FD.readlines():
        line = line.rstrip()
	sp = line.split(" ")
        commands+=generate_commands(sp)
    FD.close
else:
    commands=generate_commands(command_argv)

if FILE == "":    
    t = [x for x in sys.argv[1:] if not x[:2]=="--"]
    t[0]=os.path.split(t[0])[1]
    tmp="_".join(t)
    tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
    ### We need to remove the symbols "," as this cause trouble with bqtools
    tmp=re.sub( ',', '-', tmp )
    tmp=tmp[:200]
    tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
    dbi_param["log_dir"]=os.path.join("LOGS",tmp)
    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
else:
    dbi_param["log_dir"]=os.path.join("LOGS",FILE)
    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')


SCRIPT=open(os.getenv("HOME")+"/.dbidispatch.launched",'a');
SCRIPT.write("["+time.ctime()+"] "+str(sys.argv)+"\n")
SCRIPT.close()

if "test" in dbi_param:
    print "We generated %s command in the file"% len(commands)
    print "The script %s was not launched"% ScriptName
    SCRIPT=open(ScriptName,'w');
    SCRIPT.write(
"""#! /usr/bin/env python
#%s
from plearn.parallel.dbi import DBI
jobs = DBI([
"""% " ".join(sys.argv))
    for arg in commands:
        cmdstr = "".join(arg);
        SCRIPT.write("   '%s',\n"%cmdstr)
    SCRIPT.write("   ],'%s'"%(launch_cmd))
    for key in dbi_param.keys():
        if isinstance(dbi_param[key],str):
            SCRIPT.write(","+str(key)+"='"+str(dbi_param[key])+"'")
        else:
            SCRIPT.write(","+str(key)+"="+str(dbi_param[key]))
    SCRIPT.write(
""")
jobs.run()
jobs.wait()
# There is %d command in the script"""%(len(commands)))
    if "test" in dbi_param:
        print "[DBI dispatch] In test mode, we do not make dbi errase temp file"
    else:
        SCRIPT.write("jobs.clean()") 
    SCRIPT.close()
    os.system("chmod +x %s"%(ScriptName));

else:
    print "We generate the DBI object with %s command"%(len(commands))
    from plearn.parallel.dbi import *
    print time.ctime()
    t1=time.time()
    jobs = DBI(commands,launch_cmd,**dbi_param)
    t2=time.time()
    print "it took %f s to create the DBI objects"%(t2-t1)
    jobs.run()
    t3=time.time()
    jobs.wait()
    if "test" in dbi_param:
        print "[DBI dispatch] In test mode, we do not make dbi errase temp file"
    else:
        jobs.clean()
    print "it took %f s to launch all the commands"%(t3-t2)

