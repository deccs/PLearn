#!/usr/bin/env python
import sys,os,re,time,datetime
from plearn.utilities.toolkit import search_file

ScriptName="launchdbi.py"
ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] <back-end parameter> {--file=FILEPATH | <command-template>}

<back-end parameter>:
    bqtools, cluster option  :[--duree=X]
    bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
    cluster, condor options  : [--32|--64|--3264] [--os=X] [--mem=N]
    condor option            : [--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
                               [--[*no_]getenv] [*--[no_]prefserver] 
                               [--rank=RANK_EXPRESSION] 
                               [--files=file1[,file2...]]
                               [--env=VAR=VALUE[;VAR2=VALUE2]]
                               [--raw=CONDOR_EXPRESSION] [--tasks_filename={compact,explicit,*nb0,nb1,sh}+]
                               [*--[no_]set_special_env]
    cluster option           : [*--[no_]cwait]  [--[*no_]force]
                               [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
An * after '[', '{' or ',' signals the default value.
An + after } tell that we can put one or more of the choise separeted by a comma
'''
LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster,
local and ssh. If no system is selected on the command line, we try them in the
previous order. ssh is never automaticaly selected.

%(ShortHelp)s

common options:
  The -h, --help print the long help(this)
  The --condor, --bqtools, --cluster, --local or --ssh option specify on which 
    system the jobs will be sent. If not present, we will use the first 
    available in the previously given order. ssh is never automaticaly selected.
  The '--[no_]dbilog' tells dbi to generate (or not) an additional log
  The '--[no_]test' option makes dbidispatch generate the file %(ScriptName)s,
    without executing it. That way you can see what dbidispatch generates. Also,
    this file calls dbi in test mode, so dbi do everything but don't execute the
    jobs. (so you can check the script).
  The '--testdbi' set only dbi in test mode. Not dbidispatch
  The '--file=FILEPATH' specifies a file containing the jobs to execute, one 
    per line. This is instead of specifying job(s) on the command line.
  The '--nb_proc=nb_proc', specifies the maximum number of concurrent jobs. 
    The value -1 will try to execute all jobs concurently. This work for condor,
    bqtools, but for cluster you SHOULD add  the --cwait option.
    --local=N is the same as --local --nb_proc=N
    --cluster=N is the same as --cluster --nb_proc=N
    --bqtools=N is the same as --bqtools --nb_proc=N
    --ssh=N is the same as --ssh --nb_proc=N
    --condor=N  is the same as --condor --nb_proc=N
  The '--[*no_]clean_up' set the DBI option clean_up to true or false

bqtools and cluster option:
  The '--duree' option specifies the maximum duration of the jobs. The syntax 
    depends on the back-end. For the cluster syntax, see 'cluster --help'. 
    For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 
    13 minutes and 15 seconds.

bqtools only options:
  If the --long option is not set, the maximum duration of each job will be 
    120 hours (5 days).
  The '--micro[=nb_batch]' option can be used with BqTools when launching many 
    jobs that have a very short duration. This may prevent some queue crashes. 
    The nb_batch value is the number of experience to group together in a batch.
    (by default not used, --micro is equivalent to --micro=20)
  The '--long' option must be used with BqTools to launch jobs whose duration
    is more than 5 days. The maximum duration of a job will be either the
    BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
    set, and 1200:00:00 (50 days) otherwise. Since long jobs are launched on a
    different queue with few nodes, please make sure you are not using too many
    nodes at once with the --nb_proc option.

cluster and condor options:
  The '--3264', '--32' or '--64' specify the type of cpu for the execution node.
  The '--mem=X' speficify the number of ram in meg the program need to execute.
  The '--os=X' speficify the os of the server. 
    Cluster default: fc7. Cluster accepted value fc4, fc7 and fc9.
    Condor default to the same as the submit host and --os=FC7,FC9 
    tell to use FC7 or FC9 hosts.

cluster only options:
  The '--[no_]cwait' is transfered to cluster. 
    This must be enabled if there is not nb_proc available nodes. Otherwise 
    when there are no nodes available, the launch of that command fails.
  The '--force' option is passed to cluster
  The '--interruptible' option is passed to cluster
  The '--cpu=nb_cpu_per_node' option is passed to cluster

condor only options:
  If the CONDOR_HOME environment variable is set, then the HOME variable will
     be set to this value for jobs submitted to condor.
  The '--[no_]getenv' option is forwarded to condor. If True, the current 
    environnement variable will be forwarded to the execution node.
  The '--req=\"CONDOR_REQUIREMENT\"' add requirement for condor. 
    CONDOR_REQUIREMENT must follow the syntax of requirement for condor with 
    one exception. The symbol '\"' must be escaped 3 times! So the requirement 
    (Machine == \"computer.example.com\") must be writen in the following way:

    dbidispatch \"--req=Machine==\\\"computer.example.com\\\"\"
       or
    dbidispatch '--req=Machine=="computer.example.com"'

  The '--[no_]server' option add the requirement that the executing host must
    be a server dedicated to computing. This is equivalent to: 
    dbidispatch '--req=SERVER==True'(SERVER==False)
  The '--[no_]prefserver' option will tell that you prefer to execute on server
    first. This is equivalent to dbidispatch '--rank=SERVER=?=True'.
  The '--rank=STRING' option add rank=STRING in the submit file.
  The '--machine=full_host_name' option add the requirement that the executing
     host is full_host_name. Is equivalent to
     dbidispatch '--req=Machine=="full_host_name"'
  The '--machines=regexp' option add the requirement that the executing host 
    name must be match the regexp
     dbidispatch '--machines=computer00*'
        witch is equivalent to
     dbidispatch '--req=regexp("computer0*", target.Machine)'
  The '--[no_]nice' option set the nice_user option to condor. 
    If nice, the job(s) will have the lowest possible priority.
  The '--env=VAR=VALUE' option will set in the environment of the executing jobs the variable VAR with value VALUE. To pass many variable you can 1) use one --env option and separ the value by ';'(don't forget to quote) or 2) you can pass many time the --env parameter.
  The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
  The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, MKL_NUM_THREADS and GOTO_NUM_THREADS to the number of cpus allocated to job.
  The '--tasks_filename={compact,explicit,nb0,nb1,sh}+' option will change the filename where the stdout, stderr are redirected. We can put many option separated by comma. They will be appended in the filename with a dot. For all format except sh, they have this pattern condor.X.{out,error} where X=:
      - default : same as nb0
      - compact : a unic string with parameter that change of value between jobs
      - explicit: a unic string that represent the full command to execute
      - nb0     : a number from 0 to nb job -1.
      - nb1     : a number from 1 to nb job.
      - sh      : parse the command for > and 2> redirection command. If one or both of them are missing, they are redirected to /dev/null

where <command-template> is interpreted as follows: the first argument
is the <command> above, and the rest are interpreted as <arguments>.
The arguments may contain one or many segments of the form {{a,b,c,d}}, which generate multiple jobs to execute. Each segement will be replaced by one of the vlue in the segment separated by comma. The first will have the a value, the second the b value, etc. If their is many segment, it will generate the cross-product of possible value between the segment. The jobs will be executed serially or in parallel depending of the backend and the nb_proc option.

  For example, the command (NOTE: THERE MUST NOT
BE ANY SPACES WITHIN THE 'numhidden={{5,10,25}}' part and the quotes are
important to avoid shell misinterpretation) :

  dbidispatch aplearn myscript.plearn 'numhidden={{5,10,25}}'

is equivalent to launching three jobs:

  aplearn myscript.plearn numhidden=5
  aplearn myscript.plearn numhidden=10
  aplearn myscript.plearn numhidden=25

If several arguments contain {{ }} forms, all combinations of arguments
are taken. For instance

  dbidispatch aplearn myscript.plearn 'numhidden={{10,25}}' 'wd={{0.01,0.001}}'

is equivalent to:

  aplearn myscript.plearn numhidden=10 wd=0.01
  aplearn myscript.plearn numhidden=10 wd=0.001
  aplearn myscript.plearn numhidden=25 wd=0.01
  aplearn myscript.plearn numhidden=25 wd=0.001

In the file of the option --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.

The environnement variable DBIDISPATCH_DEFAULT_OPTION can contain default option that you always want to pass to dbidispatch. You can override them on the command line.
The environnement variable DBIDISPATCH_LOGDIR set the name of the directory where all the individual logs directory will be put. If not present default to LOGS.
"""%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}

if len(sys.argv) == 1:
    print ShortHelp
    sys.exit(1)
FILE = ""
dbi_param={}
testmode=False
tasks_filename = ["nb0"]

PATH=os.getenv('PATH')
if search_file('condor_submit',PATH):
    launch_cmd = 'Condor'
elif search_file('bqsubmit',PATH):
    launch_cmd = 'Bqtools'
elif search_file('cluster',PATH):
    launch_cmd = 'Cluster'
else:
    launch_cmd = 'Local'
LOGDIR=os.getenv("DBIDISPATCH_LOGDIR")
if not LOGDIR:
    LOGDIR="LOGS"
if not os.path.exists(LOGDIR):
    os.mkdir(LOGDIR)


to_parse=[]
env=os.getenv("DBIDISPATCH_DEFAULT_OPTION")
if env:
    to_parse=env.split()
to_parse.extend(sys.argv[1:])
command_argv = to_parse[:]
for argv in to_parse:

    if argv == "--help" or argv == "-h":
        print LongHelp
        sys.exit(0)
    #--nodbilog should be allowed do to bug in old version that requested it with _.
    elif argv == "--no_dbilog" or argv == "--nodbilog":
        dbi_param["dolog"]=False
    elif argv == "--dbilog":
        dbi_param["dolog"]=True
    elif argv.split('=')[0] in ["--bqtools","--cluster","--local","--condor",
                                "--ssh"]:
        launch_cmd = argv[2].upper()+argv.split('=')[0][3:]
        if len(argv.split('='))>1:
            dbi_param["nb_proc"]=argv.split('=')[1]
        if argv.startswith("--ssh"):
            dbi_param["file_redirect_stdout"]=False
            dbi_param["file_redirect_stderr"]=False
    elif argv.startswith("--file="):
        FILE = argv[7:]
    elif argv in ["--32","--64","--3264"]:
        dbi_param["arch"]=argv[2:]
    elif argv.startswith("--micro"):
        dbi_param["micro"]=20
        if len(argv)>7:
            assert(argv[7]=="=")
            dbi_param["micro"]=argv[8:]
    elif argv.startswith("--tasks_filename="):
        part = argv.split('=',1)
        accepted_value=["compact","explicit","nb0","nb1","sh"]
        val=part[1].split(",") 
        for v in val:
            if v not in accepted_value:
                print "The option '"+argv+"' have an invalid value. possible value are:", accepted_value
                sys.exit(2)
        tasks_filename = val
    elif argv in  ["--force", "--interruptible", "--long", 
                   "--getenv", "--cwait", "--clean_up" ,"--nice",
                   "--set_special_env"]:
        dbi_param[argv[2:]]=True
    elif argv in ["--no_force", "--no_interruptible", "--no_long",
                  "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
                  "--no_set_special_env"]:
        dbi_param[argv[5:]]=False
    elif argv=="--testdbi":
        dbi_param["test"]=True
    elif argv=="--no_testdbi":
        dbi_param["test"]=False
    elif argv=="--test":
        dbi_param[argv[2:]]=True
        testmode=True
    elif argv=="--no_test":
        dbi_param[argv[2:]]=True
        testmode=False
    elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                "--req", "--files", "--raw", "--rank", "--env"]:
        param=argv.split('=')[0][2:]
        if param in ["req", "files", "rank"]:
            #param that we happend to if defined more then one time
            dbi_param.setdefault(param,'True')
            dbi_param[param]+='&&('+argv.split('=',1)[1]+')'
        elif param == "raw":
            dbi_param.setdefault(param,'')
            dbi_param[param]+='\n'+argv.split('=',1)[1]
        elif param=="env":
            dbi_param.setdefault(param,"")
            dbi_param[param]+='"'+argv.split('=',1)[1]+'"'
        else:
            #otherwise we erase the old value
            dbi_param[param]=argv.split('=',1)[1]
    elif argv.startswith('--machine=') or argv.startswith('--machines='):
        if argv.split('=')[0] == "--machine":
            new='&&(Machine=="'+argv.split('=')[1]+'")'
        elif argv.split('=')[0] == "--machines":
            new='&&(regexp("'+argv.split('=')[1]+'", target.Machine))'
        dbi_param.setdefault('req','True')
        dbi_param["req"]+=new
    elif argv=="--server" or argv=="--no_server" \
            or argv=='--prefserver' or argv=="--no_prefserver":
        if argv.find('prefserver')!=-1:
            param='rank'
        else:
            param='req'
        dbi_param.setdefault(param,'True')
        if argv.find('no_')==-1:
            dbi_param[param]+='&&(SERVER=?=True)'
        else:
            dbi_param[param]+='&&(SERVER=?=False || SERVER =?= UNDEFINED )'
    elif argv[0:1] == '-':
        print "Unknow option (%s)"%argv
        print ShortHelp
        sys.exit(1)
    else:
        break
    command_argv.remove(argv)

if len(command_argv) == 0 and FILE == "":
    print "No command or file with command to execute!"
    print
    print ShortHelp
    sys.exit(1)

valid_dbi_param=["clean_up", "test", "dolog","nb_proc"]
if launch_cmd=="Cluster":
    valid_dbi_param +=["cwait","force","arch","interruptible",
                       "duree","cpu","mem","os"]
elif launch_cmd=="Condor":
    valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                       "raw", "os", "set_special_env"]
elif launch_cmd=="Bqtools":
    valid_dbi_param +=["micro", "long", "duree"]

from socket import gethostname
if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
    p = os.path.abspath(os.path.curdir)
    if any([p.startswith(x) for x in ["/home/fringant1/","/home/fringant2/","/cluster/"]]) or dbi_param.get('files'):
        pass
    else:
        raise Exception("You must be in a subfolder of /home/fringant2/")
    f=os.getenv("CONDOR_LOCAL_SOURCE")
    if f and not f.startswith("/home/fringant2/") and not f.startswith("/cluster"):
        dbi_param['copy_local_source_file']=True

print "\n\nThe jobs will be launched on the system:", launch_cmd
print "With options: ",dbi_param
for i in dbi_param:
    if i not in valid_dbi_param:
        print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end"
print "With the command to be expanded:"," ".join(command_argv),"\n\n"

def generate_combination(repl):
    if repl == []:
        return []
    else:
        res = []
        x = repl[0]
        res1 = generate_combination(repl[1:])
        for y in x:
            if res1 == []:
                res.append(y)
            else:
                res.extend([y+" "+r for r in res1])
        return res

def generate_commands(sp):
### Find replacement lists in the arguments
    repl = []
    for arg in sp:
        p = re.compile('\{\{\S*\}\}')
        reg = p.search(arg)
        if reg:
            curargs = reg.group()[2:-2].split(",")# if arg =~ /{{(.*)}}/
            newcurargs = []
            for curarg in curargs:
                new = p.sub(curarg,arg)
                newcurargs.append(new)
            repl.append(newcurargs)
        else:
            repl.append([arg])
    argscombination = generate_combination(repl)
    args_modif = generate_combination([x for x in repl if len(x)>1])

    return (argscombination,args_modif)

#generate the command
if FILE != "":
    FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
    commands=[]
    choise_args = []
    for line in FD.readlines():
        line = line.rstrip()
        if not line:
            continue
        sp = line.split(" ")
        (t1,t2)=generate_commands(sp)
        commands+=t1
        choise_args+=t2
    FD.close
else:
    (commands,choise_args)=generate_commands(command_argv)

if FILE == "":
    t = [x for x in sys.argv[1:] if not x[:2]=="--"]
    t[0]=os.path.split(t[0])[1]
    tmp="_".join(t)
    tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
    ### We need to remove the symbols "," as this cause trouble with bqtools
    tmp=re.sub( ',', '-', tmp )
    tmp=tmp[:200]
    tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
    dbi_param["log_dir"]=os.path.join(LOGDIR,tmp)
    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
else:
    dbi_param["log_dir"]=os.path.join(LOGDIR,FILE)
    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')

n="base_tasks_log_file"
dbi_param[n]=[""]*len(commands)

def merge_pattern(new_list):
    return [x+'.'+y for (x,y) in  zip(dbi_param[n], new_list)]

for pattern in tasks_filename:
    if pattern == "explicit":
        dbi_param[n]=merge_pattern([re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in commands])
    elif pattern == "compact":
        dbi_param[n]=merge_pattern([re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in choise_args])
       
    elif pattern == "nb0":
        dbi_param[n]=merge_pattern(map(str,range(len(commands))))
    elif pattern == "nb1":
        dbi_param[n]=merge_pattern(map(str,range(1,len(commands)+1)))
    elif pattern == "":
        pass
    elif pattern == "sh":
        stdouts=[]
        stderrs=[]
        for x in range(len(commands)):
            sp=commands[x].split()
            i=0
            output=""
            error=""
            while i < len(sp):
                if sp[i]=="2>":
                    del sp[i]
                    error=sp[i]
                    del sp[i]
                elif sp[i]==">":
                    del sp[i]
                    output=sp[i]
                    del sp[i]
                else:
                    i+=1
            if stdout_file==stderr_file and launch_cmd=="Condor":
                print "ERROR: Condor can't redirect the stdout and stderr to the same file!"
                sys.exit(1)
            stdouts.append(output)
            stderrs.append(error)
            commands[x]=' '.join(sp)
            dbi_param[n]=[]
        dbi_param["stdouts"]=stdouts
        dbi_param["stderrs"]=stderrs
    else:
        print "internal error!"
        sys.exit(2)
    assert(not (dbi_param.has_key("stdouts") and (dbi_param[n])==0))

#undef merge_pattern

SCRIPT=open(os.getenv("HOME")+"/.dbidispatch.launched",'a');
SCRIPT.write("["+time.ctime()+"] "+str(sys.argv)+"\n")
SCRIPT.close()

if testmode:
    print "We generated %s command in the file"% len(commands)
    print "The script %s was not launched"% ScriptName
    SCRIPT=open(ScriptName,'w');
    SCRIPT.write(
"""#! /usr/bin/env python
#%s
from plearn.parallel.dbi import DBI
jobs = DBI([
"""% " ".join(sys.argv))
    for arg in commands:
        cmdstr = "".join(arg);
        SCRIPT.write("   '%s',\n"%cmdstr)
    SCRIPT.write("   ],'%s'"%(launch_cmd))
    for key in dbi_param.keys():
        if isinstance(dbi_param[key],str):
            SCRIPT.write(","+str(key)+"='"+str(dbi_param[key])+"'")
        else:
            SCRIPT.write(","+str(key)+"="+str(dbi_param[key]))
    SCRIPT.write(
""")
jobs.run()
jobs.wait()
# There is %d command in the script"""%(len(commands)))
    if "test" in dbi_param:
        print "[DBI dispatch] In test mode, we do not make dbi errase temp file"
    else:
        SCRIPT.write("jobs.clean()")
    SCRIPT.close()
    os.system("chmod +x %s"%(ScriptName));

else:
    print "We generate the DBI object with %s command"%(len(commands))
    from plearn.parallel.dbi import *
    print time.ctime()
    t1=time.time()
    jobs = DBI(commands,launch_cmd,**dbi_param)
    t2=time.time()
    print "it took %f s to create the DBI objects"%(t2-t1)
    jobs.run()
    t3=time.time()
    jobs.wait()
    if "test" in dbi_param:
        print "[DBI dispatch] In test mode, we do not make dbi errase temp file"
    else:
        jobs.clean()
    print "it took %f s to launch all the commands"%(t3-t2)

