import os.path
from plearn.pyplearn import *
plarg_defaults.conf    = False
plarg_defaults.pseudo    = False

learner=pl.AdaBoost( 
# bool: Whether to compute training error at each stage.
# 
compute_training_error = 1  ,


# bool: Whether to use Confidence-rated AdaBoost (see "Improved
# Boosting Algorithms Using Confidence-rated Predictions" by
# Schapire and Singer) which takes into account the precise
# value outputted by the soft classifier. It also searchs
# the weight of a weak learner using a line search according
# to a criteria which is more appropriate for soft classifiers.
# This option can also be used to obtain MarginBoost with the
# exponential loss, provided that an appropriate choice of
# weak learner is made by the user (see "Functional Gradient
# Techniques for Combining Hypotheses" by Mason et al.).
# 
conf_rated_adaboost = plargs.conf  ,

# bool: If true, then boosting stops when the next weak learner
# is too weak (avg error > target_error - .01)
# 
early_stopping = 1  ,
expdir = ""  ,
forward_sub_learner_test_costs = 1  ,
modif_train_set_weights = 0  ,
nstages = 20  ,

# double: To interpret the output of the learner as a class, it is compared to this
# threshold: class 1 if greater than output_threshold, class 0 otherwise.
# 
output_threshold = 0.5  ,
provide_learner_expdir = 1  ,
pseudo_loss_adaboost = plargs.pseudo  ,
report_progress = 0  ,

# bool: If true, then save the model after training each weak
# learner, under <expdir>/model.psave
# 
save_often = 1  ,

# double: This is the target average weighted error belowwhich each weak learner
# must reach after its training (ordinary adaboost:target_error=0.5).
target_error = 0.5  ,
test_minibatch_size = 1  ,
verbosity = 1  ,
# bool: Whether to train the weak learner using resampling to represent the weighting
# given to examples. If false then give these weights explicitly in the training set
# of the weak learner (note that some learners can accomodate weights well, others not).
# 
weight_by_resampling = 1  ,

# PP< PLearner >: Template for the regression weak learner to beboosted into a classifier
weak_learner_template = pl.RegressionTree(
        nstages = 5,
        loss_function_weight = 1,
        missing_is_valid = 0,
        multiclass_outputs = [0, 1],
        maximum_number_of_nodes = 5,
        compute_train_stats = 0,
        complexity_penalty_factor = 0.0,
        verbosity = 2,
        report_progress = 0,
        leave_template = pl.RegressionTreeLeave( )
        )
)

learner=learner = pl.HyperLearner(
    option_fields = [ "nstages" ],
    dont_restart_upon_change = [ "nstages" ] ,
    provide_strategy_expdir = 1 ,
    save_final_learner = 0 ,
    provide_learner_expdir = 1 ,
    forget_when_training_set_changes = 0 ,
    nstages = 1 ,
    report_progress = 1 ,
    verbosity = 1 ,
    learner = learner,
    tester = pl.PTester(
        splitter = pl.FractionSplitter(splits = TMat(1,3,[ (0,0.75), (0,.75), (0.75,1) ])),
        statnames = [
            'E[test1.E[binary_class_error]]',  'E[test1.E[exp_neg_margin]]',  'E[test1.E[class_error]]',
            'E[test2.E[binary_class_error]]',  'E[test2.E[exp_neg_margin]]',  'E[test2.E[class_error]]', ],
        save_test_outputs = 0 ,
        report_stats = 1  ,
        save_initial_tester = 0 ,
        save_learners = 0 ,
        save_initial_learners = 0  ,
        save_data_sets = 0  ,
        save_test_costs = 0  ,
        provide_learner_expdir = 1  ,
        save_test_confidence = 0  ,
        save_test_names = 0,
        ),
    strategy = [

    pl.HyperOptimize(
            which_cost = "E[test2.E[class_error]]" ,
            provide_tester_expdir = 0 ,
            oracle = pl.EarlyStoppingOracle(
                option = "nstages" ,
                range = [ 1, 11, 2 ],
                min_value = -3.40282e+38 ,
                max_value = 3.40282e+38 ,
                max_degradation = 3.40282e+38 ,
                relative_max_degradation = -1 ,
                min_improvement = -3.40282e+38 ,
                relative_min_improvement = -1 ,
                max_degraded_steps = 120 ,
                min_n_steps = 2 
                )  # end of EarlyStoppingOracle
            )  # end of sub_strategy.HyperOptimize
    ]  # end of HyperLearner strategy
    )
splitter = pl.FractionSplitter(
    splits = TMat(1,3, [ (0,1), (0,0.75), (0.75,1) ])
    )
tester = pl.PTester(
    expdir = plargs.expdir,
    dataset = pl.AutoVMatrix(filename="PLEARNDIR:examples/data/test_suite/linear_4x_2y_binary_class.vmat"),
    splitter = splitter,
    learner = learner,
    statnames = [
                 'E[test1.E[binary_class_error]]',  'E[test1.E[exp_neg_margin]]',  'E[test1.E[class_error]]',
                 'E[test2.E[binary_class_error]]',  'E[test2.E[exp_neg_margin]]',  'E[test2.E[class_error]]', ],
    provide_learner_expdir = 1,
    save_test_costs = 1,
    save_test_outputs = 1,
    save_test_confidence = 0,
    save_learners = 1,
    save_split_stats = 0#not need as their is only one split
    )

def main():
    return tester
