*1 ->HyperLearner(
tester = *2 ->PTester(
expdir = "" ;
dataset = *3 ->MemoryVMatrix(
data = 200  6  [ 
0.337040679017727052 	0.412187421639805485 	0.000975192502189414778 	0.15342979927457695 	2.77555756156289135e-16 	0 	
0.837768842251384371 	0.629942465869766655 	0.99999866745689503 	0.978455821589285457 	1 	1 	
0.441028250703420444 	0.714550797205845578 	0.999661619121736456 	0.608818846025308336 	1 	1 	
0.722711173292159503 	0.534139299013230762 	0.997887018334026488 	0.886113226735210535 	0.999999999976055154 	1 	
0.299130871933929676 	0.360203732190537917 	4.5306261716615559e-05 	0.0928636905426252213 	0 	0 	
0.411124184726996522 	0.639982916836899007 	0.981226002279039111 	0.464305822137039914 	1 	1 	
0.710499249124032395 	0.539688826170337976 	0.997717138254322289 	0.875929972112166055 	0.999999999999153344 	1 	
0.184288240546414961 	0.473853871201329102 	0.000206568890665281835 	0.0439810027269273429 	6.28895564069864577e-07 	0 	
0.247835468384202062 	0.589217134980148494 	0.125253987172025016 	0.134643618374606644 	0.999999999999998446 	1 	
0.819995942533578437 	0.3742750365623283 	0.920000218196554864 	0.925438956030885773 	1.72362124573055553e-13 	1 	
0.208071391136063988 	0.288296895519905672 	1.48940319855128678e-07 	0.0271273696801949682 	0 	0 	
0.964567830275638527 	0.267751622669661571 	0.998437149798841572 	0.996322178190460095 	0 	1 	
0.622142023781279918 	0.569475125142184568 	0.994985677375217148 	0.781983006857181917 	0.999999999999999889 	1 	
0.656000195973205136 	0.632384490644939956 	0.999825304257418446 	0.86209851532548476 	1 	1 	
0.497917512041781807 	0.445255345050883378 	0.0961894574372425537 	0.441785210713063148 	0.00197616314790682868 	0 	
0.678670792025397485 	0.841421101756496204 	0.999999998654930278 	0.959419859808275932 	1 	1 	
0.283249432531651968 	0.394384255062699751 	0.000132176529349881111 	0.0922296303861396272 	0 	0 	
0.821939000754816407 	0.661650928488066725 	0.999999416552044829 	0.976559491191049567 	1 	1 	
0.623898818861622462 	0.568805675730366778 	0.995035619407568017 	0.783844059008265814 	1 	0 	
0.64844932725094484 	0.369018445347890467 	0.090859937004917124 	0.665359349749000017 	3.77475828372553224e-15 	1 	
0.617788016990077793 	0.845311267104432718 	0.999999996183034146 	0.934589218828132506 	1 	1 	
0.560568598210385627 	0.572297230594697615 	0.984167012020504606 	0.685246751326321069 	0.99999999999999889 	1 	
0.730797004413526241 	0.78620836832089469 	0.999999985005914471 	0.964466185212620175 	1 	1 	
0.634618032132718835 	0.750340665972830778 	0.999998947993276044 	0.900715501675833585 	1 	1 	
0.52095022666103985 	0.80575351668482309 	0.999999564090173987 	0.830723002874528271 	1 	1 	
0.787660374462927582 	0.642419269024499417 	0.999995935842380668 	0.96105907030885418 	1 	1 	
0.747573026163381504 	0.408777845324890643 	0.850478635167357888 	0.858465136763180037 	4.27088420185128825e-10 	0 	
0.344049451797056649 	0.59974194633533795 	0.693694777941641361 	0.292132388915243735 	0.999999999999863221 	1 	
0.380091470145261345 	0.390005285685505743 	0.000988058603141606095 	0.193680526820496524 	0 	0 	
0.136071444498306604 	0.174038733327655182 	1.67205693735184013e-11 	0.00519572415845664937 	0 	0 	
0.854708697955331553 	0.722448573065056387 	0.999999990058421329 	0.989025437968076515 	1 	1 	
0.757560278233593398 	0.483466030526050028 	0.993536629503178892 	0.901417965358118689 	0.99950050332745155 	1 	
0.883841283359871888 	0.51078332768391177 	0.999974531176691217 	0.983705356763069472 	0.999999999999489519 	1 	
0.499495000171716774 	0.128114914706039529 	4.6449742940879446e-09 	0.127761238240634289 	0 	0 	
0.518898492739471973 	0.278079094080102784 	0.000104947015177880854 	0.309516644117132977 	0 	0 	
0.65642431966618342 	0.694308507904370042 	0.999989248773869965 	0.892398673461820557 	1 	1 	
0.244436179936414477 	0.674302546821647364 	0.836802915551401627 	0.178239622821568466 	1 	1 	
0.231735044202631524 	0.68041073208306635 	0.826919421809612087 	0.162198541462586776 	1 	0 	
0.420495414774386123 	0.386383438501044674 	0.00196719004512274642 	0.249052878710770975 	0 	0 	
0.464482422012391649 	0.128947152459879366 	2.4809357901389717e-09 	0.100213915042869361 	0 	0 	
0.771956182942409086 	0.590668595471845581 	0.99994253517097742 	0.943001342964679301 	1 	1 	
0.324400846566160694 	0.392763663780197392 	0.000326988014224482981 	0.129919580057503758 	0 	0 	
0.104807823195401495 	0.662371410949871287 	0.0182405353402941284 	0.0261731357173424217 	1 	0 	
0.440594416537475053 	0.256177855855037051 	7.11662389785150395e-06 	0.176037206805503443 	0 	0 	
0.625115862267542122 	0.37115850993626881 	0.0620416273826774334 	0.621408291026117499 	1.66533453693773481e-16 	0 	
0.432416163989962576 	0.202030774074823449 	2.77740037468721113e-07 	0.12810151112620638 	0 	0 	
0.236708604174129145 	0.231879283006988346 	1.80263723947859944e-08 	0.0282227012152627688 	0 	0 	
0.847015190912564275 	0.465124225802565849 	0.99922322286709675 	0.963882434314954462 	0.992415987887875772 	1 	
0.538160645477002486 	0.737008943262299265 	0.999984420515098726 	0.791912130614169874 	1 	1 	
0.28333470835393415 	0.182425523806356349 	2.95443025599695375e-09 	0.0336709161573283122 	0 	0 	
0.587768965659763665 	0.547778771470676551 	0.975656839533297071 	0.711483510236805339 	0.999999999999949818 	1 	
0.618892475744320492 	0.332005146247901783 	0.0102816464864412538 	0.566975061872418307 	0 	0 	
0.694461302348206133 	0.609190310086851761 	0.99980542533631811 	0.88951435084880881 	1 	1 	
0.565943613014406988 	0.506652107479236169 	0.831002657098736996 	0.635906306170384461 	0.999955927919579879 	1 	
0.383086029073044121 	0.945550827907227998 	0.999999999995657696 	0.870359374800871999 	1 	1 	
0.735058297271116379 	0.533385253030013939 	0.998405580711887009 	0.897825170411419959 	0.999999999999998668 	1 	
0.247867159961008499 	0.587887685889858735 	0.11945303331244056 	0.134080683707188841 	0.999999999914290782 	1 	
0.31437592471957404 	0.748901091228292737 	0.999114931451879573 	0.385305232435436462 	1 	1 	
0.356049980032269731 	0.893799247488805126 	0.999999989146302282 	0.719902897371154649 	1 	1 	
0.234179480769041271 	0.370334983797080408 	1.32408575363451853e-05 	0.0521402176285982177 	0 	0 	
0.189684732456134986 	0.38424234704007143 	6.29266245716353367e-06 	0.0330792608751452888 	0 	0 	
0.207031917429434331 	0.565840436553556536 	0.0168649332947274355 	0.0816482575673892486 	0.99999999937393691 	0 	
0.489069241863633541 	0.385463930848654379 	0.00751776669233639172 	0.365124621784082615 	0 	0 	
0.554223418555835012 	0.262956169409198082 	9.92380307193729827e-05 	0.35573593061885489 	0 	0 	
0.546450670398301863 	0.275978125137457453 	0.000164348010774972852 	0.356545375200074566 	0 	0 	
0.331477526458701544 	0.544313426940081446 	0.150549210479969497 	0.226920877779638486 	0.999999973227484795 	0 	
0.608447255824831057 	0.576250504093414606 	0.994923430289251121 	0.766880537582550481 	1 	1 	
0.74833652534817019 	0.217437897918653533 	0.000637167891637702155 	0.710754238216330991 	0 	0 	
0.441918384856973989 	0.232118037868229266 	1.98305498017026238e-06 	0.159403906727846767 	0 	0 	
0.190465240239958922 	0.205912881869682318 	9.90857895732943916e-10 	0.014155296160479891 	0 	0 	
0.832796822890589872 	0.416922441027541113 	0.99074882064070191 	0.9466578043680558 	4.36913572099406622e-07 	1 	
0.328675871745063797 	0.276433934128551662 	1.86322469714061967e-06 	0.0840677020810715048 	0 	0 	
0.441644647089275755 	0.535400668942389912 	0.561201571146024536 	0.418897719404788704 	0.999999996397940238 	1 	
0.253670691389482728 	0.458786221981034303 	0.000868408106177720462 	0.0890735277355632071 	6.48091025290398193e-10 	0 	
0.585589013754027388 	0.224171518555312144 	2.28533497587668499e-05 	0.365583725331242504 	0 	0 	
0.58986806678244541 	0.301534783997461253 	0.00138193182218504518 	0.47202923266442659 	0 	0 	
0.85200702932755279 	0.811420997797132615 	0.999999999927318139 	0.993037708645332362 	1 	1 	
0.392618188076281682 	0.680723781534957939 	0.995455985334303017 	0.471184465788363038 	1 	1 	
0.293918116463156132 	0.507311978686203147 	0.016469475112039389 	0.151277035115703007 	0.0121899329063093198 	1 	
0.44097155133617183 	0.518326786127685679 	0.388719008958370693 	0.400848725095580616 	0.999915657472835395 	0 	
0.527938368925109258 	0.640300813976747207 	0.998214144045750151 	0.690179796975922688 	1 	1 	
0.303877709616286351 	0.433800927030910277 	0.00110349363209288898 	0.127343927610726104 	6.8489658389125907e-13 	0 	
0.413583708380048265 	0.495437626628920491 	0.126920024706036194 	0.328236931934144105 	0.4510101768362893 	1 	
0.371933728396660779 	0.493815315450534476 	0.053806134247652182 	0.254998228149233563 	0.020177763157040729 	0 	
0.42880705432535704 	0.710649379795884273 	0.99947513790175524 	0.580812579989998579 	1 	1 	
0.519185575742599603 	0.860101109434101785 	0.999999991171935276 	0.877510112077225846 	1 	1 	
0.433248369780109721 	0.603801463339838751 	0.9463565486409109 	0.470840625378211142 	1 	1 	
0.333090528816623599 	0.178629567229022346 	7.35518734806817065e-09 	0.0515043955812375387 	0 	0 	
0.363841364105409126 	0.420173792266473223 	0.00243767478559564488 	0.191508185938040587 	1.49880108324396133e-15 	1 	
0.258024822034749468 	0.778690243501383939 	0.999324390855314215 	0.298376672395714193 	1 	1 	
0.308444816500438312 	0.205530525293464994 	2.36985961965530123e-08 	0.0489708262894453794 	0 	0 	
0.275265359242249819 	0.356809252589806836 	2.18167656550471989e-05 	0.0740054232710109505 	0 	0 	
0.447978933902173193 	0.807225196990252192 	0.999998285967393041 	0.73374557424670217 	1 	1 	
0.569710475705847208 	0.124579530102164993 	1.386039960582508e-08 	0.200000697780095438 	0 	0 	
0.907073953054543303 	0.320080909610997666 	0.979327058711887077 	0.97818028340714902 	0 	1 	
0.346022306600591856 	0.33957471551374363 	5.35527421994252961e-05 	0.125863341080325264 	0 	0 	
0.603964125026318244 	0.633402136580338504 	0.999488949006740235 	0.800804485191756843 	1 	1 	
0.684720245521482473 	0.837664162630183062 	0.999999998453590444 	0.960523011225891343 	1 	1 	
0.708090895277180166 	0.471418691600586415 	0.963949563452032532 	0.840129240225992779 	0.000781299807628477172 	1 	
0.771582763339008881 	0.645452661865975386 	0.999994314239608828 	0.954011308682431336 	1 	1 	
0.460149701882857753 	0.332311270435153072 	0.000419402495029586042 	0.265641436326872871 	0 	0 	
0.831530044133463875 	0.529186781004250317 	0.999893949594790166 	0.964762122623453844 	0.999999999999998224 	1 	
0.65018898152607485 	0.387873429950195248 	0.187975328574094158 	0.686548020585021845 	1.37057032389975575e-13 	0 	
0.424831603024144355 	0.785299307796495372 	0.999989387209282654 	0.666209358798257378 	1 	1 	
0.223955933194476708 	0.85297906339534868 	0.9999884384030715 	0.325763342209897722 	1 	1 	
0.568189952611602878 	0.5042149001566536 	0.823608596003203242 	0.637266608512469634 	0.999735435267097072 	0 	
0.196164666090740902 	0.698362006304233507 	0.792958232215262515 	0.121199495873135221 	1 	1 	
0.635883152908142013 	0.739603738348431294 	0.999998198375502856 	0.896384266143035013 	1 	1 	
0.766440462928471122 	0.776889333222431144 	0.999999989972555126 	0.974032527455173658 	1 	1 	
0.439712238997702287 	0.223776970430579092 	1.18040709834454915e-06 	0.150663062771468848 	0 	0 	
0.270576426620127486 	0.800109679656329731 	0.999865169866101078 	0.355412841484316178 	1 	1 	
0.595406137160540805 	0.273123998909772792 	0.000387076909607542152 	0.44840679059006272 	0 	0 	
0.709583071638395624 	0.244892645065264869 	0.00111965991489298977 	0.659388419418811988 	0 	0 	
0.631612337149145886 	0.741398499859989402 	0.999998200949979932 	0.893976621243775638 	1 	1 	
0.694771004175420592 	0.532118213643730242 	0.995500010183678841 	0.854748431601624392 	0.99999999999676481 	1 	
0.690109228104916594 	0.857407207850628961 	0.999999999704517029 	0.967619107040243787 	1 	1 	
0.791478407262177441 	0.706123387877502617 	0.999999802130912219 	0.971923386203978446 	1 	1 	
0.591951822229620506 	0.447380558507254611 	0.437225483940870852 	0.630336343554293577 	3.58298156311054328e-07 	0 	
0.416691516057121847 	0.536761047080265952 	0.448011082314667908 	0.371359256055882048 	0.999999999896008296 	1 	
0.241271710605322209 	0.574171145689825524 	0.0606744175626963145 	0.120029588541164689 	0.999999999998035904 	0 	
0.61683834213865274 	0.786651776820837867 	0.999999800892349078 	0.905214887115931233 	1 	1 	
0.893793552025074045 	0.56782679783496337 	0.999998454853059715 	0.989364338420214429 	1 	1 	
0.65921321328677962 	0.755683846757965627 	0.99999953936066821 	0.920540241090206934 	1 	1 	
0.403657119741667958 	0.475973518743775081 	0.0515078153070945288 	0.293677438926866541 	2.50858883715232572e-05 	0 	
0.183188899325371801 	0.79680275397946343 	0.997954101035577246 	0.164779162168442184 	1 	1 	
0.159915087742074802 	0.641405583127205969 	0.0773014406994857461 	0.0607616755376079176 	1 	1 	
0.590438885457506224 	0.42420618845194985 	0.226805116304403587 	0.604891812518721261 	2.96487784323673509e-09 	1 	
0.2739605965980349 	0.488614468492700071 	0.00482695885510686651 	0.119748916573926278 	4.1677502848891379e-06 	1 	
0.265423008491722512 	0.573310286606669717 	0.105633508560364686 	0.149217695648006854 	0.999999999999996669 	0 	
0.260515685218742687 	0.730119706711473304 	0.9913015161315889 	0.25152413682623348 	1 	1 	
0.859460949480679082 	0.482986570768417611 	0.999769089631301733 	0.972226055181255377 	0.999999927521804555 	1 	
0.715253965469259345 	0.501222497563960645 	0.990567622975351059 	0.863734471591099506 	0.999999956288300407 	1 	
0.705809945524337068 	0.544043054318400721 	0.997853911496872081 	0.872929195401613289 	0.999999999703498843 	1 	
0.384680150978340629 	0.709568646181242491 	0.998620053983448219 	0.488109742125673263 	1 	1 	
0.477595014000419482 	0.277766030791077922 	4.52085102251031934e-05 	0.243125441624808647 	0 	0 	
0.363834420308997264 	0.713118834028081405 	0.998188849124578059 	0.448461205598444546 	1 	1 	
0.80244558809579325 	0.420965787314543094 	0.97854438996804638 	0.923014652663194157 	3.32325866825300409e-09 	1 	
0.472425506246140436 	0.761427188712762382 	0.999984163384094682 	0.718962280122859676 	1 	1 	
0.386511475642943525 	0.584620412924011124 	0.751705671104142548 	0.358433429691765726 	0.999999999999998224 	1 	
0.313344389534551349 	0.563876550224456041 	0.205293859180526783 	0.212465091231545666 	0.999999850909174426 	1 	
0.437498628835231718 	0.271487240160874554 	1.47010049714180191e-05 	0.18408839079913808 	0 	0 	
0.349972520410344767 	0.823272754127770234 	0.999995406942313592 	0.574459628804300237 	1 	1 	
0.511535452700446203 	0.163949577360044452 	1.05913818548852845e-07 	0.177025707750321615 	0 	0 	
0.333981910556185979 	0.777083405190116761 	0.999881013318794643 	0.467065256689115749 	1 	1 	
0.733563847159287241 	0.523964125199950947 	0.997583968485294292 	0.89304320560242656 	0.999999987101309107 	1 	
0.664233998157672811 	0.223942029527671616 	0.000121280891147379499 	0.530026326836644524 	0 	0 	
0.747159532501805002 	0.61025399546998349 	0.999949900528595004 	0.931935003882062851 	1 	1 	
0.647068916766090241 	0.842445826312409496 	0.999999997473194568 	0.947256531484346631 	1 	1 	
0.620160612968468916 	0.293360169711035401 	0.00176105431478684737 	0.525328301669363729 	0 	0 	
0.180912118693824064 	0.473026213425085496 	0.000178464923893839611 	0.0419870523418195973 	2.19526119504820372e-10 	0 	
0.171486298949319993 	0.453303976992695201 	5.83472488787095678e-05 	0.034303904575232691 	2.83106871279414918e-14 	1 	
0.480642180523963292 	0.496247570451631481 	0.368762354055697128 	0.457569793089636412 	0.0782792313851559673 	0 	
0.460646565225021232 	0.78734912020993153 	0.99999545505324261 	0.729662318511505426 	1 	1 	
0.467164824120552491 	0.375671870776489891 	0.00321294243586323081 	0.316254176329649583 	0 	0 	
0.43385005121768927 	0.85176982124301015 	0.99999990359908042 	0.771436448839911471 	1 	1 	
0.537295599170006577 	0.625186163434580333 	0.997167078991974876 	0.692086063473239976 	1 	1 	
0.77206002977913113 	0.408222101578465835 	0.915805160160764076 	0.887859404382179029 	5.1289749669614082e-09 	1 	
0.215260988690660682 	0.421377785818776385 	6.51462934598834309e-05 	0.0518779202071764645 	5.5511151231257827e-17 	0 	
0.526240868080626534 	0.344837501105276723 	0.00275164754263412803 	0.393932927643954378 	0 	1 	
0.352332982714121834 	0.228780499937454418 	2.5141515719306895e-07 	0.0807752533440048159 	0 	0 	
0.530556872338403984 	0.435769357914573607 	0.122211770968852884 	0.497170030219862114 	5.83355058703105556e-09 	0 	
0.418968313566468575 	0.676882136091753517 	0.996857941138302728 	0.521337033709274533 	1 	0 	
0.403587313458172448 	0.824644927680108442 	0.999998667856597523 	0.682776283308621723 	1 	1 	
0.361998630782866759 	0.370204408571073618 	0.00028952770396439842 	0.159220420181713229 	0 	0 	
0.908979683193673837 	0.839935736059859939 	0.999999999999364064 	0.99809217160669339 	1 	1 	
0.832844161267142113 	0.489798915748868746 	0.999510418543040791 	0.959708536196156592 	0.999999999975029863 	1 	
0.247191042132576255 	0.565702859063909158 	0.0509387213597051192 	0.123082290071373113 	0.999999999999863887 	0 	
0.236347439196011755 	0.4184425886375715 	0.000105586969146143073 	0.064450896143266434 	1.17683640610266593e-14 	0 	
0.878774939360102647 	0.250912786885272465 	0.262483546060253647 	0.946270181212051198 	0 	0 	
0.560856785545585934 	0.387939911774027868 	0.0343357208576768325 	0.507874348900542727 	3.68705066478014487e-13 	0 	
0.61900332590026208 	0.209175906866663652 	1.89742269794956897e-05 	0.411150816107167638 	0 	0 	
0.198155916735463955 	0.72655228198141264 	0.941735419830920351 	0.13952389135164156 	1 	1 	
0.869351838403421562 	0.6676266001109048 	0.999999928312545139 	0.988892993459312253 	1 	1 	
0.314412568305269202 	0.379561463986388015 	0.000148908442682282871 	0.114100891594720921 	0 	0 	
0.670922423671947432 	0.408098903170796223 	0.460947205585807318 	0.741358235198477655 	2.63092586605395695e-09 	1 	
0.307845747776586576 	0.107073381074912766 	1.06973874203220021e-11 	0.0231603669418962155 	0 	0 	
0.764897455283813699 	0.67360817009344176 	0.99999804306670792 	0.956255113223712705 	1 	1 	
0.602099322710622831 	0.705791257374221992 	0.999980034404002849 	0.845851840468065497 	1 	1 	
0.498410005359550323 	0.443684957890750631 	0.0916210989854581159 	0.440416965031909857 	9.98667815110820811e-12 	0 	
0.2494982653002355 	0.551907308225648374 	0.0315959640562301081 	0.119655134207207636 	0.999999866082263766 	0 	
0.371349918041573357 	0.649128355496379927 	0.971250387171610163 	0.392262925955319308 	1 	1 	
0.676505835838878578 	0.232669252996851095 	0.000262714478394654449 	0.569629577497201667 	0 	0 	
0.630375954790908244 	0.224279366557609772 	5.88853110093379151e-05 	0.45672355544882115 	0 	0 	
0.300711932385879122 	0.149829364552470645 	4.24941304366655004e-10 	0.031597799967373108 	0 	0 	
0.584741827477265108 	0.0924290853768408027 	6.64550636741978451e-10 	0.168118627952250577 	0 	0 	
0.201814833111637948 	0.540414142004595033 	0.00519546572841989018 	0.0699693665010114874 	0.999805662526560335 	0 	
0.558190128039561695 	0.616932877239985022 	0.997360703265658066 	0.720076277306989421 	1 	1 	
0.470688646750369089 	0.210093817686775486 	9.85101667738685904e-07 	0.173895030928417071 	0 	0 	
0.532031073717923908 	0.760036567480947722 	0.999994818185025225 	0.803668134940642487 	1 	1 	
0.208994001596146228 	0.76073439080480787 	0.992696853128328893 	0.18157395531864462 	1 	1 	
0.116346727192441657 	0.167537635143858665 	4.31321645066873316e-12 	0.0034723336958780715 	0 	0 	
0.134669253672196843 	0.830249986777758409 	0.998603683097191275 	0.105850998399096619 	1 	1 	
0.549769642170096162 	0.552231256528370573 	0.95671254064267286 	0.647573451897092478 	0.999840819088372856 	1 	
0.165243489232508756 	0.537479564772721186 	0.0013631676983125729 	0.0434812741523764035 	0.925412731147525536 	1 	
0.317949746542236178 	0.74753339550780562 	0.999123747824000263 	0.3910073121171318 	1 	1 	
0.392801824493781482 	0.487425152233795955 	0.0641128554671341333 	0.284365332967790707 	0.994211403487852863 	0 	
0.710161800997805237 	0.271536688465156772 	0.00455234525237191434 	0.690991769450284621 	0 	0 	
0.258779846056689244 	0.328034106346786924 	3.98635217790932472e-06 	0.0561383721754414866 	0 	0 	
0.606341601082149295 	0.530905550153722428 	0.967631950849599187 	0.728364397980203471 	0.999999999996517897 	0 	
0.600135566392399955 	0.291099509296859849 	0.00103701637983616424 	0.48018039043485411 	0 	0 	
]
;
fieldinfos = 6 [ "x1" 0 "x2" 0 "x3" 0 "x4" 0 "y1" 0 "target" 0 ] ;
fieldnames = []
;
deep_copy_memory_data = 1 ;
writable = 0 ;
length = 200 ;
width = 6 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
extrasize = 0 ;
metadatadir = "" ;
source = *0  )
;
splitter = *4 ->FractionSplitter(
round_to_closest = 0 ;
splits = 1  3  [ 
(0 , 0.75 )	(0 , 0.75 )	(0.75 , 1 )	
]
;
one_is_absolute = 0  )
;
statnames = 6 [ "E[test1.E[binary_class_error]]" "E[test1.E[exp_neg_margin]]" "E[test1.E[class_error]]" "E[test2.E[binary_class_error]]" "E[test2.E[exp_neg_margin]]" "E[test2.E[class_error]]" ] ;
statmask = []
;
learner = *5 ->AdaBoost(
weak_learners = 5 [ *6 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 5 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 2 [ 0 1 ] ;
leave_template = *7 ->RegressionTreeLeave(
id = -1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
root = *8 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *9 ->RegressionTreeLeave(
id = 1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 159 ;
weights_sum = 0.999999999999999889 ;
targets_sum = 99 ;
weighted_targets_sum = 0.622641509433962237 ;
weighted_squared_targets_sum = 0.622641509433962237 ;
loss_function_factor = 2  )
;
leave = *9  ;
leave_output = 2 [ 0.622641509433962348 1 ] ;
leave_error = 3 [ 0.469918120327518563 0 0.469918120327518563 ] ;
split_col = 2 ;
split_balance = 29 ;
split_feature_value = 0.188677163392186542 ;
after_split_error = 0.165982151128678812 ;
missing_node = *0 ;
missing_leave = *10 ->RegressionTreeLeave(
id = 2 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *11 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *12 ->RegressionTreeLeave(
id = 3 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 65 ;
weights_sum = 0.408805031446540845 ;
targets_sum = 10 ;
weighted_targets_sum = 0.0628930817610062892 ;
weighted_squared_targets_sum = 0.0628930817610062892 ;
loss_function_factor = 2  )
;
leave = *12  ;
leave_output = 2 [ 0.153846153846153855 1 ] ;
leave_error = 3 [ 0.106434446057087573 0 0.106434446057087573 ] ;
split_col = 4 ;
split_balance = 17 ;
split_feature_value = 8.88178419700125232e-16 ;
after_split_error = 0.0733752620545073397 ;
missing_node = *0 ;
missing_leave = *13 ->RegressionTreeLeave(
id = 5 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *14 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *15 ->RegressionTreeLeave(
id = 6 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 41 ;
weights_sum = 0.257861635220125784 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
leave = *15  ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 3 ;
split_balance = 1 ;
split_feature_value = 0.185353117285409069 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *16 ->RegressionTreeLeave(
id = 11 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *17 ->RegressionTreeLeave(
id = 12 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00628930817610067958 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *18 ->RegressionTreeLeave(
id = 13 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 40 ;
weights_sum = 0.251572327044025157 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
 )
;
left_leave = *15  ;
right_node = *19 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *20 ->RegressionTreeLeave(
id = 7 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 24 ;
weights_sum = 0.150943396226415089 ;
targets_sum = 10 ;
weighted_targets_sum = 0.0628930817610062892 ;
weighted_squared_targets_sum = 0.0628930817610062892 ;
loss_function_factor = 2  )
;
leave = *20  ;
leave_output = 2 [ 0.416666666666666685 1 ] ;
leave_error = 3 [ 0.0733752620545073397 0 0.0733752620545073397 ] ;
split_col = 4 ;
split_balance = 16 ;
split_feature_value = 3.44335671087492301e-13 ;
after_split_error = 0.0528301886792452852 ;
missing_node = *0 ;
missing_leave = *21 ->RegressionTreeLeave(
id = 14 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *22 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *23 ->RegressionTreeLeave(
id = 15 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 4 ;
weights_sum = 0.0251572327044025171 ;
targets_sum = 4 ;
weighted_targets_sum = 0.0251572327044025171 ;
weighted_squared_targets_sum = 0.0251572327044025171 ;
loss_function_factor = 2  )
;
leave = *23  ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 4 ;
split_balance = 0 ;
split_feature_value = 2.63677968348474678e-15 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *24 ->RegressionTreeLeave(
id = 17 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *25 ->RegressionTreeLeave(
id = 18 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.006289308176100631 ;
targets_sum = 1 ;
weighted_targets_sum = 0.006289308176100631 ;
weighted_squared_targets_sum = 0.006289308176100631 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *26 ->RegressionTreeLeave(
id = 19 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 3 ;
weights_sum = 0.0188679245283018895 ;
targets_sum = 3 ;
weighted_targets_sum = 0.0188679245283018895 ;
weighted_squared_targets_sum = 0.0188679245283018895 ;
loss_function_factor = 2  )
 )
;
left_leave = *23  ;
right_node = *27 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *28 ->RegressionTreeLeave(
id = 16 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 20 ;
weights_sum = 0.125786163522012578 ;
targets_sum = 6 ;
weighted_targets_sum = 0.0377358490566037721 ;
weighted_squared_targets_sum = 0.0377358490566037721 ;
loss_function_factor = 2  )
;
leave = *28  ;
leave_output = 2 [ 0.299999999999999989 1 ] ;
leave_error = 3 [ 0.0528301886792452852 0 0.0528301886792452852 ] ;
split_col = 2 ;
split_balance = 10 ;
split_feature_value = 0.0166672042033834122 ;
after_split_error = 0.045283018867924546 ;
missing_node = *0 ;
missing_leave = *29 ->RegressionTreeLeave(
id = 20 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *30 ->RegressionTreeLeave(
id = 21 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00628930817610065182 ;
targets_sum = 0 ;
weighted_targets_sum = 1.73472347597680709e-18 ;
weighted_squared_targets_sum = 1.73472347597680709e-18 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *31 ->RegressionTreeLeave(
id = 22 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 19 ;
weights_sum = 0.119496855345911937 ;
targets_sum = 6 ;
weighted_targets_sum = 0.0377358490566037721 ;
weighted_squared_targets_sum = 0.0377358490566037721 ;
loss_function_factor = 2  )
 )
;
right_leave = *28   )
;
right_leave = *20   )
;
left_leave = *12  ;
right_node = *32 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *33 ->RegressionTreeLeave(
id = 4 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 94 ;
weights_sum = 0.591194968553459099 ;
targets_sum = 89 ;
weighted_targets_sum = 0.559748427672955962 ;
weighted_squared_targets_sum = 0.559748427672955962 ;
loss_function_factor = 2  )
;
leave = *33  ;
leave_output = 2 [ 0.946808510638297851 1 ] ;
leave_error = 3 [ 0.0595477050715911282 0 0.0595477050715911282 ] ;
split_col = 3 ;
split_balance = 88 ;
split_feature_value = 0.170219082142077621 ;
after_split_error = 0.0448775543115166875 ;
missing_node = *0 ;
missing_leave = *34 ->RegressionTreeLeave(
id = 8 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *35 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *36 ->RegressionTreeLeave(
id = 9 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 3 ;
weights_sum = 0.0188679245283018895 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00628930817610062927 ;
weighted_squared_targets_sum = 0.00628930817610062927 ;
loss_function_factor = 2  )
;
leave = *36  ;
leave_output = 2 [ 0.333333333333333315 1 ] ;
leave_error = 3 [ 0.00838574423480083903 0 0.00838574423480083903 ] ;
split_col = 3 ;
split_balance = 1 ;
split_feature_value = 0.141699018667860999 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *37 ->RegressionTreeLeave(
id = 23 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *38 ->RegressionTreeLeave(
id = 24 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.006289308176100631 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *39 ->RegressionTreeLeave(
id = 25 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 2 ;
weights_sum = 0.0125786163522012585 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00628930817610062927 ;
weighted_squared_targets_sum = 0.00628930817610062927 ;
loss_function_factor = 2  )
 )
;
left_leave = *36  ;
right_node = *40 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *41 ->RegressionTreeLeave(
id = 10 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 91 ;
weights_sum = 0.572327044025157217 ;
targets_sum = 88 ;
weighted_targets_sum = 0.553459119496855334 ;
weighted_squared_targets_sum = 0.553459119496855334 ;
loss_function_factor = 2  )
;
leave = *41  ;
leave_output = 2 [ 0.967032967032967039 1 ] ;
leave_error = 3 [ 0.0364918100767157583 0 0.0364918100767157583 ] ;
split_col = 1 ;
split_balance = 41 ;
split_feature_value = 0.569140400436275673 ;
after_split_error = 0.0332075471698113078 ;
missing_node = *0 ;
missing_leave = *42 ->RegressionTreeLeave(
id = 26 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *43 ->RegressionTreeLeave(
id = 27 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00628930817610073509 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00628930817610073509 ;
weighted_squared_targets_sum = 0.00628930817610073509 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *44 ->RegressionTreeLeave(
id = 28 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 90 ;
weights_sum = 0.566037735849056589 ;
targets_sum = 87 ;
weighted_targets_sum = 0.547169811320754707 ;
weighted_squared_targets_sum = 0.547169811320754707 ;
loss_function_factor = 2  )
 )
;
right_leave = *41   )
;
right_leave = *33   )
;
priority_queue = *45 ->RegressionTreeQueue(
verbosity = 2 ;
maximum_number_of_nodes = 5 ;
next_available_node = 5 ;
nodes = 5 [ *35  *27  *22  *14  *40  ]  )
;
first_leave = *9  ;
split_cols = 4 [ 2 4 4 3 ] ;
split_values = 4 [ 0.188677163392186542 8.88178419700125232e-16 3.44335671087492301e-13 0.170219082142077621 ] ;
random_gen = *0 ;
seed = 1827 ;
stage = 5 ;
n_examples = 159 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 5 ;
report_progress = 0 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
*46 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 5 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 2 [ 0 1 ] ;
leave_template = *47 ->RegressionTreeLeave(
id = -1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
root = *48 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *49 ->RegressionTreeLeave(
id = 1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 149 ;
weights_sum = 0.999999999999998779 ;
targets_sum = 93 ;
weighted_targets_sum = 0.624161073825502677 ;
weighted_squared_targets_sum = 0.624161073825502677 ;
loss_function_factor = 2  )
;
leave = *49  ;
leave_output = 2 [ 0.624161073825503454 1 ] ;
leave_error = 3 [ 0.469168055492995228 0 0.469168055492995228 ] ;
split_col = 1 ;
split_balance = 59 ;
split_feature_value = 0.482293993618237549 ;
after_split_error = 0.260686628807433929 ;
missing_node = *0 ;
missing_leave = *50 ->RegressionTreeLeave(
id = 2 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *51 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *52 ->RegressionTreeLeave(
id = 3 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 45 ;
weights_sum = 0.302013422818791732 ;
targets_sum = 6 ;
weighted_targets_sum = 0.0402684563758389236 ;
weighted_squared_targets_sum = 0.0402684563758389236 ;
loss_function_factor = 2  )
;
leave = *52  ;
leave_output = 2 [ 0.133333333333333415 1 ] ;
leave_error = 3 [ 0.069798657718120799 0 0.069798657718120799 ] ;
split_col = 2 ;
split_balance = 35 ;
split_feature_value = 0.885239426681956321 ;
after_split_error = 0.0130872483221476481 ;
missing_node = *0 ;
missing_leave = *53 ->RegressionTreeLeave(
id = 5 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *54 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *55 ->RegressionTreeLeave(
id = 6 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 40 ;
weights_sum = 0.268456375838926009 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00671140939597315422 ;
weighted_squared_targets_sum = 0.00671140939597315422 ;
loss_function_factor = 2  )
;
leave = *55  ;
leave_output = 2 [ 0.0250000000000000153 1 ] ;
leave_error = 3 [ 0.0130872483221476498 0 0.0130872483221476498 ] ;
split_col = 1 ;
split_balance = 20 ;
split_feature_value = 0.414475818795681961 ;
after_split_error = 0.0120805369127516774 ;
missing_node = *0 ;
missing_leave = *56 ->RegressionTreeLeave(
id = 11 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *57 ->RegressionTreeLeave(
id = 12 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00671140939597316636 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *58 ->RegressionTreeLeave(
id = 13 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 39 ;
weights_sum = 0.261744966442952864 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00671140939597315422 ;
weighted_squared_targets_sum = 0.00671140939597315422 ;
loss_function_factor = 2  )
 )
;
left_leave = *55  ;
right_node = *59 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *60 ->RegressionTreeLeave(
id = 7 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 5 ;
weights_sum = 0.033557046979865772 ;
targets_sum = 5 ;
weighted_targets_sum = 0.033557046979865772 ;
weighted_squared_targets_sum = 0.033557046979865772 ;
loss_function_factor = 2  )
;
leave = *60  ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 4 ;
split_balance = 1 ;
split_feature_value = 1.66171551518878857e-09 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *61 ->RegressionTreeLeave(
id = 14 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *62 ->RegressionTreeLeave(
id = 15 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00671140939597315248 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00671140939597315248 ;
weighted_squared_targets_sum = 0.00671140939597315248 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *63 ->RegressionTreeLeave(
id = 16 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 4 ;
weights_sum = 0.0268456375838926169 ;
targets_sum = 4 ;
weighted_targets_sum = 0.0268456375838926169 ;
weighted_squared_targets_sum = 0.0268456375838926169 ;
loss_function_factor = 2  )
 )
;
right_leave = *60   )
;
left_leave = *52  ;
right_node = *64 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *65 ->RegressionTreeLeave(
id = 4 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 104 ;
weights_sum = 0.697986577181207268 ;
targets_sum = 87 ;
weighted_targets_sum = 0.583892617449663809 ;
weighted_squared_targets_sum = 0.583892617449663809 ;
loss_function_factor = 2  )
;
leave = *65  ;
leave_output = 2 [ 0.836538461538461564 1 ] ;
leave_error = 3 [ 0.190887971089313102 0 0.190887971089313102 ] ;
split_col = 4 ;
split_balance = 30 ;
split_feature_value = 0.999999999538717876 ;
after_split_error = 0.145761307736665707 ;
missing_node = *0 ;
missing_leave = *66 ->RegressionTreeLeave(
id = 8 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *67 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *68 ->RegressionTreeLeave(
id = 9 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 37 ;
weights_sum = 0.248322147651006547 ;
targets_sum = 22 ;
weighted_targets_sum = 0.147651006711409377 ;
weighted_squared_targets_sum = 0.147651006711409377 ;
loss_function_factor = 2  )
;
leave = *68  ;
leave_output = 2 [ 0.594594594594594961 1 ] ;
leave_error = 3 [ 0.119717032468710211 0 0.119717032468710211 ] ;
split_col = 4 ;
split_balance = 1 ;
split_feature_value = 0.725372806051693186 ;
after_split_error = 0.033557046979865772 ;
missing_node = *0 ;
missing_leave = *69 ->RegressionTreeLeave(
id = 17 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *70 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *71 ->RegressionTreeLeave(
id = 18 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 19 ;
weights_sum = 0.127516778523489943 ;
targets_sum = 19 ;
weighted_targets_sum = 0.127516778523489943 ;
weighted_squared_targets_sum = 0.127516778523489943 ;
loss_function_factor = 2  )
;
leave = *71  ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 4 ;
split_balance = 5 ;
split_feature_value = 0.00609705032829710447 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *72 ->RegressionTreeLeave(
id = 23 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *73 ->RegressionTreeLeave(
id = 24 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.0067114093959731386 ;
targets_sum = 1 ;
weighted_targets_sum = 0.0067114093959731386 ;
weighted_squared_targets_sum = 0.0067114093959731386 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *74 ->RegressionTreeLeave(
id = 25 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 18 ;
weights_sum = 0.120805369127516798 ;
targets_sum = 18 ;
weighted_targets_sum = 0.120805369127516798 ;
weighted_squared_targets_sum = 0.120805369127516798 ;
loss_function_factor = 2  )
 )
;
left_leave = *71  ;
right_node = *75 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *76 ->RegressionTreeLeave(
id = 19 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 18 ;
weights_sum = 0.120805369127516798 ;
targets_sum = 3 ;
weighted_targets_sum = 0.0201342281879194618 ;
weighted_squared_targets_sum = 0.0201342281879194618 ;
loss_function_factor = 2  )
;
leave = *76  ;
leave_output = 2 [ 0.16666666666666663 1 ] ;
leave_error = 3 [ 0.033557046979865772 0 0.033557046979865772 ] ;
split_col = 4 ;
split_balance = 10 ;
split_feature_value = 0.999935792696207582 ;
after_split_error = 0.0100671140939597309 ;
missing_node = *0 ;
missing_leave = *77 ->RegressionTreeLeave(
id = 26 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *78 ->RegressionTreeLeave(
id = 27 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00671140939597315248 ;
targets_sum = 0 ;
weighted_targets_sum = -1.73472347597680709e-18 ;
weighted_squared_targets_sum = -1.73472347597680709e-18 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *79 ->RegressionTreeLeave(
id = 28 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 17 ;
weights_sum = 0.11409395973154364 ;
targets_sum = 3 ;
weighted_targets_sum = 0.0201342281879194618 ;
weighted_squared_targets_sum = 0.0201342281879194618 ;
loss_function_factor = 2  )
 )
;
right_leave = *76   )
;
left_leave = *68  ;
right_node = *80 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *81 ->RegressionTreeLeave(
id = 10 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 67 ;
weights_sum = 0.449664429530200915 ;
targets_sum = 65 ;
weighted_targets_sum = 0.436241610738254626 ;
weighted_squared_targets_sum = 0.436241610738254626 ;
loss_function_factor = 2  )
;
leave = *81  ;
leave_output = 2 [ 0.970149253731343308 1 ] ;
leave_error = 3 [ 0.0260442752679554967 0 0.0260442752679554967 ] ;
split_col = 3 ;
split_balance = 65 ;
split_feature_value = 0.0434674056274751697 ;
after_split_error = 0.0132194427496440392 ;
missing_node = *0 ;
missing_leave = *82 ->RegressionTreeLeave(
id = 20 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *83 ->RegressionTreeLeave(
id = 21 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00671140939597316636 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00671140939597316636 ;
weighted_squared_targets_sum = 0.00671140939597316636 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *84 ->RegressionTreeLeave(
id = 22 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 66 ;
weights_sum = 0.442953020134227771 ;
targets_sum = 64 ;
weighted_targets_sum = 0.429530201342281481 ;
weighted_squared_targets_sum = 0.429530201342281481 ;
loss_function_factor = 2  )
 )
;
right_leave = *81   )
;
right_leave = *65   )
;
priority_queue = *85 ->RegressionTreeQueue(
verbosity = 2 ;
maximum_number_of_nodes = 5 ;
next_available_node = 5 ;
nodes = 5 [ *75  *80  *54  *70  *59  ]  )
;
first_leave = *49  ;
split_cols = 4 [ 1 2 4 4 ] ;
split_values = 4 [ 0.482293993618237549 0.885239426681956321 0.999999999538717876 0.725372806051693186 ] ;
random_gen = *0 ;
seed = 1827 ;
stage = 5 ;
n_examples = 149 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 5 ;
report_progress = 0 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
*86 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 5 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 2 [ 0 1 ] ;
leave_template = *87 ->RegressionTreeLeave(
id = -1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
root = *88 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *89 ->RegressionTreeLeave(
id = 1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 173 ;
weights_sum = 1.00000000000000422 ;
targets_sum = 91 ;
weighted_targets_sum = 0.526011560693641633 ;
weighted_squared_targets_sum = 0.526011560693641633 ;
loss_function_factor = 2  )
;
leave = *89  ;
leave_output = 2 [ 0.526011560693639413 1 ] ;
leave_error = 3 [ 0.498646797420564281 0 0.498646797420564281 ] ;
split_col = 2 ;
split_balance = 99 ;
split_feature_value = 0.995245802370935517 ;
after_split_error = 0.376402584155050013 ;
missing_node = *0 ;
missing_leave = *90 ->RegressionTreeLeave(
id = 2 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *91 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *92 ->RegressionTreeLeave(
id = 3 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 136 ;
weights_sum = 0.786127167630060186 ;
targets_sum = 54 ;
weighted_targets_sum = 0.312138728323699322 ;
weighted_squared_targets_sum = 0.312138728323699322 ;
loss_function_factor = 2  )
;
leave = *92  ;
leave_output = 2 [ 0.39705882352941041 1 ] ;
leave_error = 3 [ 0.376402584155050013 0 0.376402584155050013 ] ;
split_col = 2 ;
split_balance = 82 ;
split_feature_value = 0.994286124455373455 ;
after_split_error = 0.31500238638171546 ;
missing_node = *0 ;
missing_leave = *93 ->RegressionTreeLeave(
id = 5 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *94 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *95 ->RegressionTreeLeave(
id = 6 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 109 ;
weights_sum = 0.630057803468209054 ;
targets_sum = 54 ;
weighted_targets_sum = 0.312138728323699322 ;
weighted_squared_targets_sum = 0.312138728323699322 ;
loss_function_factor = 2  )
;
leave = *95  ;
leave_output = 2 [ 0.495412844036696332 1 ] ;
leave_error = 3 [ 0.31500238638171546 0 0.31500238638171546 ] ;
split_col = 4 ;
split_balance = 69 ;
split_feature_value = 8.32667268468867405e-16 ;
after_split_error = 0.24550237059167368 ;
missing_node = *0 ;
missing_leave = *96 ->RegressionTreeLeave(
id = 11 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *97 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *98 ->RegressionTreeLeave(
id = 12 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 20 ;
weights_sum = 0.115606936416184927 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
leave = *98  ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 3 ;
split_balance = 0 ;
split_feature_value = 0.180062798802320762 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *99 ->RegressionTreeLeave(
id = 17 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *100 ->RegressionTreeLeave(
id = 18 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00578034682080924306 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *101 ->RegressionTreeLeave(
id = 19 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 19 ;
weights_sum = 0.109826589595375682 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
 )
;
left_leave = *98  ;
right_node = *102 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *103 ->RegressionTreeLeave(
id = 13 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 89 ;
weights_sum = 0.514450867052023031 ;
targets_sum = 54 ;
weighted_targets_sum = 0.312138728323699322 ;
weighted_squared_targets_sum = 0.312138728323699322 ;
loss_function_factor = 2  )
;
leave = *103  ;
leave_output = 2 [ 0.606741573033707793 1 ] ;
leave_error = 3 [ 0.24550237059167368 0 0.24550237059167368 ] ;
split_col = 0 ;
split_balance = 59 ;
split_feature_value = 0.242853945270868343 ;
after_split_error = 0.19876061032130396 ;
missing_node = *0 ;
missing_leave = *104 ->RegressionTreeLeave(
id = 20 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *105 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *106 ->RegressionTreeLeave(
id = 21 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 15 ;
weights_sum = 0.0867052023121386989 ;
targets_sum = 2 ;
weighted_targets_sum = 0.0115606936416184965 ;
weighted_squared_targets_sum = 0.0115606936416184965 ;
loss_function_factor = 2  )
;
leave = *106  ;
leave_output = 2 [ 0.133333333333333359 1 ] ;
leave_error = 3 [ 0.0200385356454720609 0 0.0200385356454720609 ] ;
split_col = 1 ;
split_balance = 13 ;
split_feature_value = 0.689386369193649928 ;
after_split_error = 0.0107349298100743173 ;
missing_node = *0 ;
missing_leave = *107 ->RegressionTreeLeave(
id = 23 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *108 ->RegressionTreeLeave(
id = 24 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00578034682080924306 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *109 ->RegressionTreeLeave(
id = 25 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 14 ;
weights_sum = 0.0809248554913294532 ;
targets_sum = 2 ;
weighted_targets_sum = 0.0115606936416184965 ;
weighted_squared_targets_sum = 0.0115606936416184965 ;
loss_function_factor = 2  )
 )
;
left_leave = *106  ;
right_node = *110 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *111 ->RegressionTreeLeave(
id = 22 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 74 ;
weights_sum = 0.427745664739884235 ;
targets_sum = 52 ;
weighted_targets_sum = 0.30057803468208083 ;
weighted_squared_targets_sum = 0.30057803468208083 ;
loss_function_factor = 2  )
;
leave = *111  ;
leave_output = 2 [ 0.702702702702702742 1 ] ;
leave_error = 3 [ 0.178722074675831843 0 0.178722074675831843 ] ;
split_col = 2 ;
split_balance = 52 ;
split_feature_value = 0.885239426681956321 ;
after_split_error = 0.165519772456188596 ;
missing_node = *0 ;
missing_leave = *112 ->RegressionTreeLeave(
id = 26 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *113 ->RegressionTreeLeave(
id = 27 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.0057803468208092847 ;
targets_sum = 1 ;
weighted_targets_sum = 0.0057803468208092847 ;
weighted_squared_targets_sum = 0.0057803468208092847 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *114 ->RegressionTreeLeave(
id = 28 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 73 ;
weights_sum = 0.421965317919074989 ;
targets_sum = 51 ;
weighted_targets_sum = 0.294797687861271585 ;
weighted_squared_targets_sum = 0.294797687861271585 ;
loss_function_factor = 2  )
 )
;
right_leave = *111   )
;
right_leave = *103   )
;
left_leave = *95  ;
right_node = *115 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *116 ->RegressionTreeLeave(
id = 7 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 27 ;
weights_sum = 0.156069364161849661 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
leave = *116  ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = -1 ;
split_balance = 2147483647 ;
split_feature_value = 1.79769313486231571e+308 ;
after_split_error = 1.79769313486231571e+308 ;
missing_node = *0 ;
missing_leave = *117 ->RegressionTreeLeave(
id = 14 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *118 ->RegressionTreeLeave(
id = 15 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00578034682080925694 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *119 ->RegressionTreeLeave(
id = 16 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 26 ;
weights_sum = 0.150289017341040415 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
 )
;
right_leave = *116   )
;
left_leave = *92  ;
right_node = *120 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *121 ->RegressionTreeLeave(
id = 4 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 37 ;
weights_sum = 0.213872832369942117 ;
targets_sum = 37 ;
weighted_targets_sum = 0.213872832369942117 ;
weighted_squared_targets_sum = 0.213872832369942117 ;
loss_function_factor = 2  )
;
leave = *121  ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 3 ;
split_balance = 1 ;
split_feature_value = 0.893509913423101043 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *122 ->RegressionTreeLeave(
id = 8 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *123 ->RegressionTreeLeave(
id = 9 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00578034682080925694 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00578034682080925694 ;
weighted_squared_targets_sum = 0.00578034682080925694 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *124 ->RegressionTreeLeave(
id = 10 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 36 ;
weights_sum = 0.208092485549132872 ;
targets_sum = 36 ;
weighted_targets_sum = 0.208092485549132872 ;
weighted_squared_targets_sum = 0.208092485549132872 ;
loss_function_factor = 2  )
 )
;
right_leave = *121   )
;
priority_queue = *125 ->RegressionTreeQueue(
verbosity = 2 ;
maximum_number_of_nodes = 5 ;
next_available_node = 4 ;
nodes = 5 [ *110  *105  *97  *120  *0 ]  )
;
first_leave = *89  ;
split_cols = 4 [ 2 2 4 0 ] ;
split_values = 4 [ 0.995245802370935517 0.994286124455373455 8.32667268468867405e-16 0.242853945270868343 ] ;
random_gen = *0 ;
seed = 1827 ;
stage = 5 ;
n_examples = 173 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 5 ;
report_progress = 0 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
*126 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 5 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 2 [ 0 1 ] ;
leave_template = *127 ->RegressionTreeLeave(
id = -1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
root = *128 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *129 ->RegressionTreeLeave(
id = 1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 149 ;
weights_sum = 0.999999999999998779 ;
targets_sum = 58 ;
weighted_targets_sum = 0.389261744966442613 ;
weighted_squared_targets_sum = 0.389261744966442613 ;
loss_function_factor = 2  )
;
leave = *129  ;
leave_output = 2 [ 0.389261744966443113 1 ] ;
leave_error = 3 [ 0.475474077744245216 0 0.475474077744245216 ] ;
split_col = 1 ;
split_balance = 67 ;
split_feature_value = 0.575210824891620121 ;
after_split_error = 0.29407734793231588 ;
missing_node = *0 ;
missing_leave = *130 ->RegressionTreeLeave(
id = 2 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *131 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *132 ->RegressionTreeLeave(
id = 3 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 108 ;
weights_sum = 0.724832214765099847 ;
targets_sum = 22 ;
weighted_targets_sum = 0.147651006711409377 ;
weighted_squared_targets_sum = 0.147651006711409377 ;
loss_function_factor = 2  )
;
leave = *132  ;
leave_output = 2 [ 0.203703703703703914 1 ] ;
leave_error = 3 [ 0.235147899577429681 0 0.235147899577429681 ] ;
split_col = 3 ;
split_balance = 96 ;
split_feature_value = 0.861099804177139827 ;
after_split_error = 0.181076457428609006 ;
missing_node = *0 ;
missing_leave = *133 ->RegressionTreeLeave(
id = 5 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *134 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *135 ->RegressionTreeLeave(
id = 6 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 102 ;
weights_sum = 0.684563758389260979 ;
targets_sum = 16 ;
weighted_targets_sum = 0.107382550335570481 ;
weighted_squared_targets_sum = 0.107382550335570481 ;
loss_function_factor = 2  )
;
leave = *135  ;
leave_output = 2 [ 0.156862745098039408 1 ] ;
leave_error = 3 [ 0.181076457428609006 0 0.181076457428609006 ] ;
split_col = 0 ;
split_balance = 6 ;
split_feature_value = 0.591195353843563365 ;
after_split_error = 0.158960974397215959 ;
missing_node = *0 ;
missing_leave = *136 ->RegressionTreeLeave(
id = 11 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *137 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *138 ->RegressionTreeLeave(
id = 12 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 48 ;
weights_sum = 0.322147651006711166 ;
targets_sum = 14 ;
weighted_targets_sum = 0.0939597315436241642 ;
weighted_squared_targets_sum = 0.0939597315436241642 ;
loss_function_factor = 2  )
;
leave = *138  ;
leave_output = 2 [ 0.291666666666666907 1 ] ;
leave_error = 3 [ 0.133109619686800851 0 0.133109619686800851 ] ;
split_col = 3 ;
split_balance = 12 ;
split_feature_value = 0.187798288368589333 ;
after_split_error = 0.111558538404175941 ;
missing_node = *0 ;
missing_leave = *139 ->RegressionTreeLeave(
id = 23 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *140 ->RegressionTreeLeave(
id = 24 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00671140939597316636 ;
targets_sum = 0 ;
weighted_targets_sum = -1.73472347597680709e-18 ;
weighted_squared_targets_sum = -1.73472347597680709e-18 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *141 ->RegressionTreeLeave(
id = 25 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 47 ;
weights_sum = 0.315436241610738022 ;
targets_sum = 14 ;
weighted_targets_sum = 0.0939597315436241642 ;
weighted_squared_targets_sum = 0.0939597315436241642 ;
loss_function_factor = 2  )
 )
;
left_leave = *138  ;
right_node = *142 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *143 ->RegressionTreeLeave(
id = 13 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 54 ;
weights_sum = 0.362416107382550035 ;
targets_sum = 2 ;
weighted_targets_sum = 0.0134228187919463084 ;
weighted_squared_targets_sum = 0.0134228187919463084 ;
loss_function_factor = 2  )
;
leave = *143  ;
leave_output = 2 [ 0.0370370370370370697 1 ] ;
leave_error = 3 [ 0.0258513547104151122 0 0.0258513547104151122 ] ;
split_col = 1 ;
split_balance = 52 ;
split_feature_value = 0.569140400436275673 ;
after_split_error = 0.0131695580600227936 ;
missing_node = *0 ;
missing_leave = *144 ->RegressionTreeLeave(
id = 26 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *145 ->RegressionTreeLeave(
id = 27 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00671140939597316636 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *146 ->RegressionTreeLeave(
id = 28 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 53 ;
weights_sum = 0.35570469798657689 ;
targets_sum = 2 ;
weighted_targets_sum = 0.0134228187919463084 ;
weighted_squared_targets_sum = 0.0134228187919463084 ;
loss_function_factor = 2  )
 )
;
right_leave = *143   )
;
left_leave = *135  ;
right_node = *147 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *148 ->RegressionTreeLeave(
id = 7 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 6 ;
weights_sum = 0.0402684563758389236 ;
targets_sum = 6 ;
weighted_targets_sum = 0.0402684563758389236 ;
weighted_squared_targets_sum = 0.0402684563758389236 ;
loss_function_factor = 2  )
;
leave = *148  ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 4 ;
split_balance = 0 ;
split_feature_value = 0.999999941905052481 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *149 ->RegressionTreeLeave(
id = 14 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *150 ->RegressionTreeLeave(
id = 15 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00671140939597315248 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00671140939597315248 ;
weighted_squared_targets_sum = 0.00671140939597315248 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *151 ->RegressionTreeLeave(
id = 16 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 5 ;
weights_sum = 0.033557046979865772 ;
targets_sum = 5 ;
weighted_targets_sum = 0.033557046979865772 ;
weighted_squared_targets_sum = 0.033557046979865772 ;
loss_function_factor = 2  )
 )
;
right_leave = *148   )
;
left_leave = *132  ;
right_node = *152 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *153 ->RegressionTreeLeave(
id = 4 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 41 ;
weights_sum = 0.275167785234899154 ;
targets_sum = 36 ;
weighted_targets_sum = 0.241610738255033403 ;
weighted_squared_targets_sum = 0.241610738255033403 ;
loss_function_factor = 2  )
;
leave = *153  ;
leave_output = 2 [ 0.878048780487804881 1 ] ;
leave_error = 3 [ 0.0589294483548861714 0 0.0589294483548861714 ] ;
split_col = 3 ;
split_balance = 33 ;
split_feature_value = 0.0434674056274751697 ;
after_split_error = 0.0130600399056774452 ;
missing_node = *0 ;
missing_leave = *154 ->RegressionTreeLeave(
id = 8 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *155 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *156 ->RegressionTreeLeave(
id = 9 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 4 ;
weights_sum = 0.0268456375838926169 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
leave = *156  ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = -1 ;
split_balance = 2147483647 ;
split_feature_value = 1.79769313486231571e+308 ;
after_split_error = 1.79769313486231571e+308 ;
missing_node = *0 ;
missing_leave = *157 ->RegressionTreeLeave(
id = 17 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *158 ->RegressionTreeLeave(
id = 18 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00671140939597315248 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *159 ->RegressionTreeLeave(
id = 19 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 3 ;
weights_sum = 0.0201342281879194618 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
 )
;
left_leave = *156  ;
right_node = *160 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *161 ->RegressionTreeLeave(
id = 10 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 37 ;
weights_sum = 0.248322147651006547 ;
targets_sum = 36 ;
weighted_targets_sum = 0.241610738255033403 ;
weighted_squared_targets_sum = 0.241610738255033403 ;
loss_function_factor = 2  )
;
leave = *161  ;
leave_output = 2 [ 0.972972972972973027 1 ] ;
leave_error = 3 [ 0.0130600399056774452 0 0.0130600399056774452 ] ;
split_col = 1 ;
split_balance = 13 ;
split_feature_value = 0.689386369193649928 ;
after_split_error = 0.0123042505592841078 ;
missing_node = *0 ;
missing_leave = *162 ->RegressionTreeLeave(
id = 20 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *163 ->RegressionTreeLeave(
id = 21 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.0067114093959731386 ;
targets_sum = 1 ;
weighted_targets_sum = 0.0067114093959731386 ;
weighted_squared_targets_sum = 0.0067114093959731386 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *164 ->RegressionTreeLeave(
id = 22 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 36 ;
weights_sum = 0.241610738255033403 ;
targets_sum = 35 ;
weighted_targets_sum = 0.234899328859060258 ;
weighted_squared_targets_sum = 0.234899328859060258 ;
loss_function_factor = 2  )
 )
;
right_leave = *161   )
;
right_leave = *153   )
;
priority_queue = *165 ->RegressionTreeQueue(
verbosity = 2 ;
maximum_number_of_nodes = 5 ;
next_available_node = 4 ;
nodes = 5 [ *137  *142  *160  *147  *0 ]  )
;
first_leave = *129  ;
split_cols = 4 [ 1 3 3 0 ] ;
split_values = 4 [ 0.575210824891620121 0.861099804177139827 0.0434674056274751697 0.591195353843563365 ] ;
random_gen = *0 ;
seed = 1827 ;
stage = 5 ;
n_examples = 149 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 5 ;
report_progress = 0 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
*166 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 5 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 2 [ 0 1 ] ;
leave_template = *167 ->RegressionTreeLeave(
id = -1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
root = *168 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *169 ->RegressionTreeLeave(
id = 1 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 148 ;
weights_sum = 1.00000000000000222 ;
targets_sum = 95 ;
weighted_targets_sum = 0.641891891891891997 ;
weighted_squared_targets_sum = 0.641891891891891997 ;
loss_function_factor = 2  )
;
leave = *169  ;
leave_output = 2 [ 0.641891891891890554 1 ] ;
leave_error = 3 [ 0.459733382030681148 0 0.459733382030681148 ] ;
split_col = 2 ;
split_balance = 138 ;
split_feature_value = 0.00132212183813046336 ;
after_split_error = 0.430920430920432751 ;
missing_node = *0 ;
missing_leave = *170 ->RegressionTreeLeave(
id = 2 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *171 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *172 ->RegressionTreeLeave(
id = 3 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 5 ;
weights_sum = 0.0337837837837837857 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
leave = *172  ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 3 ;
split_balance = 1 ;
split_feature_value = 0.0480606101777627803 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *173 ->RegressionTreeLeave(
id = 5 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *174 ->RegressionTreeLeave(
id = 6 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00675675675675675713 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *175 ->RegressionTreeLeave(
id = 7 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 4 ;
weights_sum = 0.0270270270270270285 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
 )
;
left_leave = *172  ;
right_node = *176 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *177 ->RegressionTreeLeave(
id = 4 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 143 ;
weights_sum = 0.966216216216218338 ;
targets_sum = 95 ;
weighted_targets_sum = 0.641891891891891997 ;
weighted_squared_targets_sum = 0.641891891891891997 ;
loss_function_factor = 2  )
;
leave = *177  ;
leave_output = 2 [ 0.664335664335662934 1 ] ;
leave_error = 3 [ 0.430920430920432751 0 0.430920430920432751 ] ;
split_col = 2 ;
split_balance = 95 ;
split_feature_value = 0.0173550052261667587 ;
after_split_error = 0.387008857597093914 ;
missing_node = *0 ;
missing_leave = *178 ->RegressionTreeLeave(
id = 8 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *179 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *180 ->RegressionTreeLeave(
id = 9 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 24 ;
weights_sum = 0.162162162162162116 ;
targets_sum = 24 ;
weighted_targets_sum = 0.162162162162162116 ;
weighted_squared_targets_sum = 0.162162162162162116 ;
loss_function_factor = 2  )
;
leave = *180  ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 3 ;
split_balance = 4 ;
split_feature_value = 0.135512975844814643 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *181 ->RegressionTreeLeave(
id = 11 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *182 ->RegressionTreeLeave(
id = 12 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00675675675675677101 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00675675675675677101 ;
weighted_squared_targets_sum = 0.00675675675675677101 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *183 ->RegressionTreeLeave(
id = 13 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 23 ;
weights_sum = 0.155405405405405372 ;
targets_sum = 23 ;
weighted_targets_sum = 0.155405405405405372 ;
weighted_squared_targets_sum = 0.155405405405405372 ;
loss_function_factor = 2  )
 )
;
left_leave = *180  ;
right_node = *184 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *185 ->RegressionTreeLeave(
id = 10 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 119 ;
weights_sum = 0.804054054054055167 ;
targets_sum = 71 ;
weighted_targets_sum = 0.479729729729729049 ;
weighted_squared_targets_sum = 0.479729729729729049 ;
loss_function_factor = 2  )
;
leave = *185  ;
leave_output = 2 [ 0.596638655462183198 1 ] ;
leave_error = 3 [ 0.387008857597093914 0 0.387008857597093914 ] ;
split_col = 2 ;
split_balance = 105 ;
split_feature_value = 0.0689879291310910303 ;
after_split_error = 0.351230694980696034 ;
missing_node = *0 ;
missing_leave = *186 ->RegressionTreeLeave(
id = 14 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *187 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *188 ->RegressionTreeLeave(
id = 15 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 7 ;
weights_sum = 0.0472972972972972999 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
leave = *188  ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 4 ;
split_balance = 3 ;
split_feature_value = 0.510088881577538289 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *189 ->RegressionTreeLeave(
id = 17 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *190 ->RegressionTreeLeave(
id = 18 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00675675675675675713 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *191 ->RegressionTreeLeave(
id = 19 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 6 ;
weights_sum = 0.0405405405405405428 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
 )
;
left_leave = *188  ;
right_node = *192 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *193 ->RegressionTreeLeave(
id = 16 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 112 ;
weights_sum = 0.756756756756757576 ;
targets_sum = 71 ;
weighted_targets_sum = 0.479729729729729049 ;
weighted_squared_targets_sum = 0.479729729729729049 ;
loss_function_factor = 2  )
;
leave = *193  ;
leave_output = 2 [ 0.633928571428569843 1 ] ;
leave_error = 3 [ 0.351230694980696034 0 0.351230694980696034 ] ;
split_col = 3 ;
split_balance = 78 ;
split_feature_value = 0.141930657011306749 ;
after_split_error = 0.314935988620199336 ;
missing_node = *0 ;
missing_leave = *194 ->RegressionTreeLeave(
id = 20 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *195 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *196 ->RegressionTreeLeave(
id = 21 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 17 ;
weights_sum = 0.114864864864864871 ;
targets_sum = 17 ;
weighted_targets_sum = 0.114864864864864871 ;
weighted_squared_targets_sum = 0.114864864864864871 ;
loss_function_factor = 2  )
;
leave = *196  ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 2 ;
split_balance = 1 ;
split_feature_value = 0.122353510242232788 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *197 ->RegressionTreeLeave(
id = 23 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *198 ->RegressionTreeLeave(
id = 24 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00675675675675675713 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00675675675675675713 ;
weighted_squared_targets_sum = 0.00675675675675675713 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *199 ->RegressionTreeLeave(
id = 25 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 16 ;
weights_sum = 0.108108108108108114 ;
targets_sum = 16 ;
weighted_targets_sum = 0.108108108108108114 ;
weighted_squared_targets_sum = 0.108108108108108114 ;
loss_function_factor = 2  )
 )
;
left_leave = *196  ;
right_node = *200 ->RegressionTreeNode(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
leave_template = *201 ->RegressionTreeLeave(
id = 22 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 95 ;
weights_sum = 0.641891891891891997 ;
targets_sum = 54 ;
weighted_targets_sum = 0.364864864864864413 ;
weighted_squared_targets_sum = 0.364864864864864413 ;
loss_function_factor = 2  )
;
leave = *201  ;
leave_output = 2 [ 0.568421052631578161 1 ] ;
leave_error = 3 [ 0.314935988620199336 0 0.314935988620199336 ] ;
split_col = 3 ;
split_balance = 69 ;
split_feature_value = 0.16348885181551448 ;
after_split_error = 0.249176005273566148 ;
missing_node = *0 ;
missing_leave = *202 ->RegressionTreeLeave(
id = 26 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2  )
;
left_node = *0 ;
left_leave = *203 ->RegressionTreeLeave(
id = 27 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 1 ;
weights_sum = 0.00675675675675677101 ;
targets_sum = 1 ;
weighted_targets_sum = 0.00675675675675677101 ;
weighted_squared_targets_sum = 0.00675675675675677101 ;
loss_function_factor = 2  )
;
right_node = *0 ;
right_leave = *204 ->RegressionTreeLeave(
id = 28 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 94 ;
weights_sum = 0.635135135135135198 ;
targets_sum = 53 ;
weighted_targets_sum = 0.35810810810810767 ;
weighted_squared_targets_sum = 0.35810810810810767 ;
loss_function_factor = 2  )
 )
;
right_leave = *201   )
;
right_leave = *193   )
;
right_leave = *185   )
;
right_leave = *177   )
;
priority_queue = *205 ->RegressionTreeQueue(
verbosity = 2 ;
maximum_number_of_nodes = 5 ;
next_available_node = 5 ;
nodes = 5 [ *200  *171  *187  *179  *195  ]  )
;
first_leave = *169  ;
split_cols = 4 [ 2 2 2 3 ] ;
split_values = 4 [ 0.00132212183813046336 0.0173550052261667587 0.0689879291310910303 0.141930657011306749 ] ;
random_gen = *0 ;
seed = 1827 ;
stage = 5 ;
n_examples = 148 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 5 ;
report_progress = 0 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
] ;
voting_weights = 5 [ 1.13679877806039675 1.1515814803392157 0.711694555952663288 0.924853007982333275 0.41501925503263265 ] ;
sum_voting_weights = 4.33994707736724195 ;
initial_sum_weights = 1 ;
example_weights = 150 [ 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00506327259806789703 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0137564885108083946 0.000521219238036400986 0.00496216976946179701 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.116188573346790697 0.0331580145383834832 0.000521219238036400986 0.00331388591284825871 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0760448245655967192 0.00119536787918377106 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00331388591284825871 0.00119536787918377106 0.000521219238036400986 0.0331580145383834832 0.000521219238036400986 0.000521219238036400986 0.00506327259806789703 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00119536787918377106 0.00119536787918377106 0.000521219238036400986 0.000521219238036400986 0.00496216976946179701 0.00216366726200823561 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0331580145383834832 0.00216366726200823561 0.000521219238036400986 0.00119536787918377106 0.000521219238036400986 0.000521219238036400986 0.0321920345819545276 0.0482039349033431888 0.000521219238036400986 0.00216366726200823561 0.0321920345819545276 0.021649179367261618 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0331580145383834832 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00216366726200823561 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00331388591284825871 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.0210184819737942973 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00331388591284825871 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.00331388591284825871 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00216366726200823561 0.00506327259806789703 0.0210184819737942973 0.0331580145383834832 0.0321920345819545276 0.0496503807568692487 0.000521219238036400986 0.00521520520832954425 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0331580145383834832 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00119536787918377106 0.000521219238036400986 ] ;
learners_error = 5 [ 0.0933333333333333376 0.0908613445378150392 0.194130827514584187 0.135907417796919922 0.303636927486942487 ] ;
weak_learner_template = *206 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 5 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 2 [ 0 1 ] ;
leave_template = *207 ->RegressionTreeLeave(
id = -1 ;
missing_leave = 0 ;
loss_function_weight = 0 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 1  )
;
root = *0 ;
priority_queue = *0 ;
first_leave = *0 ;
split_cols = []
;
split_values = []
;
random_gen = *0 ;
seed = 1827 ;
stage = 0 ;
n_examples = 200 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 5 ;
report_progress = 0 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
;
target_error = 0.5 ;
pseudo_loss_adaboost = 0 ;
conf_rated_adaboost = 0 ;
weight_by_resampling = 1 ;
output_threshold = 0.5 ;
provide_learner_expdir = 1 ;
early_stopping = 1 ;
save_often = 1 ;
compute_training_error = 1 ;
forward_sub_learner_test_costs = 1 ;
modif_train_set_weights = 0 ;
found_zero_error_weak_learner = 0 ;
reuse_test_results = 1 ;
random_gen = *0 ;
seed = 1827 ;
stage = 5 ;
n_examples = 150 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 5 ;
report_progress = 0 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
;
perf_evaluators = {};
report_stats = 1 ;
save_initial_tester = 0 ;
save_stat_collectors = 1 ;
save_split_stats = 1 ;
save_learners = 0 ;
save_initial_learners = 0 ;
save_data_sets = 0 ;
save_test_outputs = 0 ;
call_forget_in_run = 1 ;
save_test_costs = 0 ;
save_test_names = 0 ;
provide_learner_expdir = 1 ;
should_train = 1 ;
should_test = 1 ;
template_stats_collector = *0 ;
global_template_stats_collector = *0 ;
final_commands = []
;
save_test_confidence = 0 ;
enforce_clean_expdir = 1 ;
redirect_stdout = 0 ;
redirect_stderr = 0  )
;
option_fields = 1 [ "nstages" ] ;
dont_restart_upon_change = 1 [ "nstages" ] ;
strategy = 1 [ *208 ->HyperOptimize(
which_cost = "E[test2.E[class_error]]" ;
min_n_trials = 0 ;
oracle = *209 ->EarlyStoppingOracle(
option = "nstages" ;
values = []
;
range = 3 [ 1 11 2 ] ;
min_value = -3.40282000000000014e+38 ;
max_value = 3.40282000000000014e+38 ;
max_degradation = 3.40282000000000014e+38 ;
relative_max_degradation = -1 ;
min_improvement = -3.40282000000000014e+38 ;
relative_min_improvement = -1 ;
max_degraded_steps = 120 ;
min_n_steps = 2 ;
nreturned = 5 ;
best_objective = 0.140000000000000013 ;
best_step = 3 ;
met_early_stopping = 0  )
;
provide_tester_expdir = 0 ;
sub_strategy = []
;
rerun_after_sub = 0 ;
provide_sub_expdir = 1 ;
save_best_learner = 0 ;
splitter = *0 ;
auto_save = 0 ;
auto_save_diff_time = 10800 ;
auto_save_test = 0 ;
best_objective = 0.140000000000000013 ;
best_results = 6 [ 0.0533333333333333368 0.166752838955117694 0.0533333333333333368 0.140000000000000013 3.62727311623549786 0.140000000000000013 ] ;
best_learner = *5  ;
trialnum = 5 ;
option_vals = []
;
verbosity = 0  )
] ;
provide_strategy_expdir = 1 ;
save_final_learner = 0 ;
learner = *5  ;
provide_learner_expdir = 1 ;
expdir_append = "" ;
forward_nstages = 0 ;
random_gen = *0 ;
stage = 1 ;
n_examples = 200 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
