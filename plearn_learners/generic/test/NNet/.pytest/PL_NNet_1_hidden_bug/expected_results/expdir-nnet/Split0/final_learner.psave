*1 ->NNet(
nhidden = 1 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 0 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *2 ->GradientOptimizer(
start_learning_rate = 0.00100000000000000002 ;
learning_rate = 0.000870322019147084396 ;
decrease_constant = 0.00100000000000000002 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 0 ;
nstages = 15 ;
early_stop = 0  )
;
batch_size = 10 ;
initialization_method = "uniform_linear" ;
operate_on_bags = 0 ;
max_bag_size = 20 ;
paramsvalues = 9 [ -0.557025969227852968 1.14928163614964474 -0.45781959698541691 -2.16987919303079169 1.08539803737234242 3.51007804148783187 4.0760403120233013 -18.5217581493060095 -10.6940446511284577 ] ;
random_gen = *3 ->PRandom(
seed = 1827 ;
fixed_seed = 0  )
;
seed = 1827 ;
stage = 10 ;
n_examples = 150 ;
inputsize = 4 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 10 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
