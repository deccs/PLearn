*1 ->NNet(
nhidden = 10 ;
nhidden2 = 0 ;
noutputs = 3 ;
weight_decay = 0 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "softmax" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "NLL" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *2 ->GradientOptimizer(
start_learning_rate = 0.100000000000000006 ;
learning_rate = 0.100000000000000006 ;
decrease_constant = 0 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 0 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
operate_on_bags = 1 ;
max_bag_size = 20 ;
paramsvalues = 73 [ -0.264371813933822586 -0.205331393475229718 0.294763050648583658 0.105289000631594784 -0.0600026001346709026 -0.0127392290430181788 0.185606746681020646 -0.146029102499062757 -0.035236628856851554 0.494463642020458116 0.393139162922157281 0.440366736932692793 -0.267097090638493184 0.145480678130172292 0.380941902198827065 -0.107676958597727371 -0.408938431273406844 0.177410084672891211 0.0851483841469526936 -0.385202883323724099 0.248204980519073803 0.0796150189345870285 -0.048167412239538715 -0.570596016554755447 -0.326360881269845637 0.0034805846719268012 0.227583204665000316 0.39565382264075305 -0.135264412864009614 -0.341423445754042321 0.191578842355264789 0.189870922901048761 -0.487345364775797374 0.0139964824590312533 0.142618001875924688 0.0938651429187691011 -0.383816497606948581 0.135487639409365007 0.251265360744742317 -0.382071708616923356 0.420885891542839952 0.00468512346816874967 -0.425571015011008635 -0.470708339942575216 0.00854415616643394021 0.298523787303021149 -0.311283749677584431 -0.135230854662720279 0.327952925466075185 0.358211698449398486 0.0436491958188225671 -0.504830864988569328 0.366310796329157806 -0.285490748564436347 -0.0400430791403840053 0.0343099477420112861 -0.34074858124809787 0.266057385700989746 -0.0659685760370418844 -0.0248485353448619009 -0.041971485702691845 0.195540425483182012 0.356120514577813407 -0.364622987168045753 -0.329868801987559246 0.178574560024803763 0.184561957317535025 -0.0293825910408284166 -0.173932379519760538 0.136426789459912712 0.570971748588152583 -0.0287276061742493087 -0.565704503389098168 ] ;
random_gen = *3 ->PRandom(
seed = 1827 ;
fixed_seed = 0  )
;
seed = 1827 ;
stage = 100 ;
n_examples = 6 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 100 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
