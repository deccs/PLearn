*1 ->BestAveragingPLearner(
learner_set = 5 [ *2 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *3 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ 0 0 0 0 -0.276083135201285246 0.000628164038062095642 -0.161594042244056851 -0.0258298339322209358 0.09975709145267804 0.101665338346113757 0.279126112194110931 0.314489404515673698 -0.0784308677539229393 0.246178180755426468 0.212050157599151134 -0.125624475069344044 0 0 0.0159477593842893839 -0.21886214823462069 0.0567027026554569602 0.0378728508949279785 0.0918025322025641799 -0.229660577955655754 -0.182542695780284703 -0.0285033329855650663 ] ;
seed = 1827 ;
stage = 0 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
*4 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *5 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ 0 0 0 0 0.332082761917263269 -0.216855328995734453 0.225814314248661191 0.0206476043288906404 0.286345914161453607 -0.27739399392157793 -0.182301338762044907 -0.199115862293789775 0.226191021967679262 -0.195542693914224686 0.148360295531650366 0.0799044688853124685 0 0 0.207922605215571821 0.249214834533631802 -0.125758215552195907 0.00108542991802096367 0.0164207242196425796 -0.0410197372548282146 -0.056742330314591527 0.0186359098879620433 ] ;
seed = 1828 ;
stage = 0 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
*6 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *7 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ 0 0 0 0 -0.0607815507488946097 0.258646931188801887 -0.0337776142793397086 -0.0399772532594700608 -0.151965384216358246 0.0784005403208235807 -0.0634994438538948602 0.214404148825754703 -0.252247036124269131 -0.025987433735281229 -0.117954509798437357 -0.144001273593554885 0 0 0.113650301238521934 -0.0830892197554931045 -0.117668823688291013 -0.156504730344749987 0.0703048727009445429 -0.224688087124377489 0.030464105075225234 0.0836414688965305686 ] ;
seed = 1829 ;
stage = 0 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
*8 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *9 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ 0 0 0 0 -0.257127145770937204 0.123597188231845692 -0.0840492880282302651 -0.115906089389075831 -0.0595171502791345119 0.21413628679389754 0.0867879525758326054 -0.295612829116483511 0.00430996172750989538 0.231859636337806763 -0.158095088321715593 0.11199888478343685 0 0 0.194505533087067306 -0.177652329904958606 0.0442702313885092735 0.220478063798509538 0.205604228773154318 0.0427621717099100351 0.221780522959306836 -0.189306822954677045 ] ;
seed = 1830 ;
stage = 0 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
*10 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *11 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ 0 0 0 0 -0.333197114368279756 -0.0487533033204575333 0.0899945247607926435 0.0347961848601698875 -0.297753678945203604 0.248070735018700361 0.0909819815618296418 0.266569927645226301 0.31332038932790357 -0.315407697111368179 0.13512422563508153 0.0312370313331484795 0 0 0.0146310953423380852 0.168359951698221266 0.220265766722150147 -0.233983727404847741 -0.190856988308951259 -0.0483694846043363214 -0.0351611654041334987 -0.116782699013128877 ] ;
seed = 1831 ;
stage = 0 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
] ;
learner_template = *12 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = -1 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *13 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = TVec(0 0 *0 )
;
seed = -1 ;
stage = 0 ;
n_examples = -1 ;
inputsize = -1 ;
targetsize = -1 ;
weightsize = -1 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
;
initial_seed = 1827 ;
seed_option = "seed" ;
total_learner_num = 5 ;
best_learner_num = 3 ;
comparison_statspec = "E[mse]" ;
splitter = *0 ;
cached_outputsize = 2 ;
learner_train_costs = TVec(0 0 *0 )
;
best_learners = TVec(0 0 *0 )
;
seed = -1 ;
stage = 0 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
