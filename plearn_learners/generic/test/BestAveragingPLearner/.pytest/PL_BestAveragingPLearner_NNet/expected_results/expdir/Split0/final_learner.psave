*1 ->BestAveragingPLearner(
learner_set = 5 [ *2 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *3 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0.00952380952380952467 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ 0.804166815994146633 0.0566506791714854116 9.23808622545284663 -4.39150161999063027 1.16971491234982272 -3.03492998868570885 -3.309988958866946 3.88369371155202669 -0.174276436077498043 4.97460774970634123 3.49343350146023468 -4.36151805019528815 4.41052797368184102 33.7980477290239989 16.6132857972995218 -27.338260667761844 0.0252544998563663074 8.11806644076657058 0.110138006311963091 -1.23067858497714422 0.459275564088585941 24.0493222712824064 0.533936973307443274 25.5602011139596748 -0.510453053562392678 -22.8723023285869829 ] ;
seed = 1827 ;
stage = 501 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
*4 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *5 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0.00952380952380952467 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ 2.28816637423737435 -2.05403132838964497 0.748230560851474102 4.95021324576375576 -1.601089136906372 4.22874853491282199 0.266189259202020301 -24.7531137993058614 3.68127646990455482 -4.71356801135592285 1.26806991216676845 13.3805612684452786 24.509823811636938 -22.3295650979088087 14.8539964250476864 12.0078783315146342 0.0863808900242894057 4.69703581489150146 0.714198692074704322 10.4857867072237383 -0.59594906367529743 -9.2330828155731286 0.571405239347905214 11.0933809913176251 -0.366599611081321552 49.053077931377743 ] ;
seed = 1828 ;
stage = 501 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
*6 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *7 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0.00952380952380952467 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ -0.751429457716492277 -11.3205917935710563 -1.7700909926613051 0.280265522055148097 2.56126789731232307 29.7482492935317495 1.64459168252889487 -1.46786842137166484 -2.6820078823529454 -15.7429935346434995 -4.77525369476942085 1.55393655430235844 -12.442492705938422 -10.792150523946102 -38.8438194574210272 6.11251714693839521 0.0813641994427079518 3.72150526549673266 -0.757870914388622974 -20.6919675306836872 0.288474179751696092 -50.1156658821951737 -0.877250337790254853 -22.5649694977646149 0.172996622192770733 -12.2488549433507128 ] ;
seed = 1829 ;
stage = 501 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
*8 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *9 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0.00952380952380952467 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ -7.77188467112115422 0.159282651587606794 -4.91752784436444212 -2.50309074276071009 8.62742986550929736 0.304981316157050486 0.933700598660339831 2.65927905858923008 -5.98965141696360082 2.68133563784246975 -1.18208704682760857 -3.61553803013968222 -14.8744138861087585 25.8109903525515811 -7.75818584352909379 -19.2115572549902502 0.0261237189616779948 7.97370428791812991 -0.176910655829009139 -16.5922101057944786 0.68025250960806205 20.1268970438538233 -0.512258181138601332 -19.7515126353668684 -0.242458563917799796 -14.8590757873720865 ] ;
seed = 1830 ;
stage = 501 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
*10 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = 2 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *11 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0.00952380952380952467 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = 26 [ 9.21563815113618112 -0.617158802571359688 4.93069655767867854 2.29964218670019749 -3.3457814858881223 1.75838072443673687 -3.0217598688384375 -0.123545384065396058 2.82110124954682862 -2.85944679135888924 2.94234958616009079 -5.7774098747684004 16.4390838311700271 -22.3550679607004241 13.0903319039676305 -61.2005334135852408 0.024243579091892304 8.10569856511560261 0.544633866033618741 21.5817518233376902 -0.207408898812266207 -14.5197691008767222 0.330459135007294402 18.6825253657234249 -0.531727752528320163 -16.5152438308757219 ] ;
seed = 1831 ;
stage = 501 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
] ;
learner_template = *12 ->NNet(
nhidden = 4 ;
nhidden2 = 0 ;
noutputs = -1 ;
weight_decay = 9.99999999999999955e-07 ;
bias_decay = 0 ;
layer1_weight_decay = 0 ;
layer1_bias_decay = 0 ;
layer2_weight_decay = 0 ;
layer2_bias_decay = 0 ;
output_layer_weight_decay = 0 ;
output_layer_bias_decay = 0 ;
direct_in_to_out_weight_decay = 0 ;
penalty_type = "L2_square" ;
L1_penalty = 0 ;
fixed_output_weights = 0 ;
input_reconstruction_penalty = 0 ;
direct_in_to_out = 0 ;
rbf_layer_size = 0 ;
first_class_is_junk = 1 ;
output_transfer_func = "none" ;
hidden_transfer_func = "tanh" ;
cost_funcs = 1 [ "mse" ] ;
classification_regularizer = 0 ;
first_hidden_layer = *0 ;
first_hidden_layer_is_output = 0 ;
n_non_params_in_first_hidden_layer = 0 ;
transpose_first_hidden_layer = 0 ;
margin = 1 ;
do_not_change_params = 0 ;
optimizer = *13 ->GradientOptimizer(
start_learning_rate = 0.0100000000000000002 ;
learning_rate = 0 ;
decrease_constant = 0.000100000000000000005 ;
lr_schedule = 0  0  [ 
]
;
use_stochastic_hack = 0 ;
verbosity = 100 ;
nstages = 1  )
;
batch_size = 0 ;
initialization_method = "uniform_linear" ;
paramsvalues = TVec(0 0 *0 )
;
seed = -1 ;
stage = 0 ;
n_examples = -1 ;
inputsize = -1 ;
targetsize = -1 ;
weightsize = -1 ;
forget_when_training_set_changes = 0 ;
nstages = 501 ;
report_progress = 1 ;
verbosity = 0 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
;
initial_seed = 1827 ;
seed_option = "seed" ;
total_learner_num = 5 ;
best_learner_num = 3 ;
comparison_statspec = "E[mse]" ;
splitter = *0 ;
cached_outputsize = 2 ;
learner_train_costs = 5 [ 3945.74140545367572 3066.887193867105 2961.53425766064447 3942.47111430447103 3944.55066491937259 ] ;
best_learners = 3 [ *6  *4  *8  ] ;
seed = -1 ;
stage = 0 ;
n_examples = 150 ;
inputsize = 3 ;
targetsize = 2 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = ""  )
