
                      Contrastive divergence multi-layer RBMs

Energy functions:
(Note that w_{ij} = w_{ji})

 * energy term for binomial unit i with value v_i and inputs u_j, parameters (b_i, w_{i.}): 
     
    b_i v_i + sum_j w_{ij} v_i u_j
   
       ==> P(v_i=1 | u) = exp(- b_i - sum_j w_{ij} u_j) / (1 + exp(- b_i - sum_j w_{ij} u_j)) = sigmoid(-b_i - sum_j w_{ij} u_j)
       NOTE THE MINUS

 * energy term for fixed-variance Gaussian unit i with value v_i and inputs u_j, parameters (a_i,b_i, w_{i,.}):

    b_i v_i + a_i^2 v_i^2 + sum_j w_{ij} v_i u_j
       ==> P(v_i | u) = (1/Z) exp(-(b_i v_i + a_i^2 v_i^2 + sum_j w_{ij} v_i u_j)) = (1/Z)(-0.5(v_i - mu)/sigma^2)
       ==> P(v_i | u) = N(v_i; mu, sigma^2) with sigma^2 = 0.5/a_i^2, mu = -0.5(b_i + sum_j w_{ij} u_j)/a_i^2
       NOTE HOW THESE BLOW UP WHEN a_i IS TOO SMALL. MAY WANT TO USE (a_i + epsilon)^2 INSTEAD, with epsilon fixed.


Basic operations in each RBM layer:

 * layer::upActivation: compute parameters of p(output_variables | input_variables)
 * layer::fprop(input,E[output|input]): find E(output_variables | input_variables). This ALWAYS calls upActivation.
 * layer::upSample(): sample output_variables from p(output_variables | input_variables). upActivation MUST have been called before.
 * layer::downActivation(): compute parameters of p(input_variables | output_variables)
 * layer::downExpectation(): find E(input_variables | output_variables). This ALWAYS calls downActivation.
 * layer::downSample(): sample input_variables from p(input_variables | output_variables). downActivation MUST have been called before.
 * layer::accumulateStats(in,out,1): accumulate positive phase statistics
 * layer::accumulateStats(in,out,-1): accumulate negative phase statistics
 * layer::layerCDupdate(): update parameters using positive and negative phase statistics
 * layer::outputMarginalSample(): sample from the marginal distribution of the output units, in output_variables
 
Combined operations, in a supervised RBM network:
 A supervised RBM network has a sequence of layers. The last_layer one is fully supervised. The 2nd to last is the last_hidden_layer.
 There is also a cost_layer that follows the last_layer, which computes the supervised cost to minimize.

 * network::computeRepresentation(input_variables):
     v <- input_variables
     for each layer except the last:
        layer->input_variables <- v.copy()
        v.resize(output_variables.size())
        layer->fprop(layer->input_variables,v)
     return v

 * network::unsupervised_learning_from_example(input_variables):
     v <- input_variables
     for each layer except the last:
        layer->input_variables <- v.copy()
        v.resize(output_variables.size())
        layer->fprop(layer->input_variables,v)
        layer->upSample() 
        layer->accumulateStats(v,output_variable)
        layer->downSample()
        layer->upSample()
        layer->accumulateStats(input_variable,output_variable)
        layer->CDupdate()

 * network::semisupervised_learning_from_example(input_variables, output_variables):
     v <- input_variables
     for each layer except the last:
        layer->input_variables <- v.copy()
        layer->fprop(layer->input_variables,layer->output)
        v <- layer->output
        layer->upSample() 
        layer->accumulateStats(v,output_variable)
        layer->downSample()
        layer->upSample()
        layer->accumulateStats(input_variable,output_variable)
        layer->CDupdate()
     last_layer->input_variables <- v.copy()
     last_layer->fprop(last_layer->input_variables,last_layer->output)
     cost_layer->fprop((output_variables,last_layer->output),cost)
     cost_layer->bpropUpdate((output_variables,last_layer->output),cost,(tmp, dout), 1)
     last_layer->bpropUpdate(v,out,dv,dout)
     for each layer except the last, going backwards:
        din <- dout.copy()
        layer->bpropUpdate(layer->input_variables,layer->output, din, dout)

  

 * network::unconditionalGenerate(): sample an input, unconditionally
     last_layer->outputMarginalSample()
     network->conditionalGenerate(last_layer->output_variables)

 * network::conditionalGenerate(output): sample an input, conditional on the given output value
     last_layer->downActivation() // compute part of the activation of last hidden units due to output units
     last_hidden_layer->output_activation_external_contribution <- last_layer->input_activation.copy()
     repeat n_MCMC_iterations
       last_hidden_layer->downSample()
       last_hidden_layer->upSample()
    ...?


     


