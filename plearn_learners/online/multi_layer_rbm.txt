
                      Contrastive divergence multi-layer RBMs


Boltzmann machines:

X (may be) observed, H is (always) hidden r.v.
Their joint is given by the Boltzmann distribution
associated with an energy function energy(x,h):
  P(X=x,H=h) = exp(-energy(x,h)) / Z
where Z is the appropriate normalization constant:
  Z = sum_{x,h} exp(-energy(x,h)).

In ordinary Boltzmann machines, the energy function is a 
quadratic polynomial. Let z=(x,h), then
  energy(z) = sum_i b_i z_i    +    sum_{ij} w_{ij} z_i z_j
where z_i \in {0,1}. There is no need for a constant term 
since it would cancel out in Z.

If X=x is observed while H remains hidden, the likelihood
involves a sum over all configurations of H:

  P(X=x) = sum_h P(X=x,H=h) = sum_h exp(-energy(x,h)) / Z

Unfortunately, the exact gradient of log P(x) wrt b or w
is intractable since it involves two intractable sums
(the one over h for the numerator of the above, and the
one over h and x, for Z, the denominator).

The gradient can be written as a sum of the corresponding two terms:

  d(-log P(x))/dtheta =
  sum_h      P(H=h|X=x) d_energy(h,x)/dtheta          [this is the POSITIVE phase contribution]
- sum_{h,x}  P(H=h,X=x) d_energy(h,x)/dtheta          [this is the NEGATIVE phase contribution]

The derivation of this result is easy (we do a similar derivation
for the restricted Boltzmann machine below).

The standard way to estimate the gradient, to avoid these sums, is
to perform an MCMC scheme to obtain one or more samples from P(h|x)
with x ~ training set and from P(x,h).

For a reason that I do not remember exactly, the w matrix
is normally constrained to be symmetric. I believe it has
to do with the MCMC scheme normally used for sampling
from P(X) or from P(H|X). However, we will not be using
that scheme, so I do not think that this restriction
is important (to be verified somehow).

Restricted Boltzmann machines:

If we set the weight between h_i and h_j to 0 
and the weight between x_i and x_j to 0, we obtain a RBM.
The advantage of an RBM is that all the H_i's become independent
when conditioning on X, and (symmetrically) all the X_i become independent
when conditioning on H.


Energy functions for Restricted Boltzmann Machines:
(Note that w_{ij} = w_{ji})

 * energy term for binomial unit i with value v_i and inputs u_j, parameters (b_i, w_{i.}): 
     
    b_i v_i + sum_j w_{ij} v_i u_j
   
       ==> P(v_i=1 | u) = exp(- b_i - sum_j w_{ij} u_j) / (1 + exp(- b_i - sum_j w_{ij} u_j)) = sigmoid(-b_i - sum_j w_{ij} u_j)
       NOTE THE MINUS

 * energy term for fixed-variance Gaussian unit i with value v_i and inputs u_j, parameters (a_i,b_i, w_{i,.}):

    b_i v_i + a_i^2 v_i^2 + sum_j w_{ij} v_i u_j
       ==> P(v_i | u) = (1/Z) exp(-(b_i v_i + a_i^2 v_i^2 + sum_j w_{ij} v_i u_j)) = (1/Z)(-0.5(v_i - mu)/sigma^2)
       ==> P(v_i | u) = N(v_i; mu, sigma^2) with sigma^2 = 0.5/a_i^2, mu = -0.5(b_i + sum_j w_{ij} u_j)/a_i^2
       NOTE HOW THESE BLOW UP WHEN a_i IS TOO SMALL. MAY WANT TO USE (a_i + epsilon)^2 INSTEAD, with epsilon fixed.

 * energy term for softmax units i with value v_i and inputs u_j, parameters (b_i, w_{i.}): 
     
    b_i v_i + sum_j w_{ij} v_i u_j
   
       ==> P(v_i=1 | u) = exp(- b_i - sum_j w_{ij} u_j) / sum_i " = softmax(- b_i - sum_j w_{ij} u_j)
       NOTE THE MINUS

Likelihood gradient for a RBM with observed inputs x, hidden outputs y:

 use Z = sum_{x,y} exp(-energy(x,y))
    P(x,y) = exp(-energy(x,y))/Z
    P(y|x) = exp(-energy(x,y)) / sum_y exp(-energy(x,y))

 For ANY energy-based (Boltzmann) distribution:
  d/dtheta (- log P(x)) = d/dtheta (- log sum_y P(x,y)) = d/dtheta (- log sum_y exp(-energy(x,y))/Z)
                        = (Z/(sum_y exp(-energy(x,y)))) sum_y exp(-energy(x,y))/Z (d_energy(x,y)/dtheta + (1/Z) dZ/dtheta)
                        = sum_y P(y|x) d_energy(x,y)/dtheta - (1/Z) sum_{x,y} exp(-energy(x,y)) d_energy(x,y)/dtheta
                        = sum_y P(y|x) d_energy(x,y)/dtheta - sum_{x,y} P(x,y) d_energy(x,y)/dtheta
                        = E[d_energy(x,y)/dtheta | x] - E[d_energy(x,y)/dtheta]
                          where E is over the model's distribution
                        = "positive phase contribution" - "negative phase contribution"
 The positive phase tries to lower the energy of observed x while the negative phase tries to increase the energy of all x ~ P.

 For a RBM, P(y|x) factorizes into P(y_i|x) and energy is a sum over energy_i(x,y_i), and theta={theta_i} so that 
  sum_y P(y|x) d_energy(x,y)/dtheta_i = sum_{y_i} P(y_i|x) d_energy_i(x,y_i)/dtheta_i
 with theta_i the parameters associated with y_i.
  
 With Contrastive Divergence we replace the expectation over (x,y) by a sample taken after 1 (or more) Gibbs sampling steps
    observed x = x0 -- (P(y|x0)) --> y0 -- (P(x|y0)) --> x1 -- (P(y|x1)) --> y1
 and the pair (x1,y1) serves as that sample in the case of 1 step (= "CD1").

  * output binomial unit i <-> input binomial unit j
      weight w_{ij}:
       positive phase contribution: P(y_{0i}=1|x_0) 1*x_{0j} + (1 - P(y_{0i}=1|x_0)) 0*x_{0j} = P(y_{0i}=1|x_0) x_{0j}
       negative phase contribution: P(y_{1i}=1|x_1) 1*x_{1j} + (1 - P(y_{1i}=1|x_1)) 0*x_{1j} = P(y_{1i}=1|x_1) x_{1j}
      bias b_i:
       positive phase contribution: P(y_{0i}=1|x_0) 
       negative phase contribution: P(y_{1i}=1|x_1) 

  * output binomial unit i <-> input Gaussian unit j
      bias b_i and weight w_{ij} as above
      parameter a_j:
       positive phase contribution: 2 a_j x_{0j}^2
       negative phase contribution: 2 a_j x_{1j}^2
      
  * output softmax unit i <-> input binomial unit j
      same formulas as for binomial units, except that P(y_i=1|x) is computed differently (with softmax instead of sigmoid)


Basic operations in each RBM layer:

 * layer::upActivation: compute parameters of p(output_variables | input_variables)
             i.e. compute activation (weighted sum) in the case of binomial units, and (mu,sigma) in the case of Gaussian units
 * layer::fprop(input,E[output|input]): find E(output_variables | input_variables). This ALWAYS calls upActivation.
             call upActivation and then compute & return corresponding expectation (p for binomial = sigmoid(activation), mu for Gaussian)
 * layer::upSample(): sample output_variables from p(output_variables | input_variables). upActivation MUST have been called before.
             sample from binomial or from Gaussian
 * layer::downActivation(): compute parameters of p(input_variables | output_variables)
             similar to upActivation, but downward, i.e. on input variables
 * layer::downExpectation(): find E(input_variables | output_variables). This ALWAYS calls downActivation.
             similar to upExpectation but downward, i.e. on input variables
 * layer::downSample(): sample input_variables from p(input_variables | output_variables). downActivation MUST have been called before.
             similar to upSample but on input variables
 * layer::accumulatePosStats(): accumulate positive phase statistics
             for binomial and softmax output units: accumulate input_variable[i]*output_prob[j] in pos_corr[i][j]
                                                    accumulate output_prob[j] in pos_mean[j]
             for Gaussian input units: accumulate 2 a[i] input_variable[i]^2 in pos_square[i]
             for all: increment pos_count
 * layer::accumulateNegStats(): accumulate negative phase statistics
             for binomial output units: accumulate input_variable[i]*output_prob[j] in neg_corr[i][j]
             for Gaussian input units: accumulate 2 a[i] input_variable[i]^2 in neg_square[i]
             for all: increment neg_count
 * layer::layerCDupdate(): update parameters using positive and negative phase statistics
             for binomial and softmax output units: 
                  gradient estimator for b_j = pos_mean[j]/pos_count - neg_corr[i][j]/neg_count
                  gradient estimator for w_ij = pos_corr[i][j]/pos_count - neg_corr[i][j]/neg_count
                  clear pos_corr, neg_corr
             for Gaussian input units:
                  gradient estimator for a_i = pos_square[i]/pos_count - neg_square[i]/neg_count
                  clear pos_square
             clear pos_count,neg_count


Combined operations, in a supervised RBM network:
 A supervised RBM network is an OnlineLearningModule that has a sequence of layers. 
 The last_layer is fully supervised. The 2nd to last is the last_hidden_layer.
 There is also a cost_layer that follows the last_layer, which computes the supervised cost to minimize.

 * network::computeRepresentation(input_variables):
     v <- input_variables
     for each layer except the last:
        layer->input_variables <- v.copy()
        v.resize(output_variables.size())
        layer->fprop(layer->input_variables,v)
     return v

 * network::fprop(input_variables)
     computeRepresentation(input_variables)
     if last_layer_takes_activations: (e.g. for undirected softmax last_layer)
       last_layer->fprop(last_hidden_layer->output_activation,last_layer->output)
     else
       last_layer->fprop(last_hidden_layer->output,last_layer->output)
     return last_layer->output

 * network::unsupervised_learning_from_example(input_variables):
     v <- input_variables
     for each layer except the last:
        layer->input_variables <- v.copy()
        layer->fprop(layer->input_variables,layer->output)
        v <- layer->output
        layer->upSample() 
        layer->accumulatePosStats()
        layer->downSample()
        layer->upSample()
        layer->accumulateNegStats()
        layer->CDupdate()

 * network::supervised_learning_from_example(input_variables, output_variables):
     unsupervised_learning_from_example(input_variables)
     if last_layer_takes_activations: (e.g. for undirected softmax last_layer)
       last_layer->fprop(v=last_hidden_layer->output_activation,last_layer->output)
     else
       last_layer->fprop(v=last_hidden_layer->output,last_layer->output)
     cost_layer->fprop((last_layer->output,output_variables),cost)
     cost_layer->bpropUpdate((last_layer->output,output_variables),cost,(dout,tmp), 1)
     last_layer->bpropUpdate(v,out,din,dout)
     for each layer except the last, going backwards:
        dout <- din.copy()
        layer->bpropUpdate(layer->input_variables,layer->output, din, dout)


 * network::unconditionalGenerate(): sample an input, unconditionally
     last_layer->clearActivations() // so that the first upSample() only samples from the biases
     repeat n_MCMC_sampling_iterations
       last_layer->upSample()
       last_layer->downActivation()
       last_hidden_layer->output_activation_external_contribution <- last_layer->input_activation.copy()
       last_hidden_layer->upActivation() // adds activation from inputs with external contribution from last_layer
       last_hidden_layer->upSample()
       last_hidden_layer->downActivation()
       last_hidden_layer->downSample()
       last_layer->input_variables <- last_hidden_layer->output_variables.copy()
       last_layer->upActivation()
     v <- last_hidden_layer->output_variables
     for each layer except the last, going backwards:
       layer->output_variables <- v.copy()
       layer->downActivation()
       layer->downSample()
       v <- layer->input_variables
     return v


 * network::conditionalGenerate(output): sample an input, conditional on the given output value
     last_layer->output_variables <- output.copy()
     last_layer->downActivation() // compute part of the activation of last hidden units due to output units
     last_hidden_layer->output_activation_external_contribution <- last_layer->input_activation.copy()
     repeat n_MCMC_sampling_iterations
       last_hidden_layer->downActivation()
       last_hidden_layer->downSample()
       last_hidden_layer->upActivation()
       last_hidden_layer->upSample()
     v <- last_hidden_layer->output_variables
     for each layer except the last, going backwards:
       layer->output_variables <- v.copy()
       layer->downActivation()
       layer->downSample()
       v <- layer->input_variables
     last_hidden_layer->output_activation_external_contribution.clear()
     return v

-------------------------------------------------------------------------------
Using and training the last layers
-------------------------------------------------------------------------------

Using
-----
If we train the network in a supervised fashion, we introduce a layer
containing the outputs (or targets), Y.
Let's call the last layer L and the previous layer P, and define:

 R.V. (=sample) of the last layer = L_i
 R.V. (=sample) of the previous (next-to-last) layer = P_j
 R.V. of the supervised layer = Y_k
 energy parameters between P_j and L_i = V,C: energies V_ij L_i P_j + C_i L_i
 energy parameters between L_i and Y_k = W,B: energies W_ki Y_k L_i + B_k Y_k
 "output" (expectation) of next-to-last layer = p(P) (given the inputs of the
network)

 * The activation of Y is computed from p(P), not p(L):

    actY_k = -B_k + sum_i softplus(-(W_ki + C_i + sum_j V_ij p(P_j)))
    with softplus: x -> log(1 + exp(x))
    (see below for explanation)

 * The expectations of Y, from the activations (Y is a multinomial units set)
       P(Y|inputs) = softmax(actY)

 * fprop = expectations ( activations )

Training
--------
There are two ways of learning the parameters V, C, W and B

 * By simple gradient descent:
We compute the cost = - log P(Y=onehot(k)|inputs), where k is the
observed target, and we backprop all the way.

 * By using contrastive divergence:
    - we consider [ Y, P ] = X a big layer, beyond L
    - we put [onehot(k), p(P)] = x in X, as input of L
    - we compute the expectations of L given x:
        p(L_i) = sigmoid(C_i + W_ki + sum_j V_ij p(P_j))
    - accumulatePosStats(x, p(L))
    - sample L according to p(L)
    - compute p(P) given L
    - sample P according to p(P)
    - compute p(L) again, using P as input (and NOT p(P))
    - accumulateNegStats([onehot(k), P], p(L))
    - update

Or a combination of the two above. See rbm_todo_and_ideas.txt for variants.

Why?
----
The output probabilities are computed as follows:
    P(Y=onehot(k)|inputs) = exp(-B_k + sum_i softplus(-(W_ki + C_i + sum_j V_ij p(P_j))) / Z

This formula can be derived by considering that P, L, and Y are binary
random variables following the Boltzmann distribution with energy:
    E(L,Y,P) = B'Y + Y'WL + C'L + P'V'L

During training, both P and Y are observed, so that E is linear in L,
i.e. P(L|P,Y) is a product of P(L_i|P,Y):
the L_i are conditionally independant given P and Y.

This corresponds to an undirected graphical model with full connectivity
between each L_i and each Y_k (and similarly between L_i and each P_j),
but no connection among the L_i or among the Y_k's. Because of this
factorization we obtain that
    P(Y|P) = sum_L exp(-E(L,Y,P)) / Z
and
   sum_L exp(-E(L,Y,P)) = exp(-B'Y) prod_i(exp(-E_i(1,Y,P)) + exp(-E_i(0,Y,P)))

where E_i(l,Y,P) = the term in L_i=l in the energy
                 = l (sum_k Y_k W_ki + C_i + sum_j V_ij P_j).

Since E_i(0,Y,P) = 0, we obtain that
    sum_L exp(-E(L,Y,P)) = exp(-B'Y) exp(sum_i log(1 + exp(-E_i(1,Y,P))))
                         = exp(-B'Y + sum_i softplus(-sum_k Y_k W_ki + C_i + sum_j V_ij P_j))

which gives the above formula for P(Y=onehot(k)|inputs), if we replace P by
its expectation p(P).


-------------------------------------------------------------------------------
Other architecture and pseudo-code proposed
-------------------------------------------------------------------------------

The units themselves, stored in layers (class RBMLayer), are separated
from the weighted links between layers (class RBMParameters, inheriting
from OnlineLearningModule). The top-level architecture of the network (say,
DeepBeliefNetwork) is a PLearner (maybe a PDistribution) with functions for
sampling.

RBMLayer stores:
  - activations: values allowing to know the distribution (the activation
    value (before sigmoid) for a binomial input, the couple (mu,sigma) for a
    gaussian).
  - expectations: expectation of each unit
  - sample: sample from the distribution
  - some flags to know what is up-to-date

RBMParameters stores (and has to update):
  - a string describing the type of upper units, "l" if the energy function is
    linear (binomial or part of multinomial), "q" if quadratic (gaussian)
  - the same thing for the lower units
  - weights: weights between up and down layers
  - TVec<Vec> up_unit_params: the Vec up_unit_params[i] contains the
    bias (and parameters like the quadratic term, in gaussian units) for
    unit i in up layer
  - TVec<Vec> down_unit_params: the same for down error
  - input_vec: a pointer to its current input vector (sample or expectation),
    and a flag to know if it is up or down

  - weights_pos_stats, weight_neg_stats: statistics accumulated during
    positive (respectively negative) phase
  - up_unit_params_pos_stats, up_unit_params_neg_stats
  - down_unit_params_pos_stats, down_unit_params_neg_stats
  - pos_count and neg_count: two counters of the numbers of statistics
    accumulated
  - a learning rate (possibly a second one specially for error gradient)

The methods are:
  * RBMParameters::setAsUpInput( Vec ) : set the input vector, and flag to 'up'
  * RBMParameters::setAsDownInput( Vec ) : same, but 'down'
  * RBMParameters::accumulatePosStats( Vec down_values, Vec up_values ) :
      weights_pos_stats += up_values * down_values';
      for i in up units:
        up_unit_params_pos_stats[i][0] += up_values[i]
      for i in "g" up units:
        up_unit_params_pos_stats[i][1] += 2 * up_unit_params[i][1] * up_values[i]^2
      for i in down units:
        down_unit_params_pos_stats[i][0] += down_values[i]
      for i in "g" down units:
        down_unit_params_pos_stats[i][1] += 2 * down_unit_params[i][1] * down_values[i]^2
      pos_count++;

  * RBMParameters::accumulateNegStats( Vec down_values, Vec up_values ) :
      weights_neg_stats += up_values * down_values';
      for i in up units:
        up_unit_params_neg_stats[i][0] += up_values[i]
      for i in "g" up units:
        up_unit_params_neg_stats[i][1] += 2 * up_unit_params[i][1] * up_values[i]^2
      for i in down units:
        down_unit_params_neg_stats[i][0] += down_values[i]
      for i in "g" down units:
        down_unit_params_neg_stats[i][1] += 2 * down_unit_params[i][1] * down_values[i]^2
      neg_count++;

  * RBMParameters::update() (from statistics accumulated)
      # are the signs OK?
      weights -= learning_rate * (weights_pos_stats/pos_count - weight_neg_stats/neg_count);
      up_unit_params -= learning_rate * (up_unit_params_pos_stats/pos_count - up_unit_params_neg_stats/neg_count);
      down_unit_params -= learning_rate * (down_unit_params_pos_stats/pos_count - down_unit_params_neg_stats/neg_count);
      # reset
      weights_pos_stats.clear();
      weights_neg_stats.clear();
      up_unit_params_pos_stats.clear();
      up_unit_params_neg_stats.clear();
      down_unit_params_pos_stats.clear();
      down_unit_params_neg_stats.clear();
      pos_count = 0;
      neg_count = 0;

  * implement fprop(), bpropUpdate() (how?)

  * RBMParameters::computeUnitActivations( int i, const Vec& activations ) :
    put in activations the parameters describing the distribution of
    unit i (i is an index in the up layer if input vector is 'down', and
    vice-versa), and invalidates 'sample' and 'expectation'

  * RBMParameters::computeUnitActivations( const Vec& all_activations ) :
    same thing, for every unit in the appropriate layer

For class RBMLayer:

  * RBMLayer::getUnitActivations( int i, RBMParams rbmp ) :
      calls rbmp->computeUnitActivations( i, layer_params.subvec(...) )
  * RBMLayer::getUnitActivations( RBMParams rbmp ) :
      calls rbmp->computeUnitActivations( layer_params ) by default, or
      rbmp->computeUnitActivations( indices, layer_params ),
        indices being a subset of units, for "sparse" layers
  * RBMLayer::generateSample()
      get 'sample' from 'activations'
  * RBMLayer::computeExpectation()
      get 'expectation' from 'activations'

  * string RBMLayer::getUnitsTypes()
      returns a string with each character encoding the type of corresponding
      unit, 'l' for linear and 'g' for gaussian

If a DeepBeliefNetwork has two RBMLayer's (input and output) and one
RBMParameters (rbmp) linking both of them, a simple up propagation of
input_vec to a sample (out_sample) would typically call:

    input.sample << input_vec;
    rbmp.setAsDownInput( input.sample );
    output.getUnitActivations( rbmp );
    output.generateSample();
    out_sample << output.sample;

If we wanted the expectation (out_exp) instead of a sample, it would have
been:

    input.sample << input_vec;
    rbmp.setAsDownInput( input.sample );
    output.getUnitActivations( rbmp );
    output.computeExpectation();
    out_exp << output.expectation;

The learning of one example (input_vec) by contrastive divergence:

    // positive phase
    input.sample << input_vec;
    rbmp.setAsDownInput( input.sample );
    output.getUnitActivations( rbmp );
    output.generateSample();
    rbmp.accumulatePosStats( input.sample, output.sample );

    // negative phase
    rbmp.setAsUpInput( output.sample );
    input.getUnitActivations( rbmp );
    input.generateSample();
    rbmp.setAsDownInput( input.sample );
    output.getUnitActivations( rbmp );
    output.generateSample();
    rbmp.accumulateNegStats( input.sample, output.sample );

    // update
    rbmp.update();

If we have three sequential layers (input, hidden, output) and two
RBMParameters:
  - rbmp_ih between input and hidden, frozen (because learned previously);
  - rbmp_ho between hidden and output, which we train;
the learning of one example (input_vec) will look like:

    // propagation to hidden
    input.sample << input_vec;
    rbmp_ih.setAsDownInput( input.sample );
    hidden.getUnitActivations( rbmp_ih );
    hidden.computeExpectation(); // not generateSample()

    // positive phase
    rbmp_ho.setAsDownInput( hidden.expectation );
    output.getUnitActivations( rbmp_ho );
    output.generateSample();
    rbmp_ho.accumulatePosStats( hidden.expectation, output.sample );

    // negative phase
    rbmp_ho.setAsUpInput( output.sample );
    hidden.getUnitActivations( rbmp_ho );
    hidden.getExpectation();
    rbmp_ho.setAsDownInput( hidden.expectation );
    output.getUnitActivations( rbmp_ho );
    output.generateSample();
    rbmp_ho.accumulateNegStats( hidden.expectation, output.sample );

    // update
    rbmp_ho.update();


