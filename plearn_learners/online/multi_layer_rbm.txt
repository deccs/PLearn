
                      Contrastive divergence multi-layer RBMs

Energy functions:
(Note that w_{ij} = w_{ji})

 * energy term for binomial unit i with value v_i and inputs u_j, parameters (b_i, w_{i.}): 
     
    b_i v_i + sum_j w_{ij} v_i u_j
   
       ==> P(v_i=1 | u) = exp(- b_i - sum_j w_{ij} u_j) / (1 + exp(- b_i - sum_j w_{ij} u_j)) = sigmoid(-b_i - sum_j w_{ij} u_j)
       NOTE THE MINUS

 * energy term for fixed-variance Gaussian unit i with value v_i and inputs u_j, parameters (a_i,b_i, w_{i,.}):

    b_i v_i + a_i^2 v_i^2 + sum_j w_{ij} v_i u_j
       ==> P(v_i | u) = (1/Z) exp(-(b_i v_i + a_i^2 v_i^2 + sum_j w_{ij} v_i u_j)) = (1/Z)(-0.5(v_i - mu)/sigma^2)
       ==> P(v_i | u) = N(v_i; mu, sigma^2) with sigma^2 = 0.5/a_i^2, mu = -0.5(b_i + sum_j w_{ij} u_j)/a_i^2
       NOTE HOW THESE BLOW UP WHEN a_i IS TOO SMALL. MAY WANT TO USE (a_i + epsilon)^2 INSTEAD, with epsilon fixed.


Basic operations in each RBM layer:

 * layer::upActivation: compute parameters of p(output_variables | input_variables)
 * layer::fprop(input,E[output|input]): find E(output_variables | input_variables). This ALWAYS calls upActivation.
 * layer::upSample(): sample output_variables from p(output_variables | input_variables). upActivation MUST have been called before.
 * layer::downActivation(): compute parameters of p(input_variables | output_variables)
 * layer::downExpectation(): find E(input_variables | output_variables). This ALWAYS calls downActivation.
 * layer::downSample(): sample input_variables from p(input_variables | output_variables). downActivation MUST have been called before.
 * layer::accumulateStats(in,out,1): accumulate positive phase statistics
 * layer::accumulateStats(in,out,-1): accumulate negative phase statistics
 * layer::layerCDupdate(): update parameters using positive and negative phase statistics
 
Combined operations, in an RBM network:

 * network::computeRepresentation(input_variables):
     v <- input_variables
     for each layer except the last:
        layer->input_variables <- v.copy()
        v.resize(output_variables.size())
        layer->fprop(layer->input_variables,v)
     return v

 * network::unsupervised_learning_from_example(input_variables):
     v <- input_variables
     for each layer except the last:
        layer->input_variables <- v.copy()
        v.resize(output_variables.size())
        layer->fprop(layer->input_variables,v)
        layer->upSample() 
        layer->accumulateStats(v,output_variable)
        layer->downSample()
        layer->upSample()
        layer->accumulateStats(input_variable,output_variable)
        layer->CDupdate()

 * network::semisupervised_learning_from_example(input_variables, output_variables):
     v <- input_variables
     for each layer except the last:
        layer->input_variables <- v.copy()
        layer->fprop(layer->input_variables,layer->output)
        v <- layer->output
        layer->upSample() 
        layer->accumulateStats(v,output_variable)
        layer->downSample()
        layer->upSample()
        layer->accumulateStats(input_variable,output_variable)
        layer->CDupdate()
     last_layer->input_variables <- v.copy()
     last_layer->fprop(last_layer->input_variables,last_layer->output)
     cost_layer->fprop((output_variables,last_layer->output),cost)
     cost_layer->bpropUpdate((output_variables,last_layer->output),cost,(tmp, dout), 1)
     last_layer->bpropUpdate(v,out,dv,dout)
     for each layer except the last, going backwards:
        din <- dout.copy()
        layer->bpropUpdate(layer->input_variables,layer->output, din, dout)

  



     


