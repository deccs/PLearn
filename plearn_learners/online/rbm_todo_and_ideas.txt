To do
=====

* Comments!

* Add a subclass of RBMParameters to put between the last layers (the layer
  containing the targets or outputs, the layer containing the input
  representation, and the layer containing the joint representation)

* Implement DeepBeliefNet as a PDistribution, trained the way Hinton
  trains its network

* Make an OnlineLearningModule stacking RBMLayers and RBMParameters doing the
  same thing? Encapsulating DeepBeliefNet? Make DeepBeliefNet encapsulate the
  module?


Ideas
=====

* specialize RBMParameters for the case where all the units on
the up (and down) layer have the same type ('l' or 'q'), because
RBMGenericParameters has to check every unit. So we could use matrix-matrix
products instead of n matrix-vector products.

* How to combine error gradient and likelihood gradient?

* Implement global wake-sleep (on every layer at one time) learning of the
  weights ==> this would seem necessary in order to have simultaneous
  unsupervised adaptation in all the RBM layers in a DeepBeliefNet.

* See if we share biases (and quadratic term) of one layer between the "upper"
  and "lower" RBMParameters modules

* Is there a way to compute analytically the "undirected softmax" if the
  output layer has Gaussian units?

* How to learn the weights V between Last layer and Previous layer, and W
  between Last layer and output/target layer (Y) if trained in a supervised
  way?
  - Learn the V unsupervisedly first, then learn W (and update the other ones?)
    by gradient descent on the cost
  - Learn [W,V] all at once by contrastive divergence, trying to model the
    distributions of [Y,P] (or [Y, expectation(P)])
  - Learn V by contrastive divergence, but clamping Y to the true target
    during both positive and negative phases

* See when we should sample from a layer, and when we should compute the
  expectation. Is it a problem if some statistics during the positive phase
  are expectations, and are samples during the negative phase (or vice-versa)?

* introducing temporal structure into the model:
   - time delays in the connections
   - recurrent connections (e.g. from layer i at t to layer i at t+1)
   - supervised targets (and corresponding gradients) from the task of
     predicting layer i at t+1 from layer j at t





