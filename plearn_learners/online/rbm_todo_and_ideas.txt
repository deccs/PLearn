To do
=====

* Comments!

* Subclass UnconditionalDistribution into JointDistribution

* implement DeepBeliefNet as a JointDistribution, trained the way Hinton
  trains its network

* Make an OnlineLearningModule stacking RBMLayers and RBMParameters doing the
  same thing? Encapsulating DeepBeliefNet? Make DeepBeliefNet encapsulate the
  module?


Ideas
=====

* specialize RBMParameters for the case where all the units on
the up (and down) layer have the same type ('l' or 'q'), because
RBMGenericParameters has to check every unit. So we could use matrix-matrix
products instead of n matrix-vector products.

* How to combine error gradient and likelihood gradient?

* Implement global wake-sleep (on every layer at one time) learning of the
  weights ==> this would seem necessary in order to have simultaneous
  unsupervised adaptation in all the RBM layers in a DeepBeliefNet.

* See if we share biases (and quadratic term) of one layer between the "upper"
  and "lower" RBMParameters modules

* Is there a way to compute analytically the "undirected softmax" if the
  output layer has Gaussian units?

* introducing temporal structure into the model:
   - time delays in the connections
   - recurrent connections (e.g. from layer i at t to layer i at t+1)
   - supervised targets (and corresponding gradients) from the task of
     predicting layer i at t+1 from layer j at t





