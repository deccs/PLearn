To do
=====

* Comments!

* Subclass UnconditionalDistribution into JointDistribution

* implement DeepBeliefNet as a JointDistribution, trained the way Hinton
  trains its network

* Make an OnlineLearningModule stacking RBMLayers and RBMParameters doing the
  same thing? Encapsulating DeepBeliefNet? Make DeepBeliefNet encapsulate the
  module?


Ideas
=====

* specialize RBMParameters for the case where all the units on
the up (and down) layer have the same type ('l' or 'q'), because
RBMGenericParameters has to check every unit. So we could use matrix-matrix
products instead of n matrix-vector products.

* How to combine error gradient and likelihood gradient?

* Implement global wake-sleep (on every layer at one time) learning of the
  weights

* See if we share biases (and quadratic term) of one layer between the "upper"
  and "lower" RBMParameters modules

* Is there a way to compute analytically the "undirected softmax" if the
  output layer has Gaussian units?



