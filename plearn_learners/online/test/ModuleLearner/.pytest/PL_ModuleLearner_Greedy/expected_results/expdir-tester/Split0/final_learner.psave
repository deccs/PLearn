*1 ->HyperLearner(
tester = *2 ->PTester(
expdir = "" ;
dataset = *3 ->AutoVMatrix(
filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y_binary_class.vmat" ;
load_in_memory = 0 ;
writable = 0 ;
length = 200 ;
width = 6 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
extrasize = 0 ;
metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y_binary_class.vmat.metadata/"  )
;
splitter = *4 ->ExplicitSplitter(
splitsets = 1  2  [ 
*3  	*3  	
]
 )
;
statnames = []
;
statmask = []
;
learner = *5 ->ModuleLearner(
module = *6 ->ForwardModule(
modules = 3 [ *7 ->NetworkModule(
modules = 1 [ *8 ->RBMModule(
visible_layer = *9 ->RBMBinomialLayer(
size = 5 ;
learning_rate = 0.00100000000000000002 ;
momentum = 0 ;
gibbs_ma_schedule = []
;
gibbs_ma_increment = 0.100000000000000006 ;
gibbs_initial_ma_coefficient = 0.100000000000000006 ;
bias = 5 [ -0.00278509775797984452 -0.0165640011469744587 -0.00598010917213076532 0.00176154865174407379 -0.00543761947532792316 ] ;
input_size = 5 ;
output_size = 5 ;
name = "RBMBinomialLayer" ;
use_fast_approximations = 0 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10 ->PRandom(
seed = 1827 ;
fixed_seed = 0  )
 )
;
hidden_layer = *11 ->RBMBinomialLayer(
size = 15 ;
learning_rate = 0.00100000000000000002 ;
momentum = 0 ;
gibbs_ma_schedule = []
;
gibbs_ma_increment = 0.100000000000000006 ;
gibbs_initial_ma_coefficient = 0.100000000000000006 ;
bias = 15 [ 0.000779997877980971718 -0.00104728075180422309 0.000162596992700449037 0.000132818845402643711 0.00133590769684036287 0.000687589776443822708 -7.76560227385681117e-05 0.00125951472842306968 -0.000202306033338346051 -0.000277744847549938072 0.000862967922845554389 0.00050754326341195062 0.000595454593460812104 0.000212416496440332819 -0.000303482986891311719 ] ;
input_size = 15 ;
output_size = 15 ;
name = "RBMBinomialLayer" ;
use_fast_approximations = 0 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
;
connection = *12 ->RBMMatrixConnection(
weights = 15  5  [ 
0.114223035535808898 	0.182442683757086221 	-0.22558965553523691 	-0.232130397907910097 	0.11730116647153796 	
-0.236729823211870999 	-0.238573859120553428 	-0.0174289393146416227 	-0.0352968636593247662 	0.0868521987958738306 	
-0.112385167931701516 	0.109165120043211133 	0.0398879707018945789 	0.213863755913404424 	-0.153776418605648835 	
0.0472304326758663795 	0.179914195257864562 	-0.241836339155833635 	-0.00219891447934676861 	-0.219975733657415989 	
-0.0801878004779727066 	0.15885585162094773 	0.194765932183775026 	-0.202183990715325973 	0.25294763522081759 	
0.142088781314686163 	0.167911434953121719 	-0.144817746011810855 	-0.245515785176769086 	-0.00427092943781542164 	
0.22482713619054745 	-0.190942470533467412 	0.143516397182502198 	-0.0873212311324905038 	0.191847265724084781 	
-0.0443110228904399148 	0.220151981275360675 	0.0383038099022214029 	0.0237199697935066532 	0.256490747789178242 	
0.148629878649942732 	-0.00573752550059243756 	0.0289256023536940202 	0.0543794629658403209 	-0.247820321229642482 	
0.169644163328278808 	-0.197948084038232108 	0.146201678801202523 	-0.0586144105923159867 	0.104114336557621506 	
-0.19614775922123337 	0.118052340202558517 	0.175372344118596563 	0.0364296668610980603 	0.191065325213556791 	
-0.225122139804147053 	0.154647360886455448 	0.183380332951252722 	0.072590554739058441 	-0.177866796759333312 	
0.206857124804643994 	0.0213917710286117141 	0.142039122446603244 	-0.0515882822962070581 	0.0809595650963546021 	
0.204445705418892149 	0.0531513116531849572 	0.0282427368276513754 	0.17829493078693312 	-0.0993056962247372693 	
0.0794396692635105095 	0.0164876104390719348 	-0.0339128563784461129 	0.210641398549506947 	-0.212868022370464677 	
]
;
gibbs_ma_schedule = []
;
gibbs_ma_increment = 0.100000000000000006 ;
gibbs_initial_ma_coefficient = 0.100000000000000006 ;
down_size = 5 ;
up_size = 15 ;
learning_rate = 0.00100000000000000002 ;
momentum = 0 ;
initialization_method = "uniform_sqrt" ;
input_size = 5 ;
output_size = 15 ;
name = "RBMMatrixConnection" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
;
reconstruction_connection = *0 ;
grad_learning_rate = 0.00100000000000000002 ;
cd_learning_rate = 0 ;
compute_contrastive_divergence = 0 ;
standard_cd_grad = 1 ;
standard_cd_bias_grad = 1 ;
standard_cd_weights_grad = 1 ;
n_Gibbs_steps_CD = 1 ;
min_n_Gibbs_steps = 1 ;
n_Gibbs_steps_per_generated_sample = 1 ;
compute_log_likelihood = 0 ;
Gibbs_step = 0 ;
log_partition_function = 0 ;
partition_function_is_stale = 1 ;
input_size = -1 ;
output_size = -1 ;
name = "rbm_0" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
] ;
connections = []
;
ports = 2 [ ("input" , "rbm_0.visible" )("output" , "rbm_0.hidden.state" )] ;
save_states = 1 ;
name = "network_0" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
*13 ->NetworkModule(
modules = 2 [ *8  *14 ->RBMModule(
visible_layer = *15 ->RBMBinomialLayer(
size = 15 ;
learning_rate = 0.00100000000000000002 ;
momentum = 0 ;
gibbs_ma_schedule = []
;
gibbs_ma_increment = 0.100000000000000006 ;
gibbs_initial_ma_coefficient = 0.100000000000000006 ;
bias = 15 [ -0.000926553686800094109 -0.0108532037578065402 -0.0131440558251403501 -0.00648439658228983432 0.00471989560268569214 -0.0109584775927349561 0.00355569914317291 0.00144157351643795493 9.59724932935818539e-05 -0.00333027077973015477 0.00821548772285247975 0.000423010261683955569 0.00594638451851331809 0.000717568772213690153 0.00993454669422304433 ] ;
input_size = 15 ;
output_size = 15 ;
name = "RBMBinomialLayer" ;
use_fast_approximations = 0 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
;
hidden_layer = *16 ->RBMBinomialLayer(
size = 20 ;
learning_rate = 0.00100000000000000002 ;
momentum = 0 ;
gibbs_ma_schedule = []
;
gibbs_ma_increment = 0.100000000000000006 ;
gibbs_initial_ma_coefficient = 0.100000000000000006 ;
bias = 20 [ -0.000453387009620988659 0.000845531434293066305 0.00162883533683450725 -3.87668695796345645e-05 0.000766236793527613085 0.000932980795007214061 0.000281682865875736458 -0.000730057603079139355 0.000925606672020178045 0.000919221687457388663 0.000712359500870028369 0.00066635569518742313 -0.000240566238039152463 0.00119560783510103694 0.00108325902627077705 0.00111064457759879262 0.00146226644400449402 0.000706339139820732847 0.000266131366869354086 -0.000122875266295231354 ] ;
input_size = 20 ;
output_size = 20 ;
name = "RBMBinomialLayer" ;
use_fast_approximations = 0 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
;
connection = *17 ->RBMMatrixConnection(
weights = 20  15  [ 
-0.199665893027455355 	-0.0465024958801317015 	-0.147568301000631952 	0.143054794116631301 	0.0966066196462358889 	-0.0976258306024285483 	-0.101260036292867034 	-0.177797091014299635 	0.209639908308190293 	-0.174018034822132461 	-0.0259767922673940534 	-0.200856387154690202 	0.0159748160488032212 	0.0511866829977281404 	-0.179770670392769255 	
-0.0394512522337529467 	0.117840023095250546 	0.0406692025192514767 	0.212098869137966661 	0.0151624548892851253 	-0.114980398362988789 	-0.0520374297605031844 	0.0642289531052787493 	0.0422259342848326294 	-0.187772552189459013 	0.0209784630500729034 	0.0512813133491341056 	-0.14641172744390632 	0.0436920843454037117 	-0.127221071086011134 	
0.0240646352843687829 	0.114480065465243114 	0.142795644595856197 	0.208236887934659359 	-0.208742793563283596 	0.175597887892326027 	0.148280878037553882 	0.185452532449155744 	0.0346077698543971399 	0.122778150266735803 	0.0845210001875947581 	0.198934487485176004 	0.086839913406693961 	-0.00898635970767380256 	-0.0111275482563717148 	
-0.213578110958704687 	0.000783623293485158487 	0.140494085029853583 	0.20977459976397464 	0.220353956831890962 	-0.227977352869991629 	-0.11721922738915512 	-0.0824019551832265162 	-0.0206905471229207605 	-0.0166860237671342983 	-0.0138231679221703201 	-0.06644867570643119 	-0.193966813932034327 	-0.0645985818367366404 	0.170276407643900035 	
-0.127523970842238821 	-0.0851757289646849075 	0.164384819742476457 	0.0140251388982130087 	-0.0732135404671881429 	0.156004639419107971 	-0.127159504142860313 	0.172245706801283233 	-0.0206740136211002734 	0.120672514310639328 	0.10628409805808868 	-0.200853600817347988 	0.223679090521664098 	0.123463793339618613 	-0.0934865472635584699 	
-0.163600932258851389 	-0.0418094868025905772 	0.214154774373236295 	-0.137245088342446725 	-0.172756288292473636 	0.211436999415832744 	0.168846152845123137 	0.115024252628079374 	-0.215975313525567147 	-0.0865929432670289601 	0.127996731837503985 	-0.0336441892227711398 	-0.0950821504813939705 	-0.0344875439759191466 	-0.0890344163684737011 	
0.0917676243332203323 	0.00647432876973066257 	0.0603689903341120895 	0.0542591111360847464 	0.207053633996143571 	-0.108213485753186317 	0.193655137718294484 	0.0375678836523921159 	0.215362120551664205 	0.153593816507550801 	-0.103225753462595063 	-0.0493100621030968245 	-0.0285065985944886607 	-0.17169186074478282 	-0.113158878902886909 	
0.0486067978554309346 	-0.134060412435438092 	0.104782818192997548 	0.0190855905819467542 	-0.0954268780990478044 	-0.0627496439144036022 	-0.104877551848392467 	0.103512582834357852 	0.159857418569593096 	-0.114480538155331588 	0.114591425115921075 	0.0849393166543545991 	0.186750914082214892 	0.125692199697273382 	0.0510844829873717612 	
-0.0661431606631400076 	0.0841866145468284505 	-0.0154465516592166962 	0.00164622472970422668 	-0.162502068098615698 	0.163538697437007885 	0.0699601254969599851 	-0.175808159566014699 	0.00442778145254507476 	0.159691474307498776 	-0.111367619281643446 	-0.163415033503308771 	0.188656812819911396 	0.173690145881305186 	0.0342121254537379044 	
0.136609915040623919 	-0.0580771352834966934 	0.135967651679254259 	-0.0556662900796270135 	0.0328655569514085949 	0.146730425379887258 	-0.0537434771234091488 	0.202439659618439766 	-0.0144895047848336948 	-0.0291653643612580227 	-0.132837722628549398 	0.148101772403925996 	0.106044862035487741 	0.177971943378715719 	-0.0779249685806468195 	
0.185878517029531509 	0.191768322809052677 	0.0768459022325266583 	0.129586434444902449 	-0.194273450712217605 	-0.136587867028291066 	-0.00207230691257036465 	0.0452998690185411959 	-0.147800010724965097 	-0.0353738952329342923 	-0.093391426851810469 	-0.147904532123091598 	0.0681951546937868969 	0.0675988322767383659 	0.121983215944044099 	
-0.0934389356416226724 	0.0742796580208564261 	-0.0740796687796987652 	0.170508684419131029 	0.0838546408778213515 	0.106760370320955617 	0.0485989308522178573 	0.0271100657127993604 	-0.191458846064656241 	0.217350646628457045 	0.081807296237196736 	0.222329288422829729 	0.122760409777772098 	-0.0880630055038586074 	-0.170762431602553749 	
-0.111209426892105417 	-0.0789584494826100491 	-0.00509111537201256247 	-0.195500594890432339 	0.0387899926972619311 	-0.127353846738302329 	0.100367944943578202 	-0.0541142586180089619 	0.17473497852335329 	0.122616552037058957 	-0.0933993395500215678 	0.064816217682076821 	-0.175539044822730528 	-0.137612966756728627 	-0.141212831687991758 	
-0.144619919922026746 	0.136671857687587411 	0.0523892905598260378 	0.0637515977285305624 	0.0428730694091070397 	0.111030712937232795 	-0.168074062951118458 	0.0994484096369092213 	0.116117466705265751 	-0.0746991941351960037 	0.0257699172506545005 	0.0968915977359569441 	-0.18908693294671805 	0.145331939853380332 	-0.0274078888771996307 	
0.200473936294629423 	0.0796472950114080896 	0.159829518589416686 	-0.0492859638056888094 	0.083823632353591776 	0.075883690665256423 	0.14163962831817084 	0.148997932669615391 	-0.194198858420308457 	0.164874030797497989 	-0.0576412089875710729 	-0.0744650365686934795 	0.126924490687926189 	-0.0473407687237771616 	-0.125163578650308832 	
0.0248530862717537583 	0.106211296949465114 	0.172465562053498639 	0.153958502157922977 	-0.132092722841237326 	0.100459233574110601 	-0.177248765737252678 	0.213131782659715552 	0.152143086860893506 	-0.111340194589212529 	0.104235842785752458 	0.00698315906037692141 	-0.0348543672686432993 	0.0410771107041951328 	0.18376298231127941 	
0.119847647485374964 	0.159723365513106952 	0.143771142846700811 	-0.102994303084915736 	0.11836461517276245 	0.215610544097093876 	-0.162335170778609095 	-0.194796983396369694 	-0.148049540636433108 	-0.0323209485217053685 	0.0041163461249623598 	0.0936775712720108039 	-0.0669931747800228961 	-0.145342144480870988 	0.0160231568615098845 	
0.192114447823598294 	-0.00771167347709920351 	0.0489591389382780448 	-0.211709068839596143 	0.0816521131270076528 	0.0760189582843147915 	0.106958451458287168 	-0.0202866958504462763 	-0.0509688750422014961 	0.167271624740515823 	-0.138516884904345883 	0.0236545195213502916 	-0.00646597850364279744 	0.0871377827793629739 	-0.126109342802363328 	
0.0632490108382660066 	0.150703644197975467 	-0.202290834600961367 	0.0500933244563882274 	0.074730197824073169 	0.131017723370825123 	0.195307550896969329 	-0.0242539328047582693 	-0.2096862509337237 	0.0443687107458422964 	-0.194568980241651646 	0.112336943259278441 	0.00360686173938918209 	-0.102790113151382603 	0.120951709749142711 	
0.19123869786015596 	-0.184565098222969626 	0.0988981591479547034 	-0.180196103199863594 	-0.107604564176469616 	-0.0520615142032379088 	0.0168105346399400335 	-0.128247449073637471 	-0.0249673569200909128 	0.217053863018495719 	0.128025739243165237 	-0.0856297093917833541 	0.0167650115631037093 	-0.0137172613840686568 	-0.185647068414078298 	
]
;
gibbs_ma_schedule = []
;
gibbs_ma_increment = 0.100000000000000006 ;
gibbs_initial_ma_coefficient = 0.100000000000000006 ;
down_size = 15 ;
up_size = 20 ;
learning_rate = 0.00100000000000000002 ;
momentum = 0 ;
initialization_method = "uniform_sqrt" ;
input_size = 15 ;
output_size = 20 ;
name = "RBMMatrixConnection" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
;
reconstruction_connection = *0 ;
grad_learning_rate = 0.00100000000000000002 ;
cd_learning_rate = 0 ;
compute_contrastive_divergence = 0 ;
standard_cd_grad = 1 ;
standard_cd_bias_grad = 1 ;
standard_cd_weights_grad = 1 ;
n_Gibbs_steps_CD = 1 ;
min_n_Gibbs_steps = 1 ;
n_Gibbs_steps_per_generated_sample = 1 ;
compute_log_likelihood = 0 ;
Gibbs_step = 0 ;
log_partition_function = 0 ;
partition_function_is_stale = 1 ;
input_size = -1 ;
output_size = -1 ;
name = "rbm_1" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
] ;
connections = 1 [ *18 ->NetworkConnection(
source = "rbm_0.hidden.state" ;
destination = "rbm_1.visible" ;
propagate_gradient = 0  )
] ;
ports = 2 [ ("input" , "rbm_0.visible" )("output" , "rbm_1.hidden.state" )] ;
save_states = 1 ;
name = "network_1" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
*19 ->NetworkModule(
modules = 5 [ *8  *14  *20 ->GradNNetLayerModule(
start_learning_rate = 0.00100000000000000002 ;
decrease_constant = 0 ;
init_weights = 0  0  [ 
]
;
init_bias = []
;
init_weights_random_scale = 1 ;
L1_penalty_factor = 0 ;
L2_penalty_factor = 0 ;
weights = 2  20  [ 
0.0128971541272204088 	0.0240948720799898826 	0.038090222095950782 	0.037163863684979502 	-0.0270027056800153653 	-0.0143830649287863613 	-0.0504111073661725542 	-0.019953259136972247 	0.00926120484532864593 	0.0322593454740600222 	-0.00196114628952165143 	-0.00379474247586950031 	-0.0253797390653449258 	-0.0444362466155495406 	-0.0103776355182317662 	0.0293393611662794568 	0.0102238931453799246 	-0.0200415901357403579 	0.0456535800127743016 	-0.0141947075442552979 	
0.0433498675730351613 	0.0205954078154498496 	0.00170036442589811452 	0.000301639148968105189 	-0.0416695674417588355 	-0.00262672591829156985 	0.035342179563095745 	0.0507752571585376108 	-0.031225741964821644 	0.0294932804765521382 	-0.00861177878077906862 	0.0445753457328775735 	0.0315607079692813847 	-0.007536097203666315 	0.0404239425343957959 	-0.0166658338379819908 	0.0503311884596000722 	-0.0449513913381749813 	-0.00878542310096892647 	0.0363060686455251025 	
]
;
bias = 2 [ -0.00617991517582894886 0.00617991517582895319 ] ;
input_size = 20 ;
output_size = 2 ;
name = "affine_net" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
*21 ->SoftmaxModule(
input_size = 2 ;
name = "softmax" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
*22 ->NLLCostModule(
target_size = 1 ;
input_size = 2 ;
output_size = 1 ;
name = "nll" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
] ;
connections = 4 [ *23 ->NetworkConnection(
source = "rbm_0.hidden.state" ;
destination = "rbm_1.visible" ;
propagate_gradient = 1  )
*24 ->NetworkConnection(
source = "rbm_1.hidden.state" ;
destination = "affine_net.input" ;
propagate_gradient = 1  )
*25 ->NetworkConnection(
source = "affine_net.output" ;
destination = "softmax.input" ;
propagate_gradient = 1  )
*26 ->NetworkConnection(
source = "softmax.output" ;
destination = "nll.prediction" ;
propagate_gradient = 1  )
] ;
ports = 4 [ ("input" , "rbm_0.visible" )("output" , "affine_net.output" )("target" , "nll.target" )("NLL" , "nll.cost" )] ;
save_states = 1 ;
name = "network_2" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
] ;
forward_to = "network_2" ;
forget_all = 1 ;
input_size = -1 ;
output_size = -1 ;
name = "network" ;
use_fast_approximations = 1 ;
estimate_simpler_diag_hessian = 0 ;
expdir = "" ;
random_gen = *10   )
;
batch_size = 10 ;
reset_seed_upon_train = 0 ;
cost_ports = 1 [ "NLL" ] ;
input_ports = 1 [ "input" ] ;
target_ports = 1 [ "target" ] ;
weight_ports = []
;
mbatch_size = 10 ;
seed = 1827 ;
stage = 4000 ;
n_examples = 200 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 4000 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1000 ;
use_a_separate_random_generator_for_testing = 1827  )
;
perf_evaluators = {};
report_stats = 1 ;
save_initial_tester = 1 ;
save_stat_collectors = 1 ;
save_learners = 1 ;
save_initial_learners = 0 ;
save_data_sets = 0 ;
save_test_outputs = 0 ;
call_forget_in_run = 1 ;
save_test_costs = 0 ;
provide_learner_expdir = 0 ;
should_train = 1 ;
should_test = 1 ;
template_stats_collector = *0 ;
global_template_stats_collector = *0 ;
final_commands = []
;
save_test_confidence = 0 ;
enforce_clean_expdir = 1  )
;
option_fields = 1 [ "nstages" ] ;
dont_restart_upon_change = 10 [ "module.forward_to" "nstages" "cost_ports" "target_ports" "module.modules[1].modules[0].cd_learning_rate" "module.modules[2].modules[1].cd_learning_rate" "module.modules[2].modules[0].grad_learning_rate" "module.modules[2].modules[1].grad_learning_rate" "module.modules[2].modules[2].start_learning_rate" "nstages" ] ;
strategy = 9 [ *27 ->HyperSetOption(
option_name = "" ;
option_value = "" ;
options = 4 [ ("module.forward_to" , "network_0" )("nstages" , "1000" )("cost_ports" , "[]" )("target_ports" , "[]" )] ;
call_build = 1  )
*28 ->HyperRetrain(
splitter = *0 ;
provide_tester_expdir = 0 ;
call_forget = 0  )
*29 ->HyperSetOption(
option_name = "" ;
option_value = "" ;
options = 5 [ ("module.forward_to" , "network_1" )("nstages" , "2000" )("cost_ports" , "[]" )("target_ports" , "[]" )("module.modules[1].modules[0].cd_learning_rate" , "0" )] ;
call_build = 1  )
*30 ->HyperRetrain(
splitter = *0 ;
provide_tester_expdir = 0 ;
call_forget = 0  )
*31 ->HyperSetOption(
option_name = "" ;
option_value = "" ;
options = 4 [ ("module.forward_to" , "network_2" )("cost_ports" , "[ \"NLL\" ]" )("target_ports" , "[ \"target\" ]" )("module.modules[2].modules[1].cd_learning_rate" , "0" )] ;
call_build = 1  )
*32 ->HyperSetOption(
option_name = "" ;
option_value = "" ;
options = 4 [ ("module.modules[2].modules[0].grad_learning_rate" , "0.01" )("module.modules[2].modules[1].grad_learning_rate" , "0.01" )("module.modules[2].modules[2].start_learning_rate" , "0.01" )("nstages" , "2000" )] ;
call_build = 0  )
*33 ->HyperRetrain(
splitter = *0 ;
provide_tester_expdir = 0 ;
call_forget = 0  )
*34 ->HyperSetOption(
option_name = "" ;
option_value = "" ;
options = 4 [ ("module.modules[2].modules[0].grad_learning_rate" , "0.001" )("module.modules[2].modules[1].grad_learning_rate" , "0.001" )("module.modules[2].modules[2].start_learning_rate" , "0.001" )("nstages" , "4000" )] ;
call_build = 0  )
*35 ->HyperRetrain(
splitter = *0 ;
provide_tester_expdir = 0 ;
call_forget = 0  )
] ;
provide_strategy_expdir = 1 ;
save_final_learner = 0 ;
learner = *5  ;
provide_learner_expdir = 0 ;
expdir_append = "" ;
stage = 1 ;
n_examples = 200 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827  )
