%% -*- mode:latex; tex-open-quote:"\\og{}"; tex-close-quote:"\\fg{}" -*-
%%
%%  Copyright (c) 1998-2002 by Pascal Vincent
%%
%%  $Id$

\documentclass[11pt]{book}
\usepackage{t1enc}              % new font encoding  (hyphenate words w/accents)
% \usepackage{ae}                 % use virtual fonts for getting good PDF
\usepackage{html}                % support URLs properly
\usepackage{hyperref}

\addtolength{\textheight}{3.5cm}
\addtolength{\topmargin}{-1.8cm}
\addtolength{\textwidth}{2.5cm}
\addtolength{\oddsidemargin}{-1cm}
\addtolength{\evensidemargin}{-1.5cm}


%%%%%%%%% Definitions %%%%%%%%%%%%
\newcommand{\PLearn}{{\bf \it PLearn}}
\newcommand{\Object}{{\bf Object}}
\newcommand{\Learner}{{\bf Learner}} 
\newcommand{\PPointable}{{\bf PPointable}} 
\newcommand{\Var}{{\tt Var}}
\newcommand{\RandomVar}{{\tt RandomVar}}

\parskip=2mm
\parindent=0mm

\title{\Huge PLearn Programmer's Guide\\ \Large A programmer's view of the Plearn C++ Machine-Learning Library and tools}

\begin{document}

%%%%%%%% Title Page %%%%%%%%%%
\pagenumbering{roman}
\thispagestyle{empty}

\maketitle

\pagebreak

\vspace*{10cm}



Copyright \copyright\ 1998-2002 Pascal Vincent, Yoshua Bengio \\
Copyright \copyright\ 2004 Martin Monperrus \\
Copyright \copyright\ 2007 Hugo Larochelle \\

Permission is granted to copy and distribute this document in any medium,
with or without modification, provided that the following conditions are
met:

\begin{enumerate}
\item Modified versions must give fair credit to all authors.
\item Modified versions may not be written with the aim to discredit, misrepresent, or otherwise taint the
      reputation of any of the above authors.
\item Modified versions must retain the above copyright notice, and append to
   it the names of the authors of the modifications, together with the years the
   modifications were written.
\item Modified versions must retain this list of conditions unaltered, 
    and may not impose any further restrictions.
\end{enumerate}


\pagebreak

%%%%%%%%% Table of contents %%%%%%%%%%%%
\addcontentsline{toc}{chapter}{\numberline{}Table of contents}
\tableofcontents

\cleardoublepage\pagebreak
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{ Overview of PLearn}
\section{ Introduction}

Machine Learning algorithms are usually described in scientific
papers in a standard mathematical formulation, often framed as an
optimization of a given cost function. PLearn is a C++ library that uses
the object-oriented and operator overloading capabilities of the C++
language to allow, among other things, to express cost functions and
their optimization as a standard C++ program, in a declarative manner
that is as close as possible to their mathematical formalization.

 Most neural-network and general machine-learning simulation
environments define their own scripting language. While it is very
tempting for every computer-scientist to craft his own language,
creating a complete, clear, efficient and bug-free language is
a horrendous task, so this is how things usually go: one starts
bulding a simple scripting syntax (typically lisp-like because
it's easy to parse) to specify simple experiments. Quickly it
appears too limited, and it may grow to include loops, functions,
data structures, etc. Eventually it ends up including some sort of
support for object-oriented programming, and finally for efficiency
you want it to be compiled rather than interpreted! In the end,
you end up with a huge mess of a system that was not designed to
grow that much from the beginning, and which is often impossible to
comprehend and maintain for anybody but its author. The end-result
might sometimes be impressive, but at the cost of a lot of efforts
diverted from your actual research. While C++ is far from being the
perfect language, it is very powerful, can be both very expressive and
generate highly efficient code, and most of all it has the immense
advantage of being developed and well-supported by worldwide teams
of dedicated and competent people...


When providing the correct type abstractions, C++ can be an
expressive-enough language to directly serve as a highly customizable,
extensible and efficient ``scripting language'' for designing and
running even the most demanding experiments in machine-learning research
and development. So this is what this library is all about: providing
the right type abstractions. What originally got me started on this
project was the desire to be able to optimize a complex cost function
by just expressing it in a declarative way as close as possible to the
mathematical formulation. This lead to the original implementation of
the Var class. Since then PLearn has grown to include many other useful
types and abstractions.


While PLearn has been successfully used by several people for over
a year, it is still very much work in progress. As its primary use
is for our own research, we did not want to carve it in stone: thus
future versions may look quite different from this one, as we are still
reworking the class hierarchy. But it is nevertheless already very
usable, so feel free to play around and experiment with it!


Probably the biggest problem, like with many projects of this kind, is
the cruel lack of documentation. This manual will attempt to give you
a high-level understanding of the basic concepts, but to work out the
details, you'll have to look at the actual code. Also, to fully use the
potential of this library, you are expected to be somewhat comfortable
with the C++ language.


Have fun!


Pascal

\iffalse

\section{Developer CVS access}

If you are going to contribute to PLearn on Berlios
(\url{http://www.berlios.de}):
\begin{itemize}
\item If you don't have one already, create a Berlios account for yourself
\item Send me ({\tt plearner@users.berlios.de}) your account login,
so that I can add you to the developer list.
\item Make sure the {\tt CVS\_RSH} environment variable is set to
{\tt ssh} in your .cshrc or .bashrc
\item Check-out PLearn as follows(Don't forget to change USERNAME to your username):
\begin{verbatim}
svn checkout svn+ssh://USERNAME@svn.berlios.de/svnroot/repos/plearn/trunk PLearn

\end{verbatim}
\item the current devellopment is on Belios, but in the past Sourceforge was used.
\end{itemize}

\fi

\section{Additional tools for developers}

In addition, if you wish to develop new learning algorithms, or otherwise
 contribute to the librery, the following tools will be useful:
\begin{itemize}
\item {\bf ssh} for write access to the SourceForge CVS repository.
\item {\bf gdb} for basic debugging (or a {\em better} debugger if you have one!)
\item {\bf valgrind} a wonderful free tool for memory-bug hunting.
\item {\bf python} for running python scripts, as well as the PLearn build system
\item {\bf perl} for running perl scripts
\item {\bf LaTeX, pdflatex, dvips, latex2html, doxygen} to re-generate the documentation. 
\end{itemize}



\chapter{Basics}

\section{PLearn for Matrix-Vectors Operations}

PLearn has its own vector and matrix data structures.
The files \texttt{PLearn/plearn/math/TVec\_\{decl,math\}.h}
contain the declaration and implementation of the
vector class template\texttt{TVec}, and the matrix
class template \texttt{TMat} can be found
in files \texttt{PLearn/plearn/math/TMat\_\{decl,math\}.h}.

\subsection{Creation and Basic Manipulations}

The PLearn vector and matrix data structures
are easy to instantiate and
support many useful basic operations, such
as subvector and submatrix access.

Here is a concrete example of how to use
these data structures.
The data type \texttt{Vec} and \texttt{Mat} refer to the \texttt{TVec<real>}
and \texttt{TMat<real>} classes, where \texttt{real} is a macro
corresponding either to \texttt{double} or \texttt{float}, depending
on the compilation options used.

\begin{verbatim}
#include <plearn/math/TMat_maths.h>
using namespace PLearn;

int main(int argc, char** argv)
{
    // Example use of a `real' variable
    // a compilation option makes it either a double or a float
    // Please don't use double nor float

    real a=15;
    cout<<"a="<<a<<endl;
    // Output:
    // a=15

    // Vector creation
    Vec b(3);
    b[0] = 2;
    b[1] = 42;
    b[2] = 21;
    cout<<"b="<<b<<endl;
    cout<<"b.length()="<<b.length()<<endl;
    // Output:
    // b=2           42          21
    // b.length()=3

    // Vector manipulations:

    // Subvector access of the last two elements (not a copy!!!)
    Vec b3 = b.subVec(1,2);
    cout<<"b3="<<b3<<endl;
    // Output:
    // b3=42          21

    // Concatenation
    Vec b4 = concat(b,b);
    cout<<"b4="<<b4<<endl;
    // Output:
    // b4=2           42          21          2           42          21

    // Note: "=" operator does not copy!!!
    Vec b5 = b4;
    b5[1] = 100000;
    cout<<"b4="<<b4<<endl;
    cout<<"b5="<<b5<<endl;
    // Output:
    // b4=2           100000      21          2           42          21
    // b5=2           100000      21          2           42          21
    

    // Copy
    b5 = b4.copy();
    b5[1]=1;
    cout<<"b4="<<b4<<endl;
    cout<<"b5="<<b5<<endl;
    // Output:
    // b4=2           100000      21          2           42          21
    // b5=2           1      21          2           42          21


    // Fill in one element
    Vec b6(b4.length());
    b6.fill(3);
    cout<<"b6="<<b6<<endl;
    // Output:
    // b6=3           3           3           3           3           3

    // Fill in elements of another vector
    b6 << b4;
    cout<<"b6="<<b6<<endl;
    // Output:
    // b6=2           100000      21          2           42          21

    // Clear
    b6.clear();
    cout<<"b6="<<b6<<endl;
    // Output:
    // b6=0           0           0           0           0           0

    // Resize
    b4.resize(7);
    b4[6] = 6;
    cout<<"b4="<<b4<<endl;
    // Output:
    // b4=2           100000      21          2           42          21          6
    
    b4.resize(4);
    cout<<"b4="<<b4<<endl;
    // Output:
    // b4=2           100000      21          2


    // Matrix creation : 
    Mat c(3,2);
    c(1,1)=1.1;
    c(1,0)=4;
    c(2,0)=5;
    c(0,1)=-73.2;
    c(0,0)=78;
    c(2,1)=5.32e-2;
    cout<<"c=\n"<<c<<endl;
    cout<<"c.length()="<<c.length()<<endl;
    cout<<"c.width()="<<c.width()<<endl;
    // Output:
    // c=
    // 78          -73.2
    // 4           1.1
    // 5           0.0532
    //
    // c.length()=3
    // c.width()=2

    // Matrix manipulation:
   
    // Submatrix access (not a copy!!!)...

    // ... of the last two rows and first column
    Mat c3 = c.subMat(1,0,2,1);
    cout<<"c3=\n"<<c3<<endl;
    // Output:
    // c3=
    // 4
    // 5

    // ... of the second column
    Mat c4 = c.column(1);
    cout<<"c4=\n"<<c4<<endl;
    // Output:
    // c4=
    // -73.2
    // 1.1
    // 0.0532

    // ... of the third row
    Mat c5 = c.row(2);
    cout<<"c5=\n"<<c5<<endl;
    // Output:
    // c5=
    // 5           0.0532

    // .. of the third row, as a vector
    Vec b7 = c(2);
    cout<<"b7="<<b7<<endl;
    // Output:
    // b7=5           0.0532

    // Note: "=" operator does not copy!!!
    Mat c6 = c;
    c6(1,1) = 100000;
    cout<<"c=\n"<<c<<endl;
    cout<<"c6=\n"<<c6<<endl;
    // Output:
    // c=
    // 78          -73.2
    // 4           100000
    // 5           0.0532
    //
    // c6=
    // 78          -73.2
    // 4           100000
    // 5           0.0532

    // Copy
    c6 = c.copy();
    c6(1,1) = 1;
    cout<<"c=\n"<<c<<endl;
    cout<<"c6=\n"<<c6<<endl;
    // Output:
    // c=
    // 78          -73.2
    // 4           100000
    // 5           0.0532
    //
    // c6=
    // 78          -73.2
    // 4           1
    // 5           0.0532

    // Fill in one element
    Mat c7(c.length(),c.width());
    c7.fill(3);
    cout<<"c7=\n"<<c7<<endl;
    // Output:
    // c7=
    // 3           3
    // 3           3
    // 3           3

    // Fill in elements of another matrix
    c7 << c;
    cout<<"c7=\n"<<c7<<endl;
    // Output:
    // c7=
    // 78          -73.2
    // 4           100000
    // 5           0.0532

    // Fill in a row of another matrix
    c7(2) << c(1);
    cout<<"c7=\n"<<c7<<endl;
    // Output:
    // c7=
    // 78          -73.2
    // 4           100000
    // 5           0.0532

    // Clear
    c7.clear();
    cout<<"c7=\n"<<c7<<endl;
    // Output:
    // c7=
    // 0           0
    // 0           0
    // 0           0

    // Resize
    c7.resize(4,4);
    c7.subMat(0,2,3,2)<<c.subMat(0,0,3,2);
    c7(3,0)=0.01;
    c7(3,1)=0.02;
    c7(3,2)=0.03;
    c7(3,3)=0.04;
    cout<<"c7=\n"<<c7<<endl;
    // Output:
    // c7=
    // 0           0           78          -73.2
    // 0           0           4           100000
    // 0           0           5           0.0532
    // 0.01        0.02        0.03        0.04


    c7.resize(2,3);
    cout<<"c7=\n"<<c7<<endl;
    // Output:
    // c7=
    // 0           0           78
    // 0           0           4

    return 0;
}

\end{verbatim}


For other useful methods for \texttt{TVec} and
\texttt{TMat} and more details on their implementation,
see files \texttt{PLearn/plearn/math/TVec\_\{decl,math\}.h}
and \texttt{PLearn/plearn/math/TMat\_\{decl,math\}.h}

\subsection{Mathematical Manipulations}

Though you might want to implement certain mathematical functions or operators
yourself, many mathematical manipulations for \texttt{TVec}
and \texttt{TMat} are already implemented in PLearn. 

In \texttt{PLearn/plearn/math/TMat\_maths\_impl.h},
many mathematical operators, such as  \texttt{+}, \texttt{-}, \texttt{*}, 
\texttt{/}, \texttt{+=}, \texttt{-=}, \texttt{*=} and \texttt{/=} are already overloaded.
When using \texttt{+}, \texttt{-}, \texttt{*} or
\texttt{/}, a new vector/matrix is created as the result of the operation,
and when using \texttt{+=}, \texttt{-=}, \texttt{*=}, \texttt{/=}, the operand on
the left is modified and no object is created. Also, many vector/matrix products
are implemented. Given the vector $x$ and $y$ and the matrices $A$, $B$ and $C$:

\begin{itemize}
\item \texttt{dot(x,y)} computes $x' y$
\item \texttt{product(y,A,x)} computes $y$ such that $A x = y$
\item \texttt{transposeProduct(y,A,x)} computes $y$ such that $A' x = y$
\item \texttt{product(C,A,B)} computes $C$ such that $A B = C$
\item \texttt{transposeProduct(C,A,B)} computes $C$ such that $A' B = C$
\item \texttt{externalProduct(A,x,y)} computes $A$ such that $x y' = A$
\end{itemize}

Moreover, the functions \texttt{productAcc}, \texttt{transposeProductAcc} and \texttt{externalProductAcc}
perform the same operations but accumulate the result of the computations in 
the modified data structure instead of overwriting what it initially contained.
For example, the computation of $A x + A y = z$ can be done by the following
calls: \texttt{product(z,A,x)} followed by \texttt{productAcc(z,A,y)}.

Many other standard functions can be found in \texttt{PLearn/plearn/math/TMat\_maths\_impl.h}. 
The most popular are probably \texttt{sign}, \texttt{max}, 
\texttt{argmax}, \texttt{min}, \texttt{argmin},
\texttt{softmax}, \texttt{exp}, \texttt{abs}, \texttt{log}, 
\texttt{logadd}, \texttt{sqrt}, \texttt{sigmoid} and \texttt{tanh}.

When considering to implement a given mathematical function on
vectors and matrices in PLearn, some time can be saved by first 
looking in \texttt{PLearn/plearn/math/TMat\_maths\_impl.h} 
in order to verify whether it has already been implemented.

In \texttt{PLearn/plearn/math/TMat\_maths\_specialisation.h}, optimized
versions of vector/matrix operators for specific data types and relying on the BLAS library
can be found.
Also, in \texttt{PLearn/plearn/math/plapack.h}, other specialized functions for vectors
and matrices (matrix inverse, eigenvalue and singular value decomposition, linear system
solver, etc.) relying on the LAPACK library can also be found.


\subsection{Loading and saving}
You can load and save a Mat with the following code (VMat.h
must be included):

\begin{verbatim}

#include <plearn/math/TMat_maths.h>
#include <plearn/var/Var_all.h>
#include <plearn/vmat/VMat.h>
#include <plearn/db/getDataSet.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Mat c(3,2);
    c(1,1)=1.0;
    c(1,0)=4.0;
    c(2,0)=5.0;
    c(0,1)=73.0;
    c(0,0)=78.0;
    c(2,1)=5.0;

    // save into a pmat file
    c.save("save.pmat");

    // save into an amat file
    VMat vm(c);
    vm->saveAMAT("save.amat");

    // load from a file
    VMat vm2 = getDataSet("save.pmat");
        // it could have been "save.amat"
    Mat m = vm2.toMat();
    cout<<m;

    return 0;
}

\end{verbatim}



\section{How to create a PLearner?}

PLearner is the super class for learner. Here we describe how to create
a PLearner. PLearner is a subclass of Object so if you want to know more
about what you are doing, go to section \ref{Object}.

\subsection{What?}

A {\tt PLearner} is an object intended to {\em learn} some structure in
the data that is provided to it during a {\em training phase}, and use
this knowledge to do some inference on (usually new) data during a {\em
testing phase}.

A typical training phase includes:
\begin{itemize}
  \item setting some options to control the behaviour during learning;

  \item providing the data the algorithm will learn from. This data
  might include {\em targets} if the goal is to perform classification
  or regression for example (but not for density estimation), and can
  include per-sample {\em weights};

  \item calling the method {\tt train()}, that performs the actual learning.
\end{itemize}

At this point, the PLearner is ready to be used on {\em test} data. You can:
\begin{itemize}

  \item compute an output value from a given test input (a trained
  PLearner can be used to process data);

  \item compute an output value and a cost from a given test input and
  expected target. This can be useful to test the error of the algorithm
  on new data, but a cost can as well be a measure of uncertainty or a
  reconstruction error;

  \item perform this last operation on a whole dataset and accumulate
  statistics.

\end{itemize}

\subsection{Where?}

If your learner is experimental, and at least until it compiles and
work perfectly under every situation, you should not commit it in {\tt
\$PLEARNDIR/plearn\_learners} with the other ones, but it is still a
good idea to put it in the version tracking system, and to commit often.
When your learner works robustly, you can move it into the corresponding
subdirectory of {\tt plearn\_learners}.

If you have an account on LisaPLearn, the best is to use
{\tt \$LISAPLEARNDIR/UserExp/your\_login/any\_path},
if you don't you can create a subdirectory in {\tt
\$PLEARNDIR/plearn\_learners\_experimental} and use it as a working
directory.

\subsection{How?}

Here is a step-by-step example of how to implement {\tt MyLearner}:

\begin{enumerate}
\item Once you are in your working directory, type:
\begin{verbatim}
$ pyskeleton PLearner MyLearner
\end{verbatim}
where {\tt MyLearner} is the name of the PLearner you want to create.

This will create two files, {\tt MyLearner.h} and {\tt MyLearner.cc},
from a template. These files contain the prototypes of the methods you
need to implement in order to follow the PLearner interface, and some
comments to help you filling them.

\item Edit {\tt MyLearner.h}, and add (possibly short) Doxygen
documentation about what your learner is supposed to do. Something like:
\begin{verbatim}
namespace PLearn {

/**
 * Learns the meaning of life.
 * This class learns how to find the meaning of life through the application
 * of stochastic methods. Tests can be performed on several 42-dimensions
 * vectors.
 *
 * @todo Make God fit into this framework.
 *
 */
class MyLearner : public PLearner
\end{verbatim}

\item Declare your public options. These will typically be the
hyperparameters of your algorithm, and the options allowing to switch
between different methods. These are the options the user will need
to provide for your algorithm to know what to do, but they can change
during the learning phase. Don't forget to put comments:
\begin{verbatim}
class MyLearner : public PLearner
{
    typedef PLearner inherited;

public:
    //#####  Public Build Options  ############################################

    //! ### declare public option fields (such as build options) here
    //! Start your comments with Doxygen-compatible comments such as //!

    //! Initial parameters, specified by the user
    Vec init_params;

    /**
     * Method to use for performing learning.
     * One of:
     *   - "none": use raw data
     *   - "first": first method
     *   - "second": second method
     */
    string learning_method;
\end{verbatim}
You can skip the ``public methods'' section for the moment.

\item Declare your protected options. These will typically be parameters
learned from your data, or from the public options (cached to avoid
always accessing them).
\begin{verbatim}
protected:
    //#####  Protected Options  ###############################################

    // ### Declare protected option fields (such as learned parameters) here

    //! Number of initial parameters
    int nparams;

    //! 'learning_method' as number: 0 for none, 1 for first, 2 for second
    int method;

    //! Learned parameters: a matrix of size (nparams * inputsize())
    // (inputsize() is a method of PLearner)
    Mat learned_params;
\end{verbatim}

\item Declare other variables you will need during learning or
computations, and you don't want to reallocate each time. This members
can be protected or private, depending if your subclasses are likely to
use them.
\begin{verbatim}
    //####  Not Options  ######################################################
    //! Stores intermediate results
    Vec tmp;
\end{verbatim}

\item Edit {\tt MyLearner.cc}. First, fill the PLearn documentation of
the class. This is usually the same as the doxygen one.
\begin{verbatim}
namespace PLearn {
using namespace std;

PLEARN_IMPLEMENT_OBJECT(
    MyLearner,
    "Learns the meaning of life.",
    "This class learns how to find the meaning of life through the"
    " application\n"
    "of stochastic methods. Tests can be performed on several 42-dimensions\n"
    "vectors.\n");
\end{verbatim}

\item Write the default constructor. First, initialize all fields which
need it (such as int, bool, real...), and the ones you want to, to a
default value. You can skip some (like the Vec and Mat, that have a
reasonable default constructor), but you have to initialize the fields
in the same order you declared them.
\begin{verbatim}
MyLearner::MyLearner()
    : learning_method("none"),
      nparams(-1),
      method(0),
      tmp(42)
{
}
\end{verbatim}

The comment says you may want to call {\tt build\_()} to finish the
construction of the object. For this default constructor, you probably
don't want to do it, because {\tt build\_()} will be called anyway after
setting the actual option values.

\item {\em [Optional]} Write other constructors. You can write
new constructors, that would take (for example) as arguments all
the parameters needed to build the learner completely. In such a
constructor, you may want to call {\tt build\_()} (so you are sure
everything is usable right after construction) or build everything
\begin{verbatim}
// In MyLearner.h
MyLearner( Vec the_init_params, string the_learning_method = "none" );

// In MyLearner.cc
// If everything is in the constructor:
MyLearner::MyLearner( Vec the_init_params, string the_learning_method );
    : init_params(the_init_params),
      learning_method(the_learning_method),
      nparams(the_init_params.length()),
      method(-1),
      learned_params(nparams, max(0,inputsize())),
      tmp(42)
{
    learning_method = lowerstring(learning_method);
    if( learning_method == "none" )
        method = 0;
    else if( learning_method == "first" )
        method = 1;
    else if( learning_method == "second" )
        method = 2;
    else
        PLERROR("MyLearner - learning_method '%s' is unknown.",
                learning_method.c_str());
}

// If we prefer to call build():
MyLearner::MyLearner( Vec the_init_params, string the_learning_method );
    : init_params(the_init_params),
      learning_method(the_learning_method),
      nparams(-1), method(-1)
{
    // We are not sure inherited::build() has been called, so:
    build();
}
\end{verbatim}

\item Now, declare (in the sense of PLearn) the options of the learner,
as you do for any Object. The options we had in section ``Public Build
Option'' will be labeled {\tt buildoption}, the ones in ``Protected
Option'' will be labeled {\tt learntoption}, and the ones in ``Not
Options'' will {\em not} be declared.
\begin{verbatim}
void MyLearner::declareOptions(OptionList& ol)
{
    // ### Declare all of this object's options here.
    // ### For the "flags" of each option, you should typically specify
    // ### one of OptionBase::buildoption, OptionBase::learntoption or
    // ### OptionBase::tuningoption. If you don't provide one of these three,
    // ### this option will be ignored when loading values from a script.
    // ### You can also combine flags, for example with OptionBase::nosave:
    // ### (OptionBase::buildoption | OptionBase::nosave)

    // First, the public build options
    declareOption(ol, "init_params", &MyLearner::init_params,
                  OptionBase::buildoption,
                  "Initial parameters");

    declareOption(ol, "learning_method", &MyLearner::learning_method,
                  OptionBase::buildoption,
                  "Method to use for performing learning.\n"
                  "One of:\n"
                  "  - "none": use raw data\n"
                  "  - "first": first method\n"
                  "  - "second": second method\n");

    // Then, the learned options
    declareOption(ol, "nparams", &MyLearner::nparams,
                  OptionBase::learntoption,
                  "Number of initial parameters");

    declareOption(ol, "method", &MyLearner::method,
                  OptionBase::learntoption,
                  "'learning_method' as a number:\n"
                  "0 for none, 1 for first, 2 for second.\n");

    declareOption(ol, "learned_params", &MyLearner::learned_params,
                  OptionBase::learntoption,
                  "Learned parameters: a matrix of size (nparams *"
                  " inputsize())");

    // Now call the parent class' declareOptions
    inherited::declareOptions(ol);
}
\end{verbatim}

\item Now, if you include your header in {\tt plearn\_inc.h}:
\begin{verbatim}
#include <plearn_learners_experimental/some_path/MyLearner.h>
\end{verbatim}
you should be able to compile {\tt plearn}, and to have help of all the
``buildoption'' options when typing:
\begin{verbatim}
$ plearn help MyLearner
\end{verbatim}
Options that are not labeled ``buildoption'', such as the
``learntoption'' options defined above, do not appear: it is not
necessary to provide them (it could even confuse your learner if their
values are inconsistent).

\item Now, let's implement a basic version of the {\tt build\_()}
method. It is intended to let you test (and debug) your class in a few
easy situations, but you will have to rewrite a more complete version
later (see section \ref{plearner_build_}), so that it works correctly in
every case.

The goal of {\tt build()} is to ensure that the object is in a
consistent state, and ready to be used. This method calls {\tt
inherited::build()} (in our case, {\tt PLearner::build()}), and then
{\tt build\_()}, which we have to implement. It is called in various
situations, so a correct version of {\tt build\_()} should check
everything. Now, we will only focus one simple scenario, where the
sequence of methods called is:
\begin{itemize}
  \item {\tt MyLearner()},
  \item {\tt build()},
  \item {\tt setOption(...)} possibly on every build option,
  \item {\tt build()},
  \item {\tt setTrainingSet( some\_trainset )},
  \item {\tt build()}.
\end{itemize}

The first call to {\tt build\_()} can be used to do some initializations
that do not fit in the default constructor, or thar use the default
values of parent object (PLearner), for example. The first and second
calls set the values of the parameters learned from build options. Here,
we resize {\tt learned\_params} only if the parameter {\tt inputsize\_}
is positive (meaning the input size of the learner have been set).

\begin{verbatim}
//! @todo rewrite this method to work in every case
void MyLearner::build_()
{
    // ### This method should do the real building of the object,
    // ### according to set 'options', in *any* situation.
    // ### Typical situations include:
    // ###  - Initial building of an object from a few user-specified options
    // ###  - Building of a "reloaded" object: i.e. from the complete set of
    // ###    all serialised options.
    // ###  - Updating or "re-building" of an object after a few "tuning"
    // ###    options have been modified.
    // ### You should assume that the parent class' build_() has already been
    // ### called.

    nparams = init_params.length();

    if( inputsize_ > 0 )
        learned_params.resize(nparams, inputsize());

    learning_method = lowerstring(learning_method);
    if( learning_method == "none" )
        method = 0;
    else if( learning_method == "first" )
        method = 1;
    else if( learning_method == "second" )
        method = 2;
    else
        PLERROR("MyLearner - learning_method '%s' is unknown.",
                learning_method.c_str());
}
\end{verbatim}

The member {\tt PLearner::inputsize\_} is equal to the size of the
elements the learner takes as input, or $-1$ if they are not
set (or variable). There are two possibilities to have it set:
calling {\tt setTrainingSet(...)} on some {\tt VMat} (see section
\ref{plearner_vmat}), in that case it will be set to the inputsize of
that {\tt VMat}, or having it set as an option (or read from a script).

In the function above, if {\tt inputsize\_} is set, no matter how, it
will resize {\tt learned\_params}, and that is what we want: always use
all the informations available.

\item Since PLearn uses smart pointers (see section \ref{PP}), when
we make a copy of an Object, it is by default a ``shallow'' copy,
meaning that the pointers are copied, but still point to the same actual
data. Each Object (hence each PLearner) class has to implement a method
called {\tt makeDeepCopyFromShallowCopy} that creates a real, ``deep''
copy of every field we have a pointer to (and recursively).

In this step, we ensure that every member of our PLearner that
is (or contain) a smart pointer (a {\tt PP<something>}) will be
``deepCopied''. Usually, it concerns the members of type {\tt
TVec<something>}, {\tt Vec}, {\tt TMat<something>}, {\tt Mat} and of
course {\tt PP<something>}. If you use classes defined elsewhere,
be careful that a class name could be a {\tt typedef} to {\tt
PP<something>}: for instance a {\tt VMat} is a {PP<VMatrix>}, so you
would have to call {\tt deepCopy} on it.

\begin{verbatim}
void MyLearner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
{
    inherited::makeDeepCopyFromShallowCopy(copies);

    // ### Call deepCopyField on all "pointer-like" fields
    // ### that you wish to be deepCopied rather than
    // ### shallow-copied.
    // ### ex:
    // deepCopyField(trainvec, copies);

    deepCopyField(init_params, copies);
    deepCopyField(learned_params, copies);
    deepCopyField(tmp, copies);
}
\end{verbatim}
Don't forget to remove these lines when finished:
\begin{verbatim}
    // ### Remove this line when you have fully implemented this method.
    PLERROR("StackedModulesLearner::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
\end{verbatim}

\item We will now implement the methods that are specific to
PLearner. Let's begin with {\tt outputsize()}. Whereas the value
of {\tt inputsize()}, {\tt targetsize()} and {\tt weightsize()}
will be automatically set from the training set's sizes (see
\ref{plearner_vmat}), you have to define the output size of your
learner. It can depend on {\tt inputsize()} and other parameters, but
you should not change it during the learning or testing phase.
\begin{verbatim}
int MyLearner::outputsize() const
{
    // Compute and return the size of this learner's output (which typically
    // may depend on its inputsize(), targetsize() and set options).
    if( method == 0 )
        return 42;
    else if( method == 1 )
        return inputsize()+1;
    else if( method == 2 )
        return inputsize()*2;
    else
    {
        PLERROR("MyLearner::outputsize() - method '%i' is unknown.\n"
                "Did you call 'build()'?\n", method);
        return 0; // to avoid warning, we must return in every case
    }
}
\end{verbatim}

\item The method {\tt forget()} is used to forget everything the
PLearner learned during the training phase. It can be called when
changing the training set (for the first training set, or if what
it learned with one data set is not usable with another one), or if
we want to try to learn with different parameters, for example. The
``learntoption'' parameters we have set during {\tt build\_()} are not
affected.
\begin{verbatim}
void MyLearner::forget()
{
    //! (Re-)initialize the PLearner in its fresh state (that state may depend
    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
    //! a fresh learner!)
    /*!
      A typical forget() method should do the following:
      - initialize a random number generator with the seed option
      - initialize the learner's parameters, using this random generator
      - stage = 0
    */
    learned_params.clear();
    stage = 0;
}
\end{verbatim}

\item The method {\tt computeOutput(input, output)} computes an {\em
output} vector from the {\em input} part of a data point. It is
routinely called when the PLearner is trained (because it is what you
train it for!), but can also be used during training. The implementation
really depends on what you want your learner to do.

\begin{verbatim}
void MyLearner::computeOutput(const Vec& input, Vec& output) const
{
    // Compute the output from the input.
    int nout = outputsize();
    output.resize(nout);

    if( method == 0 )
    {
        tmp += sum(input);
        output << tmp;
    }
    else if( method == 1 )
    {
        output.subVec(0,input.length()) << input;
        output[ inputsize() ] = sum(tmp);
    }
    else if( method == 2 )
    {
        if( inputsize() != 42 )
            PLERROR("MyLearner::computeOutput: inputsize() is '%d', but\n"
                    "Learning method 'second' only works when inputsize() =="
                    " 42.\n", inputsize());
        output.subVec(0,42) << input;
        output.subVec(42,42) << tmp;
    }
}
\end{verbatim}

\item The method {\tt computeCostsFromOutput(...)} is usually called
after {\tt computeOutput}, because it computes the costs from {\em
already} computed outputs, knowing the actual target (if any). Note that
you can have several costs, each of one being a scalar. Typical costs
are squared distances between output and target, NLL, hinge loss...
It mainly depends on how you measure the performance of your learning
algorithm.

Do not forget the ``const'' at the end of the declaration line.
\begin{verbatim}
void MyLearner::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                        const Vec& target, Vec& costs) const
{
    // Compute the costs from *already* computed output.
    costs.resize(1);
    costs[0] = powdistance(output, target, 2); // squared error
}
\end{verbatim}

\item {\em [Optional]} The method {\tt computeOutputAndCosts(...)},
as it names tells, computes both the output vector and the costs,
from an input vector and the corresponding target. There is a default
implementation in the parent {\tt PLearner} class, that calls {\tt
computeOutput} and then {\tt computeCostsFromOutput}, but you may want
to reimplement it to improve efficiency.

It would be the case if computing the output gives you the costs as
well (if you don't need the target). Then, you may want to implement
{\tt computeOutputAndCosts}, and make {\tt computeOutput} and {\tt
computeCostsFromOutput} call it.

You may also want to reimplement the method {\tt computeCostsOnly}.

\item The method {\tt train()} performs the actual training. Its
implementation will mostly depend on your algorithm. The pseudo-code
included in {\tt MyLearner.cc} will give you an idea of the general
structure of a {\tt train()} method, but it is possible that your
learner doesn't really fit in this.

Vague advice: don't hesitate to add helper functions or routines
to prevent {\tt train()} from being too big an monolithic;
keeping statistics during training can be useful; you can use
{\tt computeOutput}, {\tt computeOutputAndCosts} and {\tt
computeCostsFromOutput} from inside {\tt train()} to
avoid code duplication (but it might be impossible for some learners).

\begin{verbatim}
void MyLearner::train()
{
    // The role of the train method is to bring the learner up to
    // stage==nstages, updating train_stats with training costs measured
    // on-line in the process.

    static Vec input;  // static so we don't reallocate memory each time...
    static Vec target; // (but be careful that static means shared!)
    static Vec train_costs;

    input.resize(inputsize());    // the train_set's inputsize()
    target.resize(targetsize());  // the train_set's targetsize()
    real weight;

    // This generic PLearner method does a number of standard stuff useful for
    // (almost) any learner, and return 'false' if no training should take
    // place. See PLearner.h for more details.
    if (!initTrain())
        return;

    // learn until we arrive at desired stage
    for( ; stage < nstages ; stage ++ )
    {
        // clear stats of previous epoch
        train_stats->forget();

        // loop over all examples in train set
        for( int sample=0 ; sample<nsamples ; sample++ )
        {
            train_set->getExample(sample, input, target, weight);
            computeOutputAndCosts(input, target, output, train_costs);
            // keep statistics of costs
            train_stats->update(train_costs);

            // minimize the cost on current sample, modifying learned_params
            // this function is defined elsewhere in this file...
            minimizeByVariationalMethods(input, target, output, train_costs);
        }
        train_stats->finalize(); // finalize statistics for this epoch
    }
}
\end{verbatim}

\item {\tt getTestCostNames()} and {\tt getTrainCostNames()} return, as
a vector of strings, the names of the costs computed during testing and
during training (respectively). These values are used to interpret the
accumulated statistics.

{\tt getTrainCostNames.length()} should be equal to the {\tt
train\_costs} vector used to update the training statistics, and {\tt
getTestCostNames.length()} should be equal to the length of the {\tt
costs} vectors computed in {\tt computeCostsFromOutput}. However, the
train and test costs are not necessarily the same, and do not have
necessarily the same length.
\begin{verbatim}
TVec<string> MyLearner::getTestCostNames() const
{
    return TVec<string>(1, "squared_distance");
}

TVec<string> MyLearner::getTrainCostNames() const
{
    return getTestCostNames();
}
\end{verbatim}

\end{enumerate}

\subsection{And now?}
Now, you can compile {\tt plearn}, and try your learner in different
situations, from a program as well as from a script, with different
training sets, debug it, and see if the results seem normal. It is
usually a good thing to try it with a simple ``toy'' dataset, for
which you can do the computation by hand, in order to be sure everything
works as intended.

\subsection{\label{plearner_build_}A build\_() that works in every situation}

\begin{enumerate}

\item split the building of the different parts of your PLearner into
different functions,

\item call one of these functions as soon as you have all the elements needed,

\item it is better to compute twice the same thing at build time and
lose a bit of efficiency, than to think that everything has already been
set to the right values when one parameter has changed.

\end{enumerate}

\subsection{Useful members and methods defined in PLearner class}

\begin{enumerate}
\item {\tt inputsize()}
\item {\tt setTrainingSet()}
\item {\tt train\_set}
\item {\tt train\_stats}
\item {\tt stage}
\item {\tt nstages}
\end{enumerate}

\subsection{\label{plearner_vmat}Datasets}

In PLearn, the data structure for datasets (training, validation
and test sets) correspond to the \texttt{VMatrix} class. 
Conceptually, it corresponds
to a matrix where each row is a sample
(training or test example). The length of the \texttt{VMatrix}, that is
the number of samples it contains, is given by the
\texttt{length()} method. 

A row is divided in three parts called the input,
target and weight parts. Their respective sizes are given
by the methods \texttt{inputsize()}, \texttt{targetsize()}
and \texttt{weightsize()} of the \texttt{VMatrix} object. 
The total width of
the \texttt{VMatrix} is given by the method \texttt{width()} 
and should be equal to the sum of the input, target
and weight part sizes. The weight size should also
be either 1 or 0, that is a sample either
has a weight or does not.

There are two ways of accessing a sample of a \texttt{VMatrix}.
The method {\tt getRow(i, row)} can be called, where {\tt i}
is the index of the row and {\tt row} is a {\tt Vec} which
will be filled with the input, target and weight parts
of the $i^{th}$ sample. The length of {\tt row}
should already be set to the width of the {\texttt VMatrix}.

Another possibility is to call
{\tt getExample(i, input, target, weight)}, where
{\tt input} and {\tt target} are {\tt Vec} objects and
will be filled with the input and target parts of
the $i^{th}$ sample. The {\tt weight} variable is a reference
to a {\tt real}, which will be equal to the weight
of the $i^{th}$ sample. The {\tt input} and {\tt target}
vectors do not have to be sized according to the
{\tt VMatrix} input and target part sizes. They
will be resized appropriately by {\tt VMatrix}
(this is possible since they are passed as
references to the method).

Usually, instead of manipulating {\tt VMatrix} objects,
you will have access to a {\tt VMat} object, which
can be seen as a pointer to a {\tt VMatrix} object
and should be used as such. The {\tt VMat} class
inherits from the class {\tt PP<VMatrix>}, that
is the class of smart pointers for {\tt VMatrix} objects.
To know more about smart pointers, see section \ref{PP}.

The {\tt VMatrix} object is implemented in files
{\tt PLearn/plearn/vmat/VMatrix.\{cc,h\}}, where
you will find many more methods for this class.
However, the ones that we described in this
section will usually be sufficient for the
implementation of a {\tt PLearner}.

\subsection{Testing phase}
TODO

\subsection{How to get the dataset?}
TODO

\subsection{How to manage the dataset?}
TODO


\subsection{If you need gradients on a cost function...}

Go read the section \ref{Var} and the section \ref{Optimizer}.

%----------------------------------------------------------------------
\chapter{Intermediate}
%----------------------------------------------------------------------

\section{Low-level concepts}

\subsection{Important compilation flags}
TODO: explication

\begin{itemize}
\item BOUNDCHECK or nothing
\item USEFLOAT or USEDOUBLE
\item LITTLEENDIAN or BIGENDIAN
\end{itemize}
 Default with Pymake and Linux is BOUNDCHECK, USEDOUBLE, \linebreak
LITTLEENDIAN.

\subsection{Smart Pointers}
\label{PP}
 Memory management is one of the most error-prone aspects of traditional
C and C++ programming. PLearn makes it easier through the use of
\emph{reference-counted smart pointers}.

 Traditionally, there are two basic ways an object can typically be created: 

\begin{itemize}
\item  on the stack:
\begin{verbatim}
void f()
{   // Beginning of scope
    MyClass myinstance; 
        // memory is allocated on the stack,
        // and constructor is called

    myinstance.dosomething();
        // methods and members are called using a dot

}   // when exiting the scope, destructor of object is
    // called automatically and stack memory is freed
\end{verbatim}

\item  by calling new:
\begin{verbatim}
void f()
{   // Beginning of scope

    MyClass* ptr = new MyClass(); 
        // memory is allocated on the heap by the "new"
        // opearator, which returns a pointer

    ptr->dosomething(); 
        // methods and members are called using "->"

    delete ptr;
        // we have to call "delete" explicitly,
        // because object is NOT automatically destroyed
}       // when leaving the scope
\end{verbatim}

\end{itemize}

In more complex cases, where several ojects may contain pointers to
other objects, keeping track of when to delete an object quickly becomes
a complex and error-prone bookkeeping task.


 PLearn uses reference-counted smart pointers to automate this, so
that you don't have to worry about calling delete. It is based on a
\emph{smart pointer} template (PP which stands for PLearnPointer)
that can be used on any class that derives from SmartPointable. A
SmartPointable object contains the count of the number of smart
pointers that point to it, and is automatically destroyed when this
\emph{reference count} becomes 0 (i.e. when nobody any longer points
to it)

\begin{verbatim}
class MyClass: public SmartPointable;

void f()
{   // Beginning of first scope
    PP<MyClass> ptr = new MyClass();
        // memory is allocated on the heap
        // (reference count for object is 1)

    { // Beginning of second scope
        PP<MyClass> ptr2 = ptr;
            // ptr2 and ptr point to the same object
            // (reference count becomes 2)

    } // Object is not destroyed upon exiting the second scope 
      // (reference count becomes 1)

    ptr->dosomething();
        // methods and members are called using "->"

} // Object is automatically destroyed here 
  // when reference count becomes 0
\end{verbatim}

It is possible to mix traditional pointers to an object with smart
pointers, and there are automatic conversions between the two. However,
in general we discourage doing this, although it might prove useful in
some situations (such as to keep a pointer to the actual specific type
of the object rather than its base-class). If you do mix them, just
remember that the object will get deleted as soon as the last smart
pointer pointing to it is gone (when it gets out of scope for instance),
regardless whether there are still traditional pointers pointing to it
(the automatic reference count can only counts smart pointers!).


 Many base classes in PLearn have an associated smart pointer type
with a similar (and usually shorter) name, as shown in the following
table. Sometimes this corresponding smart pointer type is a simple
typedef to the type PP<\emph{baseclass}>.

But we also often specialised them (by deriving PP<\emph{baseclass}>)
to add operators and methods for user convenience. So that, for
instance, element at row i and column j of a VMat m can be accessed
as m(i,j) as an alternative to the more verbose m->get(i,j).

\begin{tabular}{|c|c|}
\hline
base class &smart pointer \\
 \hline
VMatrix &VMat \\
 \hline
Variable &Var \\
 \hline
RandomVariable &RandomVar \\
 \hline
Kernel &Ker \\
 \hline
CostFunction &CostFunc \\
 \hline
StatsIterator &StatsIt \\
 \hline

\end{tabular}




Several concepts in PLearn can be seen as having two levels of
implementation:

 \begin{enumerate}

\item A base class and its derived classes form the basic internal
working mechanism for the concept which can be extended by deriving new
classes. We call this the \emph{designer level}.

\item A correponding smart pointer type for the base class, and a
number of utility functions give a more user-friendly syntax to use
the concept. They are mostly wrapping and syntactic sugar around the
\emph{designer level} classes. We call this the \emph{user level}.

\end{enumerate}

The person who only wishes to use the library typically doesn't need
to understand all the details of the designer level hierarchy. Some
concepts (such as Var) can be manipulated almost entirely through the
smart pointer type and user-level functions, although knowing the most
useful methods of the uinderlying base class, and the role of each
subclass certainly doesn't harm.

\section{How to subclass a PLearn \Object}
\label{Object}

\subsection{\Object}

 PLearn defines an \Object\ class. There is not much to it. Its role
is mainly to standardise the methods for printing, saving to file, and
duplicating objects. Not all classes in PLearn are derived from Object
(many low-level classes aren't). But all non-strictly-concrete classes
(i.e. those with virtual methods) in PLearn derive from Object. This
includes the Learner base class.

\Object\ allows an easy support for a number of useful generic facilities:
\begin{itemize}
\item automatic memory management (through reference counted smart pointers: \Object\ derives from \PPointable)
\item serialization/persistence ({\tt read, write, save, load})
\item runtime type information ({\tt classname})
\item displaying ({\tt info, print})
\item deep copying ({\tt deepCopy})
\item a generic way of setting options ({\tt setOption}) and a generic
  {\tt build()} method (the combination of the two allows for instance
  to change the object structure and rebuild it at runtime)
\end{itemize}


\subsection{Creating a basic class deriving from Object}

First, you can use \texttt{pyskeleton}, a python script which creates
automatically the .h and .cc files.

\texttt{pyskeleton Object Person} creates a class called Person derived from Object.

The first thing to do is to fill the .h file.

Example:

\begin{verbatim}
...

private:
    typedef Object inherited;

protected:
    // *********************
    // * protected options *
    // *********************

    // ### declare protected option fields 
    // ###  (such as learnt parameters) here
    // ...

public:
    // here we had the good things
    string firstname;
    int age;

\end{verbatim}

Then you just have to fill the \texttt{declareOptions} method in the .cc .

\begin{verbatim}
void Person::declareOptions(OptionList& ol) 
{
    // ### Declare all of this object's options here.
    // ### For the "flags" of each option, you should typically specify
    // ### one of OptionBase::buildoption, OptionBase::learntoption or
    // ### OptionBase::tuningoption. If you don't provide one of these three,
    // ### this option will be ignored when loading values from a script.
    // ### You can also combine flags, for example with OptionBase::nosave:
    // ### (OptionBase::buildoption | OptionBase::nosave)

    // ### ex:
    declareOption(ol, "firstname", &Person::firstname,
                  OptionBase::buildoption,
                  "Help text describing this option");

    declareOption(ol, "age", &Person::age,
                  OptionBase::buildoption,
                  "Help text describing this option");
// ...

  // Now call the parent class' declareOptions
  inherited::declareOptions(ol);
}
\end{verbatim}

\subsection{Setting option fields and calling build()}

There are several techniques to implement the facilities of finishing
building afterwards and named parameters.  In PLearn, we typically
use public option fields (or sometimes protected fields with setter
methods) and a public {\tt build()} method that does the actual
building. Think of those public fields as really nothing but named
constructor parameters, and {\tt build()} as the one and only true
constructor.

The building of {\em me} in the previous example could then look
as follows:

\begin{verbatim}
#include "Person.h"

using namespace PLearn;

int main(int argc, char** argv)
{
    Person me; // default constructor can set default values 
               // for the option-fields
               // for ex: suppose default profession is "student"
    me.firstname = "Pascal";
    me.age = 29;
    me.build(); // finalize the building process

    cout<<me.firstname; 
}
\end{verbatim}

Note that there has to be a default (empty) constructor, whose role is also
to set the default values of the parameters.


\subsection{A generic way of setting options from ``outside''}

Sometimes, you want to set options and build an object from some
form of interpreted language environment or from a text description,
etc. That is to say from ``outside'' a C++ program. For this, \PLearn\ 
provides the setOption method. Suppose {\tt Person} is a subclass of
\Object, then we could do the following:

\begin{verbatim}
#include <plearn/base/Object.h>
#include "Person.h"

using namespace PLearn;

int main(int argc, char** argv)
{
    Object *o = new Person(); // o is a smart pointer to an
                              // object whose true type is Person
    o->setOption("firstname","Pascal");
    o->setOption("age","29");

    Person *p = dynamic_cast<Person*>(o);

    cout<<p->firstname;
}
\end{verbatim}

Note that {\tt setOption} takes 2 strings: the name of the option, and
its value serialised in string form. Strings are universal because
anything can be represented (serialized) as a string. Actually,
setOption calls a lower-level method called {\tt readOptionVal}
which reads the option value from a stream (a string stream in
this case\ldots) rather than a string. Similarly there is a {\tt
getOption} method which returns a string representation of a named
option, and whose implementation simply calls {\tt writeOptionVal}
on a string stream.


\subsection{Building an object from its specification in a file}

Building an object from a specification in a file is a natural
extension of the setOption/build mechanism.
Suppose we now have a file {\tt me.psave} containing the following text:

\begin{verbatim}
Person( firstname="Pascal"; 
        age = 29;
      );
\end{verbatim}

In the following code, we a way to build {\tt me} from its description
in the file.

\begin{verbatim}
#include <plearn/base/Object.h>
#include "Person.h"

using namespace PLearn;

int main(int argc, char** argv)
{

    Object* o = loadObject("me.psave"); 
    Person *p = dynamic_cast<Person*>(o);

    cout<<p->firstname;

    return 0;
}
\end{verbatim}


There are others ways to do that:

\begin{verbatim}
string filename = "me.psave";

// 1) The loadObject function
{
    Object *me;
    me = loadObject(filename);
}

// 2) What loadObject actually does
{
    ifstream in(filename.c_str());
    Object *me = readObject(in);
}

// 3) An alternative (loadObject actually calls Object::read)
{
    Person me; 
    ifstream in(filename.c_str());
    me.read(in);
}

// 4) An alternative using the global generic 
//    plearn::read function
{
    Object *me;
    ifstream in(filename.c_str());
    ::read(in, me);
}

// 5) What if we have the string representation at hand?
{
    // get the content of the file as a string
    // (function in fileutils.h)
    string description = loadFileAsString(filename);
    Object *me = newObject(description);
}
\end{verbatim}

Naturally, all that these functions do is parse the description in
the file, and call {\tt readOptionVal} (the lower-level equivalent
of {\tt setOption}) for each specified option, before finally calling
{\tt build()}.

Note that options may have arbitrarily complex types. They are not limited to strings and numbers; in particular they may
themselves be compex objects or arrays of things. For example:

\begin{verbatim}
Drawing(
    color = "blue";

    # path is an array of objects
    path = [ Line(x0=0, y0=0, x1=10, y1=20); 
             Line(x0=0, y0=0, x1=10, y1=20, width=2);
             Circle(x=20; y=30; radius=5.3, fill=true);
           ];
    );
\end{verbatim}

Finally, you should use a SmartPointable for Person, as seen before in \ref{PP}.

\begin{verbatim}
#include <plearn/base/Object.h>
#include "Person.h"

using namespace PLearn;

int main(int argc, char** argv)
{
    Object* o = loadObject("me.psave"); 
    PP<Person> p = dynamic_cast<Person*>(o);

    cout<<p->firstname;

    return 0;
}
\end{verbatim}


\subsection{Human description versus saved object}

The {\tt me.psave} file in the previous section may have been produced
either manually by a human being, or automatically by calling
\begin{verbatim}
plearn::save("me.psave",me);
\end{verbatim}
on a previously constructed {\tt Person me} object.

The mechanism for building an object is the same in both cases: it
automatically calls a series of {\tt readOptionVal} followed by {\tt
build()}. However the options specified in both cases are not always the
same:

\begin{itemize}

\item A hand-written description file will typically be used to give
a small number of options for the {\em initial building} of an object
(with the other options taking their default value).

\item A file resulting from a saved object, will typically include {\em
everything} that is necessary to reconstruct a new instance in the full
and exact same state as the instance that was saved. This may include
options, such as the learnt synaptic weights of a neural network, that
are not given at the time of {\em initial building}, but only when {\em
reloading} a serialised object.

\end{itemize}

We call the options typically used for initial building {\em \bf build
options}, and the second type {\em \bf learnt options}. Note that the
behaviour of the {\tt build} method may have to be quite different when
we are {\em reloading} a saved object (and providing it with {\em learnt
options}) from when we are only doing an {\em initial building} (and
providing it only with {\em build options}). It is natural that our
"one and only" constructor may have to behave differently depending on
the parameters it is given, but it is important to keep in mind the
distinction between {\em build options} on one hand, and {\em learnt
options} that are only present whe reloading, on the other hand.

There is a third conceptual category of options, that we call {\em \bf
tuning options}, which are used mostly to {\em tune} the object {\em
after} an intial building. They often overlap with {\em build options},
but not necessarily, the distinction is nevertheless more conceptual
than real.





%----------------------------------------------------------------------
\section{Matrix-Vectors Operations with Gradients}
%----------------------------------------------------------------------
\label{Var}

\subsection{Introduction to Var}

The class Var is at the heart of PLearn and aims at providing
matrix-variables in the mathematical sense. It is built on top of the
Mat class, that provides matrix-variables in the more traditional sense
of sequential computer languages.

Var should be used for Matrix-Vectors operations when you need gradients
on operations. Otherwise, use Mat and Vec classes.

NO NUMERICAL COMPUTATION IS DONE AT THIS LEVEL. The purpose of the
Var definitions is only to build the symbolic relationship between
mathematical variables.

 One can write arbitarily complex expressions using many implicit or
explicit intermediary variables, and predefined functions such as
in: $w=exp(-(abs(sqrt(lambda)*v)/3.0))$.  This will construct an
internal representation, only with a larger number of intermediate
nodes to represent intermediate states (variables) of the calculations.

 Each Var contains two Vec fields.

One is called value and holds the current value assigned to that
variable, and the other is called gradient and is used to backpropagate
gradients with respect to another variable.

Every Var has an fprop() method that updates its value field according
to the value field of its direct parents.

Every Var also has a bprop() method that updates the gradient field of
its direct parents according to its own gradient field (backpropagation
algorithm). Note that it \emph{accumulates} gradients into its parents
gradient field.

 Example:

\begin{verbatim}
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Var v(3,2); // declares a Variable of size 3x2
    Var lambda(1,1); // declares a scalar Variable

    v->matValue(1,0)=4.0;
    v->matValue(2,0)=5.0;
    v->matValue(0,1)=73.0;
    v->matValue(0,0)=78.0;
    v->matValue(2,1)=5.0;

    cout << v->matValue << endl;

    lambda->value = 2.0;

    Var w = lambda*v;

    w->fprop();

    cout << w->matValue << endl;
    return 0;
}
\end{verbatim}


If the expression to be calculated involves intermediate variables,
fprop must be called in a correct order on all those intermediate
variables before it can be called on the result variable we are
interested in. For example, suppose we have $z=dot(x,tanh(y))$ where
$x,u \in R^3$.


 A Var builds a directed acyclic graph whose nodes are Var's, with the
following structure:
\nopagebreak
\begin{verbatim}

 x    u                           x
  \   |                           |   
   \  |                           |  
    \ |                           |  
     \|                           |  
     [+]                          |
      |                           |           
      y ---> tanh() --> w  -----> dot----> z

\end{verbatim}

To obtain the correct value of z as a function of x and u, after setting
x->value and u->value, we need to perform fprop on all the intermediate
nodes as well as z.

\begin{verbatim}
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Var x(3,1);
    Var u(3,1);

    x->matValue(0,0)=1;
    x->matValue(1,0)=2;
    x->matValue(2,0)=3;
    u->matValue(0,0)=4;
    u->matValue(1,0)=5;
    u->matValue(2,0)=6;

    cout<<x->matValue<<endl;
    cout<<u->matValue<<endl;

    Var y = x + u;
        // y is also a 3x1 matrix
    Var w = tanh(y);
    Var z = dot(x,w);
        // z is a scalar variable result of the dot product 
        // of x and tanh(y)

    cout<<z->matValue<<endl;
    y->fprop();
    w->fprop();
    z->fprop();
    cout<<z->matValue<<endl;

    return 0;
}
\end{verbatim}

 To simplify the computation of values and gradients in a graph of
Var's, we use a VarArray (don't forget the include).

 A VarArray is simply an array of Vars, which has a method {\tt
fprop()} and a method {\tt bprop()} which calls the {\tt fprop()}
(resp. {\tt bprop()}) methods of all the elements of the array in the
right order (note that a right order for {\tt bprop} is the reverse of
the order for {\tt fprop}). The above function finds all the Var's on
the paths from the the inputs Vars to the output Var. There are may
be several input Vars so they are put in a VarArray. Once the path
is obtained, we can propagate values through it with the fprop method:

\begin{verbatim}
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Var x(3,1);
    Var u(3,1);

    x->matValue(0,0)=1;
    x->matValue(1,0)=2;
    x->matValue(2,0)=3;
    u->matValue(0,0)=4;
    u->matValue(1,0)=5;
    u->matValue(2,0)=6;

    cout<<x->matValue<<endl;
    cout<<u->matValue<<endl;

    Var y = x + u;
        // y is also a 3x1 matrix
    Var w = tanh(y);
    Var z = dot(x,w);
        // z is a scalar variable result of the dot product
        // of x and tanh(y)

    cout<<z->matValue<<endl;

    VarArray path = propagationPath(x & u, z);
    path.fprop();

    cout<<z->matValue<<endl;
    return 0;
}
\end{verbatim}

In the previous, the \texttt{VarArray} is useful but not essential. Let
consider the following example:

\begin{verbatim}
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Var x(3,1);
    Var u(3,1);

    x->matValue(0,0)=1;
    x->matValue(1,0)=2;
    x->matValue(2,0)=3;
    u->matValue(0,0)=4;
    u->matValue(1,0)=5;
    u->matValue(2,0)=6;

    cout<<x->matValue<<endl;
    cout<<u->matValue<<endl;

    Var z = dot(x,tanh(x+u));
        // z is a scalar variable result of the dot product
        // of x and tanh(y)

    cout<<z->matValue<<endl;

    VarArray path = propagationPath(x & u, z);
    path.fprop();

    cout<<z->matValue<<endl;
    return 0;
}
\end{verbatim}

This example performs exactly the same thing as the previous one. But
in this case, we don't have any reference to the previous \texttt{Var
y,w} to do \texttt{fprop()}. That's why a \texttt{VarArray} could be
essential.

You can also use \texttt{y->fprop\_from\_all\_sources()} instead of a
\texttt{VarArray} but this reconstruct the path each time and so don't
store it. It's not efficient for a multi fprop and it's not possible to
back-propagate gradients.

Once we have this path, we can also back-propagate gradients. For
example, if we set the gradient of z to 1,

\begin{verbatim}
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Var x(3,1);
    Var u(3,1);

    x->matValue(0,0)=1;
    x->matValue(1,0)=2;
    x->matValue(2,0)=3;
    u->matValue(0,0)=4;
    u->matValue(1,0)=5;
    u->matValue(2,0)=6;

    cout<<x->matValue<<endl;
    cout<<u->matValue<<endl;

    Var y = x + u;
        // y is also a 3x1 matrix
    Var w = tanh(y);
    Var z = dot(x,w);
        // z is a scalar variable result of the dot product
        // of x and tanh(y)

    cout<<z->matValue<<endl;

    VarArray path = propagationPath(x & u, z);
    path.fprop();

    cout<<z->matValue<<endl;

    z->gradient = 1.0;
    path.bprop();
    cout << "dz/dx = " << x->gradient << endl;
    cout << "dz/du = " << u->gradient << endl;

    return 0;
}
\end{verbatim}

 We obtain the partial derivatives of z with respect to x and u in their
 gradient field.


\subsection{Creating}

You can create Var with several methods.
The main are:
\begin{verbatim}
    Var(int the_length, int width_=1);
    Var(int the_length, int the_width, const char* name);
    Var(const Mat& mat);
\end{verbatim}

The last one id used as in the following example:
\begin{verbatim}
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>
using namespace PLearn;

int main(int argc, char** argv)
{
    Mat mx(3,1);
    mx(0,0) = 1;
    mx(1,0)=2;
    mx(2,0)=3;
    Var x(mx);
    cout<<x->matValue<<endl;
    return 0;
}
\end{verbatim}

\subsection{Manipulating}

In the introduction to Var, you saw how to manipulate them.

Don't forget that all is symbolic (it will tricks you).

You can find numerous var in \texttt{PLearn\/plearn\/var}, some are
shortcuted by overloaded operators (such as +).

\subsection{Loading and saving}

\subsubsection{Only the value}

With the following method, you can load and save THE VALUE of a var (not
the symbolic path).
\begin{verbatim}
#include <plearn/vmat/VMat.h>
#include <plearn/db/getDataSet.h>
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Var y(3,1);
    y->matValue(0,0)=1;
    y->matValue(1,0)=2;
    y->matValue(2,0)=3;
    cout<<y->matValue;

    // save into a pmat file
    y->matValue.save("save.pmat");

    // save into an amat file
    VMat vm(y->matValue);
    vm->saveAMAT("save.amat");


    // load from a file
    VMat vm2 = getDataSet("save.pmat");
        // it could have been "save.amat"
    Var x(vm2.toMat());
    cout<<x->matValue;
    return 0;
}
\end{verbatim}

\subsubsection{All the var}

Var is a subclass of Object, so you can use the methods of Object
as in the following example. Note that it will save all the Var,
including the sub ones.

\begin{verbatim}
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Var x(3,1);
    Var u(3,1);

    x->matValue(0,0)=1;
    x->matValue(1,0)=2;
    x->matValue(2,0)=3;
    u->matValue(0,0)=4;
    u->matValue(1,0)=5;
    u->matValue(2,0)=6;

    cout<<x->matValue<<endl;
    cout<<u->matValue<<endl;

    Var z = dot(x,tanh(x+u));
        // z is a scalar variable result of the dot product
        // of x and tanh(y)

    cout<<z->matValue<<endl;

    VarArray path = propagationPath(x & u, z);
    path.fprop();

    cout<<z->matValue<<endl;

    save("z.psave",z);

    Object* o = loadObject("z.psave");

    // There is no PP<> nor * here,
    // because Var is already a PP<Variable>
    Var p = dynamic_cast<Variable*>(o);

    cout<<p->matValue<<endl;

    return 0;
}
\end{verbatim}


\subsection{Func}

In order to make the usage of Var more friendly, you can use Func. The
Func class is mode for those who want to make fprop on different values
of Var in an elegant way. The two following examples illustrate this:
they do exactly the same thing, but the first one without Func and the
second one with.

\begin{verbatim}
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Vec a(3),c(1);
    Vec da(3),dc(1);

    // Without Func
    Var x(3,1);
    Var y = dot(x,tanh(x));
        // y is a scalar variable result of the dot product
        // of x and tanh(x)
    VarArray path = propagationPath(x,y);

    a<<"1 2.3 4";

    x->value<<a;
    path.fprop();
    c=y->value;
    cout<<a<<endl;
    cout<<c<<endl;

    a<<"4 8.3 -12";
    x->value<<a;
    path.fprop();
    c=y->value;
    cout<<a<<endl;
    cout<<c<<endl;

    dc<<2.3;
    y->gradient<<dc;
    path.bprop();
    da<<x->gradient;
    cout<<dc<<endl;
    cout<<da<<endl;

    return 0;
}
\end{verbatim}

With Func:

\begin{verbatim}
#include <plearn/var/Var_all.h>
#include <plearn/math/TMat_maths.h>

using namespace PLearn;

int main(int argc, char** argv)
{
    Vec a(3),c(1);
    Vec da(3),dc(1);

    // With Func
    Var x(3,1);
    Var result(1,1);
    Func f(x, result ,dot(x,tanh(x)));
        // z is a scalar variable result of the dot product
        // of x and tanh(y)

    a<<"1 2.3 4";
    f->fprop(a,c);
    cout<<a<<endl;
    cout<<c<<endl;

    a<<"4 8.3 -12";
    f->fprop(a,c);
    cout<<a<<endl;
    cout<<c<<endl;

    dc<<2.3;
    f->fbprop(a,c,da,dc);
    cout<<dc<<endl;
    cout<<da<<endl;

    return 0;
}
\end{verbatim}

%----------------------------------------------------------------------
\section{Online Learning}
%----------------------------------------------------------------------

\subsection{OnlineLearningModule}

They can be found in {\tt \$\{PLEARNDIR\}/plearn\_learners/online}.


\chapter{Advanced}

\section{RandomVar}

\section{Function-like types}
\subsection{Ker}
\subsection{CostFunc}
\subsection{StatsIt}
\section{Optimizers}
\label{Optimizer}

%% plus a jour au niveau des repertoires
% \section{Learning algorithms}
%
%  While all the previously mentionned classes are to be found in the
%  PLearnLibrary/PLearnCore directory, the higher level learning algortihms
%  are to be found in PLearnLibrary/PLearnAlgo
%
\section{ Miscalleanous utilities}

 The PLearnLibrary/PLearnUtils directory contains classes and
functions to perform various useful things such as graphically
displaying things. The Scripts directory contains a number of
perl-scripts and also binary programs to both help manage the
source-tree, and to manipulate matrix files.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Managing software growth}

A large library and collection of tools in constant development like PLearn is comparable to a
vine which keeps growing branches endlessly. As developers adapt it for
diverse uses, leading it simultaneously in different directions, this
growth will manifest as an overwhelming tendency to:
\begin{itemize}
\item add new classes and files (that's fine!) 
\item extend existing methods of base classes by adding extra arguments (possibly with
  default values)
\item add new member fields to existing base classes
\item add new methods to existing base classes
\item add new dependencies between existing files/classes (by adding new
  {\tt \#include} directives, due for example to the addition of a method taking a novel
  {\em type} of argument which needs to be "included").
\item add dependencies to new external libraries/tools 
\end{itemize}
This process is natural in the course of development, and is desirable if
we want to keep the code base adaptable. However if left unchecked it will lead to
its logical undesirable outcome: a huge collection of files, base classes with
hundreds of member fields and hundreds of methods (many for obscure special
purposes), very long compile and link times and huge executables (due to all
the added dependencies) and an installation nightmare on new environments
(due to the dependency on a large number of exotic "external libraries").

The aim of this chapter is to raise awareness of these issues, by shedding
light on them from several angles. This will hopefully help developers get a better
grip on them and take them properly into account when making design
decisions (in particular decisions that imply adding a {\tt \#include} in an
existing file).

\section{A few words on the build system}

Believe it or not, but the {\tt pymake} system is designed to optimally link
together {\em only} the object files that are {\em strictly necessary} for
a given executable program, and among them to re-compile only those that really
need re-compilation.

Now in working with PLearn, many experience the very long compilatin and
linking of a huge number of files. I insist that this is not due to the
build system, but only to what you, directly or indirectly, {\em include}
in your executable.

So while it is useful for some purposes to have a {\tt plearn} executable that
includes almost everything, it is clearly not {\em this} {\tt plearn.cc} that you
should be compiling linking and working with when you are {\em developing} new
algorithms to carry experiments on a more limited subject field.

{\bf Suggestion:} 
\begin{itemize}
\item Make a copy of {\tt plearn.cc} into {\tt mylearn.cc} and edit it to
include only the few classes you (or your script) will need.
\item Don't include something like {\tt plearn\_inc.h} which includes a large number of
files you probably don't need. Rather copy-paste the content of {\tt
plearn\_inc.h} into your {\tt mylearn.cc} and comment out all the things
you won't need.
\item Don't commit {\em your} {\tt mylearn.cc} under version control (so
  that it remains yours and seperate from the others' mylearn).
\end{itemize}

Using such a {\tt mylearn} should make the list of files linked together
and correpsonding link time a little shorter.

Now when using pymake's parallel compilation facility, the {\em
compilation} time for plearn projects is usually reasonable. But since {\em
linking} cannot be parallelized, link time can be problematic, especially
when the object files being linked are not on your local machine (due to
NFS sluggishness). So it is highly recommended, if you want link time to be
acceptable, that linking does not go through NFS (i.e. link on the machine
where the files are physically located).

TODO: write about {\tt pymake -dependency}

The only remaining way to improve the issue of compilation and link time
and executable size is to have a {\em sensible dependency graph} between
files, so that unnecessary files don't get indirectly "included". This can
only be achieved by developer awareness and proper restraint when the urge
rises to add a {\tt \#include} in an existing file. The following section
will try to give a few hints as to how this can be achieved.

{\bf Remark 1:} You may have had the feeling that if only plearn was made
into a proper library or set of libraries (static {\tt .a} or dynamic {\tt
.so}) the compilation and link time problems would somehow vanish. This is
simply untrue: such a system would necessarily be suboptimal compared to
the {\bf pymake} approach, as illustrated in the following example. Suppose
we have a number of object files A,B,C,D,E,F,G bound together in a library,
and suppose that B,C,D,E,F,G all depend on A but are otherwise independent of each other (ex: they are all direct
subclasses of A). Now suppose that your executable only needs G
directly. In addition suppose, we had to make a slight modifitation in
{\tt A.h} If you go the library route, regenerating your executable will imply
first regenerating the out-of-date library, which means recompiling
A,B,C,D,E,F,G and rebuilding the library archive. The linking phase will
then build your executable from A and G. But with the {\bf pymake}
approach, only the necessary files A and G will be recompiled (and bound
together in the executable), which is a more optimal use of ressources.
Having the ability to generate proper libraries may be desirable for other considerations though.

{\bf Remark 2, limitation of pymake:} for efficiency reasons, pymake doesn't do a
full C preprocessor pass, and thus doesn't currently understand the
preprocessor logic of {\tt \#define \#ifdef \#if ... \#else \#endif}. All it
understands is C and C++ comments. Thus if for ex. you {\tt \#include} something
within a {\tt \#ifdef SOMEDEF}, whether SOMEDEF happens to be defined or not in
the current context, pymake will still conclude there is a dependency link
with the {\tt \#included} file. So in short if you want pymake to ignore a
{\tt \#include} the only current way is to comment it out. 


\section{How to limit compilation and link dependencies}

\subsection{Compilation dependency versus link dependency}

For the rest of our discussion, it is important to distinguish between two
different kinds of dependencies:
\begin{itemize}
\item Saying that a file Y has a {\bf compilation dependency} on a file X
  means that whenever X changes, Y is to be considered out-of-date and will
  have to be recompiled if we need an up-to-date version of Y. Ex: Y {\tt
  \#includes} X or includes another file that includes another file that
  includes X, etc\ldots In short compilation dependencies affect which files
  will have to be "recompiled" (and thus affect re-compilation time).
\item Saying that a file Y has a {\bf link dependency} on a
  file X means that whenever we need to link with the corresponding object
  file "X.o" we will also have to link with "Y.o". In short link
  dependencies affect which files will have to be linked together to
  produce an executable (and thus directly affects link time).
\end{itemize}

These two concepts are related, yet subtly different, as will become
apparent shortly.

\subsection{How dependencies tend to creep in, and ways around them}

Let us begin with a few remarks. Old-fashioed C libraries tended to put
{\em one function} per .c file (i.e. per compilation unit). This resulted
in libraries with very fine granularity, and only the {\em necessary
functions} were included and linked in a given executable. But with classes
in object oriented C++, the granularity cannot go below the class level
(one class per compilation unit). And as a minimum, a class carries with it 
the compilation and link dependencies implied by {\em all its
 methods} and {\em all their argument types}.

Now when people learn C++ object-oriented programming, especially when
their background programming experience is a language like Java, there is a
tendency to want to make everything a method within a class, because it
appears elegant, and the OO way. But it is important to pause and reflect
on the consequences and alternatives, especially when adding a method
implies adding a {\tt \#include} which adds compile and link dependencies.

An example will better illustrate the alternative possible choices:

Suppose we have two independent classes A (files A.h, A.cc) and B (B.h, B.cc),
and we want to add some operation {\tt f} that requires instances of A and B
as arguments (or return value). There are essentially 3 ways
to implement such an operation:
\begin{itemize}
\item As a new method of A, taking a B as argument {\tt A::f(B)}
\item As a new method of B, taking an A as argument {\tt B::f(A)}
\item As a regular function of A and B {\tt f(A,B)}
\end{itemize}

Now the consequences in terms of introduced file dependencies are very
different.
\begin{enumerate}
\item Adding {\tt A::f(B)} implies making A depend on B (i.e. you'll no
  longer be able to link with A without also linking with B, and any change
  to B.h will at least trigger a recompile of A.cc).
%, and you'll need
%  to write a {\tt \#include "B.h"} at least in {\tt A.cc} {Introducing a new method of A taking a B as argument implies adding
%  at least a link dependency from A on B, and a compilation dependency from
%  A.cc on B.h (and often also from A.h on B.h)
\item Similarly, adding {\tt B::f(A)} implies making B depend on A.
\item On the other hand {\tt f(A,B)} can be put in a separate file
  (compilation unit) possibly together with other functions of A and
  B. This does not create any direct compilation or link dependency between
  classes A and B, and the file containing {\tt f} needs only be included (and consequently
  compiled and linked with) if the specific funcitonality {\tt f} is
  needed.
\end{enumerate}

So whenever it appears at first obvious that we need to add a method
{\tt A::f(B)} to a
given class {\tt A}, and that adding such a method forces us to add an {\tt
\#include "B.h"} directive, it should trigger a red light in the mind of the
developer. The red light is an invitation to consider the other two alternatives. The
following considerations should then weigh in the design decision.
\begin{itemize}
\item Obviously, if {\tt A::f(B)} is to be a virtual method designed to be
  redefined in sub-classes of A, then there is no discussing it, it should be
  {\tt A::f(B)}. But if there is no reason to expect that {\tt f} needs to
  be virtual in order to be redefined in sub-classes, then the
  alternatives can and should be considered.
\item If {\tt f} is a rarely needed functionality with a large
  implementation code, it is probably best left as a separate function in
  its own file (posibly with other similar functions).
\item It may be nicer to group {\tt f(A,B)} with other functions relating
  to a same topic in a separate file, rather than unreasonably increase the
  number of methods of A.
\item But if A is meant to almost always work together with B (i.e. if it
  makes no sense having an A without also manipulating some kind of B), or
  if A already depends on B due to some other reason (such as having a
  member variable of type B) then
  {\tt A::f(B)} is probably a reasonable design choice. 
\item When hesitating between {\tt A::f(B)} and {\tt B::f(A)} the choice
  should be to make the least basic, higher level, least used class depend
  on the most basic, lower level, more widely used class, rather than the
  other way round.
\end{itemize}

TODO: talk about use of forward declaration to reduce compilation dependency,
but how it doesn't reduce link dependency.

\section{Regarding external library dependencies}

External dependencies tend to creep in in the codebase, and if unchecked, end up causing
a nighightmare for installation, link times, and memory usage of the
running software. So it is a good policy to try and limit this, based on
the following three guideleines
\begin{itemize}
\item {\bf If something similar already exists within PLearn, prefer using that.}
\item {\bf If it can easily be done without introducing a new dependency, then
  do it that way (even if it feels a little less {\em cool}.}
\item {\bf Prefer using an external library that is already used in PLearn
(and approved) than a introducing new one.} 
\end{itemize}

A few remarks regarding specific external libraries
\begin{itemize}
\item Use PLearn's intrusive smart pointers ({\tt PP}, \ldots) for all
  PLearn objects. Use boost's \verb!shared_ptr! for non PLearn classes. 
\item Use PLearn's PStream and serialization mechanism rather than any
  other C++ streams or serialization system  (including std::stream).
\end{itemize}

\section{Evolving software in a backward-compatible way}

TODO: talk about 
\begin{itemize}
\item Tolerant, evolution-friendly, serialization format
\item class versioning
\item new version of class (filename scheme)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{ PLearn coding guidelines and philosophy}

 Several people wrote significant parts of PLearn, and if you take a
closer look at the code, you will see a number of clearly different
coding styles and philosophies. However, as of this writing (09/2000),
the overall design and organization of most of the library is still to
be blamed (or praised\ldots) on me. So these remarks are my personal
view of things, and do not necessarily reflect the opinion of everybody
on the PLearn developer team, but I hope it will help you understand
the reasons why things are the way they are, and hopefully have you
choose to keep them that way\ldots

 Pascal 

\section{A few words on C++}
 Agreed, C++ \emph{can be} a very complex language. The main reason
being that it is extremely feature-rich, but that is also what
makes C++ so powerful and expressive, and thus appropriate for a
machine-learning library. Yet I insist on the \emph{can be} : it
doesn't always have to be, it depends a great deal on what features
you choose to use and when.

People who discover C++ tend to first be overwhelmed with its wealth
of features, and then seem to want to use them all at once in even the
simplest piece of code (complex templates, deep multiple inheritance
trees, exceptions, multiple nested namespaces; add multi-threading on
top of that and you're sure to write the most unreadable, unportable
and compiler-bug trigerring error-prone code ever). Finally, after
great intellectual efforts, they discover that even their compiler
(not to mention their debugger!) has trouble understanding it all
and, if they manage to have it swallow the code, they realise that
no other compiler will (portability anybody?). This is still quite
true as of this writing (09/2000) and was even more so a few years
ago, yet tools will keep improving until some day, hopefully, they
all behave perfectly by the book, according to the standard, but
until this blessed day comes, beware\ldots\ Many people then give up,
frustrated, and decide to go back to C, which is a shame. C++ \emph{is}
a much better language than C, especially for writing Object Oriented
code, and it \emph{does} make the programmer's life much easier\ldots\
as long as \emph{you} keep things simple.


 So please, especially if you're a beginner, keep this in mind
when writing C++ code: having so many ``cool'' features in the language
doesn't mean that you must use them all at once. Choose wisely and,
if in doubt, always prefer the simplest solution\ldots

\section{Design goals and priorities}

 Any project implicitly or explicitly sets some goals and these
directly influence the way code is written. With PLearn, one of the
founding goal, was to be able to describe complex machine-learning
experiments by assembling simple building-blocks directly in C++,
without resorting to a layer of home-grown dedicated language (as
experience had proven us that it is hard to grow and maintain such
a language, which appears always too limited anyway). Obviously
we also want to have them run efficiently (hence the choice of C++
rather than a higher level interpreted language).

 Any system should ideally be simple to understand and use, lightning
fast, and extremely geneal. Yet there is always a tradeoff to be made
between these 3 highly desirable characteristics. Here is the priority
I gave them, in the design of the library, it logically follows from
the project's primary goals:

\begin{enumerate}
\item  readability, simplicity, ease of use (and portability)
\item  computational efficiency
\item  genericity
\end{enumerate}

\section{Usage of C++ features in PLearn}

 As I mentionned earlier, moderation is good in everything, including in
 moderation\ldots\ ;) 

\subsection*{Function and method prototypes without parameter names}
 C++ code is typically divided between .h files which contain class
layout, function and method prototypes, and .cc files which contain the
actual implementation. Ideally, it should be possible to understand
what a method or function does by looking it up only in the .h
file. Comments are part of achieving this, but having a meaningful
name for the parameters of the function also helps a great deal.

 C++ allows you to omit parameter names in prototypes (and only
give their types). This defies the purpose of clarity, and is thus
considered bad practice by the author and in PLearn in general. Except
for possible default values (that are to appear only in the .h),
the prototype in the .h file should be identical to the definition
in the .cc file and include parameter names.

 (Ex: people usually have trouble understanding what float* f(float*,
int, int, char, char*, float); is supposed to do, and defining a new
type for each argument is \emph{not} the right way of making this
more understandable\ldots\ giving them a meaningful name is.)

\subsection*{Basic data types}
 Conceptually, people usually think of 3 simple basic data types:
integers, reals, and booleans (possibly 4 if you add character). C++
has them in many flavours, including signed and unsigned, several
precisions, etc. These all have their use from a low-level hardware
perspective (which woud have been much better if they had been given
standard byte sizes by the standard\ldots), but to the mathematically
minded library-user they are an annoyance. So throughout most of
PLearn, unless otherwise dictated by low-level precision or space
considerations, we use only 3 types that correspond to the 3 concepts:

\begin{itemize}
\item \textbf{int} is used for integers
\item \textbf{bool} is used for booleans
\item \textbf{real} is used for reals

{\tt real} is defined throughout the whole library to be either {\tt
float} or {\tt double}, depending on a compilation flag (USEDOUBLE
or USEFLOAT).
\end{itemize}

 Also we encourage people \emph{not to} define a new type if it
conceptually corresponds to one of those three concepts, in particular
I for one (and I'm surely not alone) dislike to have to write\\
\texttt{namespace::subnamespace::classname::interiorclass::length\_type}\\
when the damn thing is just an integer, if you get my point. Please
use int, it saves the user keystrokes, code lookup time, and
eases understanding (i.e.: genericity-\,- but simplicity++ and
ease\_of\_use++, see section on desing priorities above).

 The use of unsigned int types is also a source of annoyance to me,
and of potential nasty bugs. Ex:
\verb!for (unsigned int i=10; i>=0; --i)!

So again, unless you really need the extra bit of precision,
use int (also saves a few keystrokes).

 A kind of string type is also usually seen as part of the set of basic
types, but we'll discuss this in the section on the standard library.

\subsection*{Namespaces}
 Namespaces are most useful to prevent name clashes between different
libraries. So ultimately, all of PLearn is to reside in the PLearn
namespace. However gdb currently seems to have trouble coping with
them, so the namespace directives are currently surrounded by ugly
\#ifdef USENAMESPACE which we usually keep undefined.

 Also, for now, I do not encourage the use of sub-namespaces to
organize the code within PLearn (with or without \#ifdefs). It's
already hard enough to get the organization right in terms of
concepts, class hierarchies, and files, without introducing yet
another hierarchy of things (which besides, would go mostly untested
as we always compile with USENAMESPACE undefined, for now anyway).

\subsection*{Exceptions and runtime errors}
 Exceptions can be a nice and useful feature, allowing you to build
sophisticated error recovery mechanisms and the like\ldots\ But
designing a consistent error-recovery scheme with an appropriate
exception class hierarchy is a complex task. Besides in PLearn, we
typically have no use for a sophisticated error recovery mechanism:
a runtime error is always a sign of a bug somewhere, and the policy
in PLearn is to never try to second-guess the programmer: all we want
is for the program to abort immediately with a somewhat meaningful
message, and the debugger to be able to trace the call. Unfortunately,
as of this writing, exceptions are poorly supported by debuggers
(and they can create a nightmare in multi-threaded code).

 So essentially we don't use exceptions in PLearn, but a very simple
runtime error mechanism: \texttt{error("my meaningful error message");}
will result in a call to function {\tt errormsg} that simply prints out
the message and exits the program. Thus it is easy to set a breakpoint
in {\tt errormsg} in the debugger and trace what happened. This is
a no-fuss solution that does the job. Notice that the {\tt errormsg}
function can easily be modified to throw an exception if you wish to
do a proper error recovery (in case brutally exiting the program is
not an acceptable behaviour).

 Exceptions can also be useful for other things, but for typical
runtime-errors, please use {\tt error(\emph{errormsg})}.

\subsection*{Templates}
 Templates is one of the most powerful features of C++. But it's also
the most complex, and the one with which compilers and debuggers
have the most trouble (almost all but the simplest template code is
hardly portable across compilers because of inconsistencies between
them, and it was much worse a few years back!). The early versions
of PLearn deliberately did not use \emph{any} template code at all
(many other librariy designers out there for whom portability was a
major concern made the same choice).

 As the compilers improved, I started allowing myself to use
\emph{simple} templates for things where they were \emph{really}
appropriate, (i.e. smart pointers and generic containers). And I
would recommend everybody to stick to this. Please, \emph{refrain}
from using templates as much as possible: it will make your code
easier to write, to read, to debug, to port, to understand, and
also faster to compile. It's usually easy to later ``templatize'' a
working and well-tested non-templated code if really needed. But it's
always annoying to have to ``de-templatize'' a complex template code
because the compiler on your new target platform cannot understand it
(chances are that you won't either).

\subsection*{Multiple inheritance and complex class hierarchies}
 Multiple inheritance poses a number of technical problems and a multiple
inheritance tree is also usually more difficult to understand
conceptually. Therefore, PLearn uses only single inheritance and I would
like to keep it that way. The only kind of ``multiple inheritance'' that we
have is for inheriting \emph{interfaces } (\`a la Java) i.e. abstract
classes with only purely virtual methods.

Also we often use concrete classes, and in general prefer flat class
hierarchies than very deep ones, as they are easier to comprehend.


\subsection*{const}
 const is number one on my list of C++ annoyances. But unfortunately
there is no way to really do without it, so try to use it consistently,
and try not to get too frustrated in case of code constipation,
pardon me, const problems\ldots\ there is always a (hopefully clean)
way around them.


\subsection*{public, private, protected}
 There are probably too many class members that are public in
PLearn. But, as we love our potential library users (they are mostly
us for now anyway), we tend to avoid paranoia, and to trust them
for not doing dirty things with our not-so-private members. Hell,
they have access to the source code anyway!

\section{Usage of the C++ standard library in PLearn} 

 In early versions of PLearn, we did not use much of the standard
library (as no compilers yet agreed on a standard), except for
iostreams. Now that there is a well established standard, and that
all compiler makers are working towards conforming to it, we are
slowly moving PLearn to using more of the standard library facilities.


\subsection*{Strings}
 Many places in PLearn still use char* to represent strings,
but they'll slowly be changed into using the std::string class
instead. Please use string from now on. Feel free to change any usage
of char* you meet into string.

 A number of useful additional functions for user-friendly string
manipulation can be found in file PLearnCore/stringutils.h A brief
(and certainly not up to date) description of it, as well as a pointer
to a quick overview of the basic string operations can be found here.

\subsection*{Streams}
 Several pieces of old PLearn code still use the C stream library
(FILE* \ldots), but the standard C++ stream facilities is the
officially approved way to go for new code.


\subsection*{Standard containers and algorithms}
 It's now OK to use STL containers wherever appropriate. Two other
generic containers were previously developed for PLearn: Array is
heavily used, and is a base class for a number of other specialised
array types, so it is not likely to vanish any time soon (although
I may have it derive from {\tt std::vector} one of these days). The
main advantage of Array over {\tt std::vector} is that runtime
bound-checking can be turned on or off with a compilation flag
(BOUNDCHECK), and there's also a user-friendly (but inefficient)
syntax to build arrays from simple elements using the \& operator. Hash
may also be progressively abandoned in favour of std::map, hash\_map
(is this one part of the C++ standard?) and the like\ldots


\section{Naming conventions}

 The following naming conventions are used throughout PLearn. They
are mostly inspired by the Java naming conventions. Anybody who
uses or wishes to extend PLearn should be aware of them (as it makes
understanding of the code easier) and try to respect them (as it will
make the understanding of their code easier to other people who will
have the privilege to dig into it).

\subsection*{To make it short and simple:}

\begin{itemize}
\item {\tt MyClassName}
\item {\tt myMethodName()}
\item {\tt my\_variable\_name} \emph{OR }
      {\tt myvariablename} \emph{(both for member variables or otherwise)}

\item {\tt my\_global\_function()} \emph{OR}
      {\tt myGlobalFunction()}
\item {\tt MY\_CONSTANT}  \emph{(for \#define constants or other)}

\end{itemize}

\subsection*{Remarks:}

\begin{itemize}

\item A classname should always start with an uppercase.

\item Methods, functions, and variable names should always start with a
lowercase.

\item Underscores(\_) should never be used in class names or method
names.

\item Typical methods that return a bool status should begin with is or
has. Ex: {\tt isEmpty()} {\tt isNull()} {\tt hasChildren()}.

\item In case you want to provide a read-only accessor method to a
protected or private member variable, use {\tt varname\_} for the member
variable and {\tt varname()} for the accessor method.

\item The arguments of a constructor often carry initial values for
member variables. We usually name the argument in the constructor
`the\_varname' so that it doesn't clash with the targetted member
variable (which is just `varname')

\end{itemize}

 A few reasonable exceptions are tolerated throughout the code (such as
function P for probability instead of a lowercase p, or a member
variable K for a kernel matrix\ldots) But exceptions that don't serve
any purpose should not be!

\section{Final word}

 The PLearn library is far from perfect, it still has a lot of
rough edges (my to-do list is growing every day), and there are
several things that I would do differently if I was to start all
over again. But it is nevertheless already a very usable tool, that
for the most part, I feel, meets its primary design goals. Besides I
consider good code design an iterative process: one starts with an
initially rough version and iteratively refines it under the light
of real-world experience. The code base is not carved in stone, it
is an evolving being, and the source code is there so that you can
tweak it and adapt it to your needs, and hopefully help make it better.

\chapter{Debugging}

 There are several types of problems you'll encounter, and each has a
proven solving technique.

\section{Compilation problems}

{\bf Solution: } learn how to program in C++

 No, I mean seriously, learn C++, {\em thoroughly}, until you are
able to truly and fully understand every single bit of the cryptic
message issued by the compiler, and why on earth it may have chosen
to insult your intelligence with it. Because that cryptic message
always contains the solution.

 In particular, you need to really understand the difference between
a const thingy and a non-const thingy, because to C++ they are often
two totally different beasts (although to a decent human, they may
look the same at first inspection).

 So make sure you truly understand {\em all} the subtle differences
between for ex.: \\
\verb!const char* MyObj::mymethod(const char* &foo, const int& bar)!  \\
and
\verb!char* MyObj::mymethod(const char *const foo, int& bar) const! \\


 On rare occasions, you might occasionaly stumble upon a compiler
{\em bug} (as in {\em compiler internal error!!!} ), in which case
you may try the following: upgrade your dusty compiler to the newest
less-buggy version; check on google to see if anybody else had the
same problem and if they found a workaround; try to find a workaround
yourself (split your call in several pieces, using intermediate
variables, add a cout here, reorder the instructions there\ldots\ 
and pray!); try posting an SOS on the appropriate newsgroups; write
a bug report! Somebody somewhere, is responsible for it and might be
interested in fixing the mess, or already has\ldots


\subsection{Frequently encountered compilation errors}

To save you some time, you may look up in the following list if
somebody stumbled upon the same problem.

\begin{itemize}

\item {\bf typical error msg } \\
explanation and fix.

\end{itemize}



\section{Linking problems}

Problems reported by the linker can have several causes:

\begin{itemize}
\item It doesn't find a function that's supposed to be defined somewhere in
  your code but isn't.  A frequent case is that of instantiating an object
  of a class derived from a base class with a pure virtual mamber function
  that you forgot to define in the subclass. Another case is declaring a
  function in the .h and forgetting to implement it in the .cc, or (more
  often) implementing it with a slightly different signature (forgot
  Classname:: ?, forgot to put a const somewhere?)
\item It doesn't find symbols because it doesn't find the required
  libraries. Examine the linking command, are all the necessary libraries there? Are they
  indeed located where you say they are? There should be no space between the
  -L and the library path.
\item Libraries are specified in an inappropriate order. A library that
  depends on another library should appear before it in the linking
  command; the most basic libraries should be last.
\end{itemize}



\section{Clean runtime errors}

What I mean by clean runtime errors, is that the program displayed a nice
error message and exited.

This most likely means that something in your program caused a call to the
PLERROR macro, which called the {\tt errormsg} function, which threw a
PLearnException, which got caught in the very external try/catch of your
main program, which printed it out (you main program {\em does} catch
PLearn exceptions and report them, doesn't it???).

Tracing the problem is easy:
\begin{itemize}

\item Launch your favorite debugger ({\tt gdb}) from within your
favorite development environment ({\tt emacs}).

\item Put a breakpoint in the {\tt errormsg} function by typing: \\
\verb!br 'PLearn::errormsg(!{\it TAB} \\
Pressing the {\it TAB} key will complete the signature of the function
for you. Note that the single quote at the beginning is important for
this to work.

\item {\tt run} your program until it reaches the breakpoint.

\item trace {\tt up} the call stack and figure out what and why it
happens.

\end{itemize}

{\bf Hints for using gdb:} \\
{\tt gdb} is always at quite a lag behind, playing catch-up with the
latest compiler. Sot it {\em has} problems.  Here are a few hints for
working with it, or in spite of it\ldots
\begin{itemize}

\item printing a {\tt std::string} doesn't work. Cast it to a {\tt
  char*} first, as in \verb!p (char *) my_string!, or call
  \verb!p my_string.c_str()!

\item {\tt gdb} often seems lost when you attempt to examine the insides
  of a complex object, replying that it can't find info on that class.  In
  this case, unfortunately, you'll have to insert instructions in the code
  to print the desired debug info (with \verb!cerr << ...!), recompile and
  rerun gdb.

\item If you want to see an object on which you have a {\tt PP} smart
  pointer (or similar type), you can access the raw pointer inside (it's
  called {\tt ptr}!), for ex: \verb!p my_var.ptr->value.length_!
\end{itemize}

I also suggest you learn using a good integrated development
environment (IDE) like Emacs. Emacs has multiple windows, a compilation
mode (pressing return on an error will bring you directly to the
problematic line of the problematic file), and a gdb mode (with
which you can easily follow the step by step execution, as an arrow
is always displayed before the next instruction to be executed, and
you can rapidly put a breakpoint (Ctrl-X SPACE) anywhere).

With the proper key definitions (learn how to define your own keys for
maximum efficiency!), a complete recompile of your code, and reload in
gdb is just one key away.  You won't have to touch the rat. And yes,
it even all works perfectly fine (multiple windows and all) inside
a {\em single} text terminal over a telnet connexion for ex. (invoke
it using \verb!emacs -nw!).


\section{Dirty runtime errors}

By this, I mean {\tt segmentation fault} and the like.

\begin{itemize}
\item First try running your code in {\tt gdb}, as above.
\item If this doesn't appear too helpful on its own (and probably won't),
  try running your program with {\tt valgrind}. It's a great tool for
  catching memory bugs (like writing or reading from memory areas you never
  allocated or initialized.). You can run it like that, from a shell: \\
  \verb!valgrind --gdb-attach=yes your_prg_and_its_args!
\end{itemize}



\chapter*{License}

This document is covered by the license appearing after the title page.

The PLearn software library and tools described in this document are
distributed under the following BSD-type license:

\begin{verbatim}
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
notice, this list of conditions and the following disclaimer in the
documentation and/or other materials provided with the distribution.

3. The name of the authors may not be used to endorse or promote
products derived from this software without specific prior written
permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
\end{verbatim}



\end{document}

