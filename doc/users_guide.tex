%% -*- mode:latex; tex-open-quote:"\\og{}"; tex-close-quote:"\\fg{}" -*-
%%
%%  Copyright (c) 2002 by Pascal Vincent
%%
%%  $Id$

\documentclass[11pt]{book}
% \usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage{html}               % package for latex2html
% \usepackage{t1enc}              % new font encoding  (hyphenate words w/accents)
% \usepackage{ae}                 % use virtual fonts for getting good PDF
\usepackage{hyperref}
\usepackage{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Support for PDF and required packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}	% enhanced pictures import (w/ PDF)


%%% %%% Basic switch to detect whether to use PDF or not
%%% \newif\ifpdf
%%% \ifx\pdfoutput\undefined
%%%   \pdffalse
%%%   \newcommand{\mypdfoption}{{}}
%%% \else
%%%   \pdfoutput=1
%%%   \pdftrue
%%%   \newcommand{\mypdfoption}{pdftex}
%%%   \pdfcompresslevel=9
%%% \fi
%%% 
%%% %%% Packages that behave differently in PDF and non-PDF
%%% \usepackage[\mypdfoption]{graphicx}	% enhanced pictures import (w/ PDF)
%%% \usepackage[\mypdfoption]{rotating}	% rotate stuff on the page
%%% \usepackage[\mypdfoption]{color}        % color support
%%% \usepackage[\mypdfoption]{colortbl}     % Add color and shading to tables
%%% 
%%% %%% Declare graphics file extensions as .pdf or .eps for \includegraphics.
%%% %%% (You must NOT specify any extension in the filenames!  \includegraphics
%%% %%% will by itself determine the correct extension.)
%%% \ifpdf
%%%   \DeclareGraphicsExtensions{.jpg,.png,.pdf,.mps} % .png before .pdf ==> okay!
%%% \else
%%%   \DeclareGraphicsExtensions{.jpg,.eps,.png,.mps} % .eps before .png ==> okay!
%%% \fi


%%%%%%%%% Definitions %%%%%%%%%%%%
\newcommand{\R}{\sf{I\!R}} 

\newcommand{\PLearn}{{\bf \it PLearn}~}
\newcommand{\Object}{{\bf Object}~} 
\newcommand{\Learner}{{\bf Learner}~} 
\newcommand{\PPointable}{{\bf PPointable}~} 
\newcommand{\VMatrix}{{\bf VMatrix}~} 
\newcommand{\VMat}{{\bf VMat}~} 
\newcommand{\PLearner}{{\bf PLearner}~} 

\parskip=2mm
\parindent=0mm

\title{{\Huge PLearn User's Guide\\ \Large How to use the PLearn Machine-Learning library and tools}}

\begin{document}

%%%%%%%% Title Page %%%%%%%%%%
\pagenumbering{roman}

\maketitle

\vspace*{8cm}

Copyright \copyright\ 1998-2006 Pascal Vincent, Yoshua Bengio \\
Copyright \copyright\ 2002 Julien Keable \\
Copyright \copyright\ 2003 R\'ejean Ducharme \\
Copyright \copyright\ 2004 Martin Monperrus \\
Copyright \copyright\ 2005 Olivier Delalleau \\

Permission is granted to copy and distribute this document in any medium,
with or without modification, provided that the following conditions are
met:

\begin{enumerate}
\item Modified versions must give fair credit to all authors.
\item Modified versions may not be written with the aim to discredit, misrepresent, or otherwise taint the
      reputation of any of the above authors.
\item Modified versions must retain the above copyright notice, and append to
   it the names of the authors of the modifications, together with the years the
   modifications were written.
\item Modified versions must retain this list of conditions unaltered, 
    and may not impose any further restrictions.
\end{enumerate}

%%%%%%%%% Table of contents %%%%%%%%%%%%
\addcontentsline{toc}{chapter}{\numberline{}Table of contents}
\tableofcontents

% \begin{latexonly}
% \cleardoublepage\pagebreak
% \end{latexonly}
% \pagenumbering{arabic}

\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}

PLearn is an Open Source C++ library and framework with an associated
collection of software tools developped and used for \emph{research} in
\emph{statistical machine learning.} 

The emphasis here is on \emph{``research''}: it was built by researchers
mostly for their own use, i.e. not too much with the general public in
mind. It is not for the faint of heart, and you are more likely to
find here exotic algorithms at the forefront of research, rather than a
comprehensive collection of all the ``standard proven and well tested''
algorithms of Machine Learning. This being said, if you want to program
your new idea of a learning algortihm in efficient modern C++, the PLearn
framework offers a solid foundation.


\chapter{Tutorial}

This chapter is a tutorial that will walk you through the basic concepts from a user-level perspective.

We assume you have a copy of the plearn distribution, and a working plearn
executable accessible through yout PATH. All the files in this tutorial are in \verb!examples/Tutorial/!
so you should first {\tt cd} to this directory.

\section{The plearn Commands and Help}

Usual PLearn executables such as \verb!plearn! or \verb!plearn_light! are
typically called in command-line fashion.

\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> plearn
plearn 0.92.0  (Jun 21 2005 12:04:50)
Type 'plearn help' for help


valhalla:~/PLearn/examples/Tutorial> plearn help
plearn 0.92.0  (Jun 21 2005 12:04:50)
To run a .plearn script type:                       plearn scriptfile.plearn
To run a command type:                              plearn command [ command arguments ]

To get help on the script file format:              plearn help scripts
To get a short description of available commands:   plearn help commands
To get detailed help on a specific command:         plearn help <command_name>
To get help on a specific PLearn object:            plearn help <object_type_name>
To get help on datasets:                            plearn help datasets
\end{verbatim}

The {\tt plearn} executable can be invoked either with a {\em PLearn script} (more on that later) or with a {\em PLearn command}.\\
To get the list of available commands:

\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> plearn help commands
plearn 0.92.0  (Jun 21 2005 12:04:50)
To run a command, type:  % plearn command_name command_arguments

Available commands are:
FieldConvert    :  Reads a dataset and generates a .vmat file based on the data, but optimized for training.

autorun :  watches files for changes and reruns the .plearn script
help    :  plearn command-line help
htmlhelp        :  Output HTML-formatted help for PLearn
jdate   :  Convert a Julian Date into a JJ/MM/YYYY date
ks-stat :  Computes the Kolmogorov-Smirnov statistic between 2 matrix columns
learner :  Allows to train, use and test a learner
read_and_write  :  Used to check (debug) the serialization system
run     :  runs a .plearn script
server  :  Launches plearn in computation server mode
test-dependencies       :  Compute dependency statistics between input and target variables.
test-dependency :  Compute dependency statistics between two selected columns of a vmat.
vmat    :  Examination and manipulation of vmat datasets


For more details on a specific command, type:
  % plearn help <command_name>

\end{verbatim}

PLearn commands accept a number of arguments that are command specific.
Very often the first argument is itself a sub-command\ldots

{\tt help} is actually a {\em PLearn command}! Thus we can ask help on help!

\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> plearn help help
plearn 0.92.0  (Jun 21 2005 12:04:50)
*** Help for command 'help' ***
plearn command-line help
help <topic>
Run the help command with no argument to get an overview of the system.
\end{verbatim}

The {\tt help} command can give detailed help on any available PLearn {\em
  command}, as well as on any PLearn {\em object class}.

{\bf There is an on-line html version of the help provided by the help command...}
See {\em PLearn help on user-level commands and objects} on the PLearn homepage\ldots

\section{Data Matrices}

Machine-learning algorithms learn from data and are then used for prediction on new data.
In this tutorial, we'll concentrate on the simplest and most usual form of
data samples: vectors in $\R^d$. 

A dataset of $l$ samples is then simply an $l \times d$ matrix of reals.
In PLearn such datasets are implemented through the concept of a \VMatrix
(or \VMat in short).

A \VMat is essentially:
\begin{itemize}
\item A $l \times d$ matrix of reals ($l$ is its {\em length}, $d$ its {\em width}),
\item optionally with an associated {\em fieldname} for each column (or {\em field}),
\item optionally with associated {\em inputsize, targetsize, weightsize, extrasize}
\item optionally with strings associated to specific values of a given column
\end{itemize}

The {\em inputsize, targetsize, weightsize, extrasize} are important information for
learning algorithms, as they specify which part of each row is to be
considered the known input (the first {\em inputsize} elements), which part
is the target to predict (the next {\em targetsize} elements), and whether
or not they are followed by a sample weight ({\em weightsize = 0 or
  1}). The {\em extrasize} fields can be used to store any extra information.

For the traditional {\em tasks} of statistical machine learning, we have
the following conventions regarding datasets and ``sizes'':
\begin{itemize}
\item {\bf regression:} \\
{\em inputsize} = number of known inputs (``variables'', ``factors'' or ``features'', i.e. dimensioality of ``$x$'')\\
{\em targetsize} = number of values to predict (i.e. dimensionality of ``$y$'')
\item {\bf classification:} \\
{\em inputsize} = number of known inputs \\
{\em targetsize} = 1: the target is the class number (between 0 and nclasses-1)
\item {\bf density estimation:} \\
{\em inputsize} = dimensionality of $x$ \\
{\em targetsize} = 0
\end{itemize}


For ex., let's create a simple data set for 1D regression, i.e. to predict a real $y$ from a real $x$.
Open a file \verb!1d_reg.amat! with your favorite editor, and and enter the following text definint a 
$5 \times 2$ matrix:
\begin{verbatim}
#size: 5 2
#: x y     
#sizes: 1 1 0 0

0    3
0.5  4
1    5
2    6
3    7.5
\end{verbatim}

This represents a $5 \times 2$ matrix whose columns are named {\em x} and
{\em y}, and whose {\em inputsize=1, targetsize=1, weightsize=0, extrasize=0}.

\section{Viewing Data Matrices}

Data matrices can be manipulated with the PLearn command {\em vmat}:

\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> plearn help vmat
plearn 0.92.0  (Jun 21 2005 12:04:50)
*** Help for command 'vmat' ***
Examination and manipulation of vmat datasets
Usage: vmat info <dataset>
       Will info about dataset (size, etc..)
   or: vmat fields <dataset> [name_only] [transpose]
       To list the fields with their names (if 'name_only' is specified, the indexes won't be displayed,
       and if 'transpose' is also added, the fields will be listed on a single line)
   or: vmat fieldinfo <dataset> <fieldname_or_num> [--bin]
       To display statistics for that field
   or: vmat bbox <dataset> [<extra_percent>]
       To display the data bounding box (i.e., for each field, its min and max, possibly extended by +-extra_percent ex: 0.10 for +-10% of the data range )
   or: vmat cat <dataset> [<optional_vpl_filtering_code>]
       To display the dataset
   or: vmat sascat <dataset.vmat> <dataset.txt>
       To output in <dataset.txt> the dataset in SAS-like tab-separated format with field names on the first line
   or: vmat view <dataset>
       Interactive display to browse on the data.
   or: vmat stats <dataset>
       Will display basic statistics for each field
   or: vmat convert <source> <destination> [--cols=col1,col2,col3,...]
       To convert any dataset into a .amat, .pmat, .dmat or .csv format.
       The extension of the destination is used to determine the format you want.
       If the option --cols is specified, it requests to keep only the given columns
       (no space between the commas and the columns); columns can be given either as a
       number (zero-based) or a column name (string).  You can also specify a range,
       such as 0-18, or any combination thereof, e.g. 5,3,8-18,Date,74-85
       If .csv (Comma-Separated Value) is specified as the destination file, the
       following additional options are also supported:
         --skip-missings: if a row (after selecting the appropriate columns) contains
                          one or more missing values, it is skipped during export
         --precision=N:   a maximum of N digits is printed after the decimal point
         --delimiter=C:   use character C as the field delimiter (default = ',')
   or: vmat gendef <source> [binnum1 binnum2 ...]
       Generate stats for dataset (will put them in its associated metadatadir).
   or: vmat genvmat <source_dataset> <dest_vmat> [binned{num} | onehot{num} | normalized]
       Will generate a template .vmat file with all the fields of the source preprocessed
       with the processing you specify
   or: vmat genkfold <source_dataset> <fileprefix> <kvalue>
       Will generate <kvalue> pairs of .vmat that are splitted so they can be used for kfold trainings
       The first .vmat-pair will be named <fileprefix>_train_1.vmat (all source_dataset except the first 1/k)
       and <fileprefix>_test_1.vmat (the first 1/k of <source_dataset>
   or: vmat diff <dataset1> <dataset2> [<tolerance> [<verbose>]]
       Will report all elements that differ by more than tolerance (defauts to 1e-6).
       If verbose==0 then print only total number of differences
   or: vmat cdf <dataset> [<dataset> ...]
       To interactively display cumulative density function for each field
       along with its basic statistics
   or: vmat diststat <dataset> <inputsize>
       Will compute and output basic statistics on the euclidean distance
       between two consecutive input points

<dataset> is a parameter understandable by getDataSet:
Dataset specification can be one of:
 - the path to a matrix file (or directory) .amat .pmat .vmat .dmat or plain ascii
 - ... 
\end{verbatim}

OK, too many subcommands here, but let's concentrate on the few ones you're most likely to use:

\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> plearn vmat info 1d_reg.amat
plearn 0.92.0  (Jun 21 2005 12:04:50)
5 x 2
inputsize: 1
targetsize: 1
weightsize: 0
extrasize: 0


valhalla:~/PLearn/examples/Tutorial> plearn vmat fields 1d_reg.amat
plearn 0.92.0  (Jun 21 2005 12:04:50)
FieldNames:
0: x
1: y


valhalla:~/PLearn/examples/Tutorial> plearn vmat fieldinfo 1d_reg.amat y
plearn 0.92.0  (Jun 21 2005 12:04:50)
[------------------------------------- Computing statistics (5) -------------------------------------]
[....................................................................................................]
Field #1:  y     type: UnknownType
nmissing: 0
nnonmissing: 5
sum: 25.5
mean: 5.09999999999999964
stddev: 1.74642491965729807
min: 3
max: 7.5


valhalla:~/PLearn/examples/Tutorial> plearn vmat cat 1d_reg.amat
plearn 0.92.0  (Jun 21 2005 12:04:50)
0 3
0.5 4
1 5
2 6
3 7.5
\end{verbatim}

If you want to browse the data matrix interactively, you can use the
command \verb!plearn vmat view 1d_reg.amat! (This is most useful for huge data sets\ldots. \verb!plearn! need to be compiled with \verb!curse!.)

You can also see the points graphically by using the \verb!pyplot! script
\verb!pyplot plot_2d 1d_reg.amat!

\section{vmat File Formats}

The {\em V} in \VMatrix stands for {\em Virtual}, because \VMatrix is a C++ virtual
base class of which there are several concrete derived classes (do a
\verb!plearn help VMatrix! if you want to see how many\ldots).

Accordingly, there are several file formats that represent real data
matrices, distinguished by their file extension:

% \begin{table}[h]
% \caption{vmat file extensions and formats}
% \label{tab:vmat-formats}
\begin{center}
\begin{tabular}{|l|l|} \hline 
{\bf extension} & {\bf format description} \\ \hline
.amat           & Simple ascii format  \\ \hline
.pmat           & Simple raw binary format with 1 line ascii header  \\ \hline
.dmat           & Directory containing compressed binary data        \\
                & (possibly split in several files for huge data)    \\ \hline
.vmat           & Contains the specification of a C++ VMatrix object \\ 
                & (in PLearn's ascii serialisation format)           \\ \hline
.pymat          & Python preprocessing code that generates the       \\ 
                & specification of a C++ VMatrix object (a la .vmat) \\ \hline
\end{tabular}
\end{center}
% \end{table}

In addition, several of those tend to have an associated {\tt .metadata}
directory, that will contain associated data that is not held within the
file itself (for ex: fieldnames, inputsize and targetsize, field statistics, etc\ldots)

You can convert from any format to {\tt .amat, .pmat, .dmat, .csv} with PLearn
command {\tt vmat convert}:
\begin{verbatim}
plearn vmat convert 1d_reg.amat 1d_reg.pmat
plearn vmat view 1d_reg.pmat
\end{verbatim}

\section{PLearn Objects, Their Serialization and Specification}

PLearn is first and foremost a C++ class library. PLearn also provides a
mechanism to serialize such objects to and from files (i.e. write a
representation of an in-memory object to a file, or later reload such a
saved object from that file). PLearn serialization supports both an ASCII
human-readable format (\verb!plearn_ascii!), and a more efficient binary format (\verb!plearn_binary!).

As a result of this capability, it is also possible to {\em specify} a
PLearn object by simply writing its ASCII serialized form by hand. 
This is basically what a {\tt .vmat} file contains: {\em the ASCII serialised form of a C++ subclass of VMatrix}.

For example, create a file \verb!selected_rows.vmat! with the following content:

\begin{verbatim}
SelectRowsVMatrix(
  source = AutoVMatrix( specification = "1d_reg.amat" ),
  indices = [ 1 1 3 0 3 4],
  inputsize =   1,
  targetsize =  0,
  weightsize =  1
);
\end{verbatim}

The serialised form of most PLearn objects, as can be seen here, is:
\begin{verbatim}
ObjectName(  
  optionname = optionval
  optionname = optionval
  ...
)
\end{verbatim}

Note that in \verb!plearn_ascii! format, in general, spaces, newlines, commas and semicolons
are ignored (any sequence of those is considered a single separator).

There is typically a one to one correspondance between an object's {\em
  options} (in its serialised form) and the fields of the corresponding C++
object. A PLearn object often has many options, but they always have a default value,
so that there is no need to explicitly set those for which the default value is fine.

The above {\tt .vmat} specifies an object of type {\tt SelectRowsVMatrix},
which is a sort of vmat that will select desired rows from another
``source'' vmat.  \verb!selected_rows.vmat! will thus be an {\em altered
  view} of \verb!1d_reg.amat!, for which we also change the values of {\em
  inputsize, targetsize, weightsize}.

\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> plearn vmat info selected_rows.vmat
plearn 0.92.0  (Jun 22 2005 19:42:18)
6 x 2
inputsize: 1
targetsize: 0
weightsize: 1


valhalla:~/PLearn/examples/Tutorial> plearn vmat cat selected_rows.vmat
plearn 0.92.0  (Jun 22 2005 19:42:18)
0.5 4
0.5 4
2 6
0 3
2 6
3 7.5
\end{verbatim}

Help on any plearn object can be obtained, as usual, by invoking
\verb!plearn help ! {\em objectclass}. This will output a commented serialised object, with all its build {\em
  options} and their default value. This help is also available in online html form. For ex. try:

\begin{verbatim}
plearn help SelectRowsVMatrix
\end{verbatim}

This makes for a good starting point for writing a {\tt .vmat} (or {\tt
  .plearn}), as you can issue:
\begin{verbatim}
plearn help SelectRowsVMatrix > mymat.vmat
\end{verbatim}
and then edit the file to your liking (removing unnecessary options that
are to keep their default value, etc...)

{\tt .vmat} is not the only file extension associated with specifications
of PLearn objects in serialised form. Here are the other extensions you may
encounter:

\begin{center}
\begin{tabular}{|l|l|} \hline 
{\bf extension} & {\bf format description} \\ \hline
.vmat           & specification of a subclass of VMatrix in \verb!plearn_ascii!    \\ 
                & serialization format (with rudimentary macro-processing)  \\ \hline
.plearn         & specification of any PLearn object in \verb!plearn_ascii!        \\
                & format (with rudimentary macro-processing)                \\ \hline
.psave          & serialized PLearn object in \verb!plearn_ascii! or \verb!plearn_binary! \\
                & format (does not undergo macro-explansion)                \\ \hline
.pymat          & Python preprocessing code that generates the              \\ 
                & \verb!plearn_ascii! specification of a VMatrix subclass          \\ \hline 
.pyplearn       & Python preprocessing code that generates the              \\ 
                & \verb!plearn_ascii! specification of any PLearn object           \\ \hline
\end{tabular}
\end{center}

While {\tt .vmat} and {\tt .plearn} support some rudimentary
macro-processing, this is deprecated in favor of the power of the Python
preprocessing of {\tt .pymat} and {\tt .pyplearn} files. We will get back
to this later.

\section{plearn Learner}

The concept of a learning algorithm in PLearn is implemented through the \PLearner class.
Conceptually a \PLearner is an object that:
\begin{itemize}
\item can be {\em trained} using a training data set (which contains input and target)
\item can then be {\em used} by {\em computing outputs} corresponding to new inputs
\item can be {\em tested} on a test set (containing input and target) and
  report statistics on some {\em costs} (ex: classification error rate).
\item can be saved to and loaded from file (like any PLearn object)
\end{itemize}

The meaning and form of the output vector are learner-dependant, but in PLearn we try to respect 
the following convention for standard tasks:
\begin{itemize}
\item {\bf regression:} output is the {\em predicted} target (i.e. same dimension as terget)
\item {\bf classification:} target is a scalar between $0$ and {\em
    nclasses-1}; output is a vector of length {\em nclasses} giving a score for each class (the higher, the more likely).
\item {\bf density estimation:} output is typically the log of the estimated density at $x$ (but this can be controlled by an option, if you want for ex. the density instead of the log).
\end{itemize}

For ex. let us create a file \verb!linreg.plearn! with the following content:
\begin{verbatim}
LinearRegressor(
  weight_decay = 1e-6
  )
\end{verbatim}

LinearRegressor is a subclass of \PLearner and as such, it can be trained,
used, tested with the \verb!plearn learner! command:

\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> plearn help learner
plearn 0.92.0  (Jun 22 2005 19:42:18)
*** Help for command 'learner' ***
Allows to train, use and test a learner
learner train <learner_spec.plearn> <trainset.vmat> <trained_learner.psave>
  -> Will train the specified learner on the specified trainset and save the resulting trained learner as
     trained_learner.psave

learner test <trained_learner.psave> <testset.vmat> <cost.stats> [<outputs.pmat>] [<costs.pmat>]
  -> Tests the specified learner on the testset. Will produce a cost.stats file (viewable with the plearn stats
     command) and optionally saves individual outputs and costs

learner compute_outputs <trained_learner.psave> <test_inputs.vmat> <outputs.pmat> (or 'learner co' as a shortcut)

learner compute_outputs_on_1D_grid <trained_learner.psave> <gridoutputs.pmat> <xmin> <xmax> <nx> (shortcut: learner cg1)
  -> Computes output of learner on nx equally spaced points in range [xmin, xmax] and writes the list of (x,output)
     in gridoutputs.pmat

learner compute_outputs_on_2D_grid <trained_learner.psave> <gridoutputs.pmat> <xmin> <xmax> <ymin> <ymax> <nx> <ny> (shortcut: learner cg2)
  -> Computes output of learner on the regular 2d grid specified and writes the list of (x,y,output) in gridoutputs.pmat

learner compute_outputs_on_auto_grid <trained_learner.psave> <gridoutputs.pmat> <trainset.vmat> <nx> [<ny>] (shortcut: learner cg)
  -> Automatically determines a bounding-box from the trainset (enlarged by 5%), and computes the output along a
     regular 1D grid of <nx> points or a regular 2D grid of <nx>*<ny> points. (Note: you can also invoke command vmat
     bbox to determine the bounding-box by yourself, and then invoke learner cg1 or learner cg2 appropriately)

learner analyze_inputs <data.vmat> <results.pmat> <epsilon> <learner_1> ... <learner_n>
  -> Analyze the influence of inputs of given learners. The output of each sample in the data VMatrix is computed
     when each input is perturbed, so as to estimate the derivative of the output with respect to the input. This
     is averaged over all samples and all learners so as to estimate the influence of each input. In the results.pmat
     file, are stored the average, variance, min and max of the derivative for all inputs (and outputs).

The datasets do not need to be .vmat they can be any valid vmatrix (.amat .pmat .dmat)
\end{verbatim}

To train this linear regressor on our data-set \verb!1d_reg.amat! and save
the resulting trained learner as \verb!linreg_trained.psave! we issue the
following command:
\begin{verbatim}
plearn learner train linreg.plearn 1d_reg.amat linreg_trained.psave
\end{verbatim}

To get the predicions of the trained learner on new data that was not in
the training set, (for ex. $x=0.25, x=1.5, x=2.5$) we can create a file
\verb!1d_reg_test.amat! containing
\begin{verbatim}
#size: 3 1
#: x
#sizes: 1 0 0
0.25
1.5
2.5
\end{verbatim}

and issue the commands
\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> plearn learner compute_outputs linreg_trained.psave 1d_reg_test.amat 1d_reg_test_outputs.pmat
plearn 0.92.0  (Jun 22 2005 19:42:18)
[---------------------------------------- Using learner (3) -----------------------------------------]
[....................................................................................................]

valhalla:~/PLearn/examples/Tutorial> plearn vmat cat 1d_reg_test_outputs.pmat
plearn 0.92.0  (Jun 22 2005 19:42:18)
3.58836232959270118
5.3879309848394854
6.82758590903691243
\end{verbatim}

We thus get the predictions output by the learner.

To see the learnt parameters of the trained learner, we can examine the file \verb!linreg_trained.psave! :
\begin{verbatim}
*1 ->LinearRegressor(
include_bias = 1 ;
cholesky = 1 ;
weight_decay = 9.99999999999999955e-07 ;
output_learned_weights = 0 ;
weights = 2  1  [
3.22844859854334443
1.43965492419742724
]
;
AIC = -2.53047027031051597 ;
BIC = -2.6866951053368755 ;
resid_variance = 1 [ 0.0596271276504959716 ] ;
expdir = "" ;
stage = 0 ;
n_examples = 5 ;
inputsize = 1 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0  )
\end{verbatim}

We can see that there are many more {\em options} in the saved learner than
what we specified. In particular the {\em weights} option gives us the
parameters tuned by the learning (i.e. the regression weights).

For 1D regression problems such as this, we can easily display the
predicted output along the real line:
\begin{verbatim}
pyplot 1d_regression 1d_reg.amat linreg.plearn
\end{verbatim}

This will train the given learner on the given training set, compute the
output prediction along the real line, and plot the result.

\section{A density estimation example}

Let's make a new data matrix \verb!spiral.vmat! containing:

\begin{verbatim}
VMatrixFromDistribution(
  distr = SpiralDistribution(),
  # nsamples=10600,
  nsamples=200,
  inputsize=2,
  targetsize=0,
  weightsize=0);
\end{verbatim}

\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> plearn vmat view spiral.vmat

valhalla:~/PLearn/examples/Tutorial> pyplot plot_2d spiral.vmat
\end{verbatim}

Now let's make \verb!parzen.plearn!
\begin{verbatim}
ParzenWindow(
sigma_square = 0.06;
outputs_def = "d"  ;
);
\end{verbatim}

and check how well it estimates the density:

\begin{verbatim}
valhalla:~/PLearn/examples/Tutorial> pyplot 2d_density spiral.vmat parzen.plearn
\end{verbatim}


\section{A classification example}

See the older tutorial.\ref{oldtut}

Note that we can make a classification data set by issuing
\begin{verbatim}
pypoints 2d_classif.amat
\end{verbatim}

\section{Running a Full Experiment: PTester}


The class \verb|PTester| is used to wrap the action of running a complete
experiment in a single runnable \verb|PLearn| object.  The goals of this
class are as follows:

  \begin{itemize}
  \item Take a dataset (either a .amat, .vmat, .pmat or .pymat) and
    \emph{split it} into one or more training and test sets.  We shall
    denote the $k$-th such split as \emph{Split-k}.

  \item For each split, the \verb|PTester| trains an associated learner
    (which must be of a class derived from \verb|PLearner|) on the training
    set of the split.

  \item For each split, the \verb|PTester| then tests the trained learner
    on the testset data.  Afterwards, it can compute performance statistics
    and report.
  \end{itemize}

The relationship among the various parts is illustrated in
Figure~\ref{fig:ptesteroverall}.

\begin{figure}
  \centering
  \resizebox{0.85\textwidth}{!}{\includegraphics{Figures/PTesterOverall}}
  \caption{Relationship among the classes taking part in the experiment run
    by PTester.  The PLearner must actually be an instance of a class
    derived from PLearner; likewise, the Splitter must be an instance of a
    class derived from Splitter.  The desired statistics are specified as
    options of the PTester object, and the experiment results are stored in
    the experiment directory.}
\label{fig:ptesteroverall}
\end{figure}

\subsection{Process Underlying PTester}

The process underlying PTester is illustrated in
Figure~\ref{fig:ptesterprocess}.

\begin{figure}
  \centering
  \resizebox{0.85\textwidth}{!}{\includegraphics{Figures/PTesterProcess}}
  \caption{Process Underlying PTester}
  \label{fig:ptesterprocess}
\end{figure}

\subsection{Experiment Directory}

\verb|PTester| executes its experiment in a designated \emph{experiment
  directory} (often abbreviated \verb|expdir|, the name of the option used
to specify it within the \verb|PTester| object.)  This directory should be
empty at the beginning of the experiment (if it does not exist, it is
created automatically); if it contains the results of a previous
experiment, \verb|PTester| complains loudly and exits immediately.

Note that if you run your experiments from \verb|.pyplearn| scripts, a
synthetic experiment directory of the form
\verb|expdir_YYYY_MM_DD_HH:MM:SS| is created for you automatically, which
pretty much guarantees uniqueness of the name.

\subsection{Example}

(See the \verb|.pyplearn| tutorial.)


\section{Python Preprocessing}

See the {\bf pyplearn tutorial}

\chapter{Older Tutorial}
\label{oldtut}
\include{tutorial}


\chapter{Basics}

\section{The plearn Program}

The {\tt plearn} program is to be found in PLearn/commands and is used to
\begin{itemize}
\item either run a .plearn script 
\item or run a plearn command
\end{itemize}

Plearn scripts are essentially text files ending in .plearn that describe
a learning experiment to be performed.

Plearn commands are typically little tools that allow you to manipulate or examine
datasets or result files, but they can also launch more evolved interactive programs.

The {\tt plearn} program has a simple yet very useful command-line help system.
Type \verb!plearn help! to have an overview.

\section{Essential Commmands}

The basic plearn command is \texttt{plearn script.plearn}.

The wisest command is \texttt{plearn help ClassFoo}.

But there are others:

\texttt{plearn vmat view bidule.vmat} to view any .vmat, .pmat or .amat file.

\texttt{plearn vmat convert truc.pmat truc.amat} to convert a specific data format in an other.

\texttt{plearn learner train}, \texttt{plearn learner test}, \texttt{plearn
  learner computes\_output} provide useful shortcuts to avoid creating long
.plearn script (cf. Tutorial).

If you are interested in more information,

\begin{verbatim}
plearn help commands
plearn help vmat
plearn help learner
\end{verbatim}

\section{Essential Classes}

Here is a list of essential classes.

\begin{verbatim}
plearn help AutoVMatrix
plearn help PTester
plearn help Optimizer
--- plearn help GradientOptimizer
plearn help PLearner
--- plearn help NNet
\end{verbatim}

\section{The .plearn Object File Format}

PLearn uses the same simple file format, both to describe experiments to be
performed (in .plearn scripts), and to save and restore objects
such as a trained neural-network (in .psave or .spec files).

Essentially these files contain the specifications of PLearn objects.

This is a typical .plearn script:

\verbatiminput{../examples/regression.plearn}

Objects are specified by the name of their type, followed by a list of
\verb!option = value! pairs. 

Any sequence of spaces, newlines, tabs, comma, or semicolon is considered a
separator. So colons and semicolons are just there to ease the reading,
spaces would work just as well.

Comments start with a \verb!#! and continue until the end of the line.

The following table sums up the formats that can be used for the values of
an option of a given type

\begin{table}[h]
\caption{ Ascii format for given data-types }
\label{tab:ascii-format-ex}
\begin{tabular}{|l|l|} \hline 
{\bf Data type}         & {\bf Format example} \\ \hline
Any subclass of Object & {\tt ObjectType( option1 = value1, option2 = value2, ... )} \\ \hline
integer                 & {\tt -365} \\ \hline
floating number         & {\tt -3.2e-4} \\ \hline
string                  & {\tt "any string"} \\ \hline
character               & {\tt 'x'} \\ \hline
1D sequences            & {\tt [ 10, 20, 30, 40 ] } \\ 
                        & {\tt [ 10 20 30 40 ] } \\
                        & {\tt 4 [ 10 20 30 40 ] } \\ 
                        & {\tt 4 [ "aa", "bb", "cc", "dd" ] } \\ \hline
2D matrices             & \verb!3 2 [ 1 2  10 20  30 40 ]!    \\ \hline
pairs                   & {\tt (1, "one")}  \\ \hline
tuples                  & {\tt (1, "one", 3.5)}  \\ \hline
maps                    & \verb!{ 1:"one", 2 :"two", 3: "three" }! \\ \hline
pointers to new object  & \verb!*1 -> ObjType( ... )! \\ \hline
reference to pointer    & \verb!*1;! \\ \hline
\end{tabular}
\begin{center}
\end{center}
\end{table}

Note for strings: unquoted strings, while not recommended are also supported. They are read until
a separator (blank, comma, \ldots) or opening or closing symbol (parenthesis, bracket, \ldots) is met.


\section{The .amat File Format}

Ascii data file.

The new format is as follows:
\begin{itemize}
\item The size of the matrix is indicated by a line starting with \verb!#size:! and followed by length (number of rows) and width (number of columns).
\item An optional line starting with \verb!#sizes:! gives the {\tt
  inputsize, targetsize, weightsize, extrasize}.
\item An optional line starting with \verb!#:! gives the names of the fields (the columns)
\item Regular comment lines start with a single \verb!#!.
\end{itemize}

ex:

\begin{verbatim}
# Characteristics of a population of 534
#size: 534 3
#sizes: 2 1 0 0 
#:  age height weight
     33  1.72   71
    25  1.80   80
\end{verbatim}

\section{The .pmat File Format}

PLearn native binary format.

\section{The .vmat File Format}

File containing a description of a virtual dataset.

A .vmat contains the specification of a subclass of VMatrix, in plearn
serialization format. 

\verbatiminput{../examples/train.vmat} 

% This is not a NO-Lisa information

% \section{Preprogrammed datasets}
% 
% \begin{itemize}
% \item 2d
% \item letters -> Letter Image Recognition data
% \item breast -> The Wisconsin Breast Cancer problem
% \item usps
% \item mnist
% \end{itemize}


\chapter{Howto}

\section{How to Build a Neural Network?}

You should have learned with the tutorial basic PLearn neural network. The class used is NNet.

Here is a basic NNet script object:

\verbatiminput{../examples/BasicNNet.spec}


% ------------------------------------------------------------------------------------------
% ------------------------------------------------------------------------------------------
\chapter{Advanced}
% ------------------------------------------------------------------------------------------
% ------------------------------------------------------------------------------------------

\section{The .dmat/ Format }

Directory containing compressed data.

Contains:
\begin{itemize}
\item 0.data, 1.data, 2.data
\item indexfile
\item fieldnames
\end{itemize}


\section{The VPL language}
VPL (vmat processing language) is a home brewed mini-language in postfix
notation. As of today, it is used is the \{PRE,POST\}FILT\-ERING and
PROCESSING sections of a .vmat file. It can handle reals as well as dates
(format is: CYYMMDD, where C is 0 (1900-1999) or 1 (2000-2099). The
language will not be extensively described here. For more info, you can
look at plearn/vmat/VMatLanguage.*.

A VPL code snippet is always applied to the row of a VMatrix, and can only refer to data of that row. The result of the execution will be a vector, which is the execution stack at code termination. 

When you use VPL in a PROCESSING section, each field you declare must have its associated fieldname declaration. The compiler will ensure that the size of the result vector and the number of declared fieldnames match. This doesn't apply in the filtering sections since the result is always a single value. 

To declare a fieldname, use a colon with the name immediately after. To batch-declare fieldnames, use eg.:myfield:1:10. This will declare fields myfield1 up to myfield10.

There are two notations to refer to a field value: the @ symbol followed by the fieldname, or \% followed by the field number.

To batch-copy fields, use the following syntax: [field1:fieldn] (fields can be in @ or \% notation).

Here's a real-life example:

\begin{verbatim}
@lease_indicator 88 == 1 0 ifelse :lease_indicator
@rate_class 1 - 7 onehot :rate_class:0:6
@collision_deductible { 2->1; 4->2; 5->3; 6->4; 7->5; 
[8 8]->6; MISSING->0; OTHER->0 }
7 onehot :collision_deductible:0:6
@roadstar_indicator 89 == 1 0 ifelse :roadstar_indicator
\end{verbatim}

\section{The Metadata Directory}

A metadata directory is associated with each dataset.  For the datasets
corresponding to a file ({\tt .amat, .pmat, .vmat}) or directory ({\tt .dmat/}) the
associated metadata directory is obtained by appending {\tt .metadata/} to the
file or directory name.

A metadata directory will typically contain the following cache directories to avoid recomputing costly things

\begin{itemize}
\item \verb!STATSCACHE/! contains cached statistics
\item \verb!MODELCACHE/<classname>/! contains any pertinent cached data computed on this dataset by objects of class \verb!<classname>!
\end{itemize}

In addition, the {\tt .metadata} directory associated with a {\tt .vmat} may contain
\begin{itemize}
\item {\tt precomputed.dmat/} or {\tt precomputed.pmat} if the {\tt .vmat} description specified \verb!<PRECOMPUTE>!
\item {\tt source.index} containing row indexes in the source (resulting from \verb!<PREFILTER>!, \verb!<POSTFILTER>!, \verb!<SHUFFLE>!)
\end{itemize}



\chapter{Appendix A: File Formats}

\section{The .plearn and .psave Formats}

\subsection{Generalities on mixing ascii and binary}

The following characters are in many cases skipped before reading any
element: space, tab, newline, carriage-return, comma and semicolon. They
are essentially ignored. Binary serialized things should always start with
a non-printable ascii character.


\subsection{TVec and TMat}

TVec and TMat will be serialized differently depending on the {\em
implicit\_storage} flag of the PStream they are being written to.

If {\em implicit\_storage} is set, then serialization won't write the actual
whole structure of the TVec or TMat, but will only save the size information
and elements as a 1D or 2D {\em sequence} (see \ref{ascii-sequence} and
\ref{binary-sequence}), ex:

\begin{verbatim}
4 [ 1.2 3.5 2.8 5.2 ]

3 2 [
0.1    0.2
0.3    0.4
0.5    0.6
]
\end{verbatim}

If {\em implicit\_storage} is false, then the complete structure of the
TVec or TMat with the pointer to its storage (possibly shared with others)
will be written explicitly. This corresponds to true, deep serialization.

Ex:

\begin{verbatim}
TVec( 4 0 
*1->Storage(4 [ 1.2 3.5 2.8 5.2 ]) )

TMat( 3 2 2 0 
*2->Storage(6 [ 0.1 0.2 0.3 0.4 0.5 0.6 ] ) )
\end{verbatim}

For TVec, we have {\em length offset} followed by the storage pointer.
For TMat, we have {\em length width mod offset} followed by the storage pointer.

This allows to keep structure. For example, if we had a submatrix viewing
the second column of the previous TMat, we would have:

\begin{verbatim}
TMat( 3 1 2 1
*2 )
\end{verbatim}

\subsection{Binary PLearn format for base types}

To allow mixing of ascii and binary in a file, a non-printable ascii
character is used as a one-byte header to identify any binary portion.  In
Table~\ref{tab:base-types} we give the header codes for all basic types

Note that char is considered to be the same as signed char, and long is
considered to be the same as int, i.e.: 4-bytes long, which is the case on
current architectures.

\begin{table}[h]
\caption{ Binary-header codes for base types }
\label{tab:base-types}
\begin{tabular}{|llcl|} \hline 
Base type      & Byte order    & Header byte & Number of bytes to follow \\ \hline 
char           & -             & 0x01        & 1 \\
signed char    & -             & 0x01        & 1 \\
unsigned char  & -             & 0x02        & 1 \\
short          & little-endian & 0x03        & 2 \\
short          & big-endian    & 0x04        & 2 \\
unsigned short & little-endian & 0x05        & 2 \\
unsigned short & big-endian    & 0x06        & 2 \\
int            & little-endian & 0x07        & 4 \\
int            & big-endian    & 0x08        & 4 \\
unsigned int   & little-endian & 0x0B        & 4 \\
unsigned int   & big-endian    & 0x0C        & 4 \\
long           & little-endian & 0x07        & 4 \\
long           & big-endian    & 0x08        & 4 \\
unsigned long  & little-endian & 0x0B        & 4 \\
unsigned long  & big-endian    & 0x0C        & 4 \\
float          & little-endian & 0x0E        & 4 \\
float          & big-endian    & 0x0F        & 4 \\
double         & little-endian & 0x10        & 8 \\
double         & big-endian    & 0x11        & 8 \\ \hline 
PRInt64        & little-endian & 0x16        & 4 \\
PRInt64        & big-endian    & 0x17        & 4 \\
PRUint64       & little-endian & 0x18        & 4 \\
PRUint64       & big-endian    & 0x19        & 4 \\
\end{tabular}
\begin{center}
\end{center}
\end{table}

\begin{itemize}
\item booleans are represented the same way in binary mode as in ascii mode: with the character 0 (for false) or 1 (for true). There is no header byte.
\item A date (PDate) is written with the header-byte 0xFE followed by a
  binary serialized double (with appropriate double header) representing the date in YYYYMMDD format.
\end{itemize}


\subsection{Ascii PLearn format for a sequence}
\label{ascii-sequence}

We consider both one-dimensional sequences ( array, vector, \ldots) which only have a length,
and two-dimensional sequences which have a length and a width.

Ascii-serialized one-dimensional sequences will have the following format:

{\em length} \verb{[ ... ... ... ]{

with the elements of the sequence separated by a single space.

However, on reading, several variations of this format are recognized:
\begin{itemize}
\item The elements may be separated by any number of blanks (space, tab, newline) and/or commas or semicolons.
\item The {\em length} may be omitted
\end{itemize}

Ascii-serialized two-dimensional sequences will have the following format:

{\em length} {\em width} {\tt [} 
\begin{verbatim}
... ... ...
... ... ... 
]
\end{verbatim}

with the elements of each row separated by a tab, and the rows separated by a newline.

However on reading, blanks, commas and semi-colons between elements are
completely ignored (skipped), so you may format the data as you wish.

2D Sequences are used exclusively for TMats.
Notice that it's also possible to make a 1D sequence of 1D sequences, but that's different from a 2D sequence.

\subsection{Binary PLearn format for a sequence}
\label{binary-sequence}

We consider both one-dimensional sequences ( array, vector, \ldots) which only have a length,
and two-dimensional sequences which have a length and a width.

The following table gives the corresponding header-byte:

\begin{tabular}{|lll|} \hline
Type of sequence & byte-order    & Header byte \\ \hline
one-dimensional  & little-endian & 0x12        \\ 
one-dimensional  & big-endian    & 0x13        \\ 
two-dimensional  & little-endian & 0x14        \\ 
two-dimensional  & big-endian    & 0x15        \\ \hline
\end{tabular}

All that follows is supposed to be in the byte-order implied by the header-byte.

The first header-byte is followed by an {\em element-type} byte giving the nature
of the elements in the sequence.  It can be either the byte identifying a
base-type given in Table~\ref{tab:base-types} (the endianness must match),
or {\tt '0' = 0x30} to indicate a sequence of booleans (1 byte per boolean) 
or {\tt 0xFF} to indicate a {\em generic} sequence.

The header bytes are followed by one (for 1D sequences) or two (for 2D)
4-byte int to indicate the length (and possibly width) of the sequence.
So the total header size for sequences is 6 bytes for 1D sequences and 10
bytes for 2D sequences.

This header is followed by a dump of the elements of the sequence (in
row-major mode for 2D).  Notice that a sequence of a base type, may be
saved as a {\em generic} sequence (with the {\em element-type} byte {\tt 0xFF})



\begin{tabular}{|l|l|l|} \hline
Type of sequence         & Header byte & Followed by \\ \hline
Generic on little-endian & 0x12        & size as 4-byte little-endian int, \\
                         &             & then binary serialization of the elements \\ \hline
Generic on big-endian    & 0x13        & size as 4-byte big-endian int, \\ 
                         &             & then binary serialization of the elements \\ \hline
Sequence of a base-type  & 0x14        & size as 4-byte little-endian int, \\ 
on little-endian         &             & base-type given by header byte in previous \\
                         &             & table, followed by binary dump of elements \\ \hline
Sequence of a base-type  & 0x15        & size as 4-byte big-endian int, \\ 
on big-endian            &             & base-type given by header byte in previous \\
                         &             & table, followed by binary dump of elements \\ \hline
\end{tabular}




\chapter*{License}

This document is covered by the license appearing after the title page.

\vspace*{.5cm}

The PLearn software library and tools described in this document are
distributed under the following BSD-type license:

\begin{verbatim}
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:
 
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions and the following disclaimer.
 
  2. Redistributions in binary form must reproduce the above copyright
     notice, this list of conditions and the following disclaimer in the
     documentation and/or other materials provided with the distribution.
 
  3. The name of the authors may not be used to endorse or promote
     products derived from this software without specific prior written
     permission.
 
 THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
 NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
 TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
\end{verbatim}

\end{document}
