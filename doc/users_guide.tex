%% -*- mode:latex; tex-open-quote:"\\og{}"; tex-close-quote:"\\fg{}" -*-
%%
%%  Copyright (c) 2002 by Pascal Vincent
%%
%%  $Id: users_guide.tex,v 1.4 2003/01/31 19:34:13 ducharme Exp $

\documentclass[11pt]{book}
\usepackage{t1enc}              % new font encoding  (hyphenate words w/accents)
\usepackage{ae}                 % use virtual fonts for getting good PDF
\usepackage{isolatin1}		% support for French accents

%%%%%%%%% Definitions %%%%%%%%%%%%
\newcommand{\PLearn}{\bf \it PLearn}
\newcommand{\Object}{\bf Object} 
\newcommand{\Learner}{\bf Learner} 
\newcommand{\PPointable}{\bf PPointable} 


\begin{document}

%%%%%%%% Title Page %%%%%%%%%%
\pagenumbering{roman}
\thispagestyle{empty}

\thispagestyle{empty}
\begin{center}
{\Huge PLearn User's Guide}\\
\vspace{.5cm}
{\Large How to use the PLearn Machine-Learning library and tools}\\ 
\end{center}
\pagebreak


\vspace*{10cm}

{\small

Copyright \copyright\ 1998-2002 Pascal Vincent, Yoshua Bengio \\
Copyright \copyright\ 2002 Julien Keable \\

Permission is granted to copy and distribute this document in any medium,
with or without modification, provided that the following conditions are
met:

\begin{enumerate}
\item Modified versions must give fair credit to all authors.
\item Modified versions may not be written with the aim to discredit, misrepresent, or otherwise taint the
      reputation of any of the above authors.
\item Modified versions must retain the above copyright notice, and append to
   it the names of the authors of the modifications, together with the years the
   modifications were written.
\item Modified versions must retain this list of conditions unaltered, 
    and may not impose any further restrictions.
\end{enumerate}
}

\pagebreak

%%%%%%%%% Table of contents %%%%%%%%%%%%
\addcontentsline{toc}{chapter}{\numberline{}Table of contents}
\tableofcontents

\cleardoublepage\pagebreak
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Using plearn commands}

\section{Directory and file structure for datasets}

Dataset root directory:
{\tt /home/db}

\subsection{Dataset formats}

The following dataset formats are understood by any command that expects a dataset (and by the getDataSet(\ldots) function).

\subsubsection{.amat}

Ascii data file

The new format includes a one or two lines starting with \verb!###! which have a special meaning:  
The (optional) first line will contain length and width of the matrix
The (optional) second line will contain the field names separated by \verb!|!
Regular comment lines start with a single \verb!#!.

\begin{verbatim}
# Characteristics of a population of 534
### 534 3
###   age    |   height (cm)  |  weight (in kg) 
...
\end{verbatim}

\subsubsection{.pmat}

PLearn native binary format.
Fieldnames can be specified in an associated .pmat.fieldnames file

\subsubsection{.sdb}

Any simpledb can be automatically seen as a numeric vmat dataset. 
String fields are removed.

\subsubsection{.dmat/}

Directory containing compressed data.

Contains:
\begin{itemize}
\item 0.data, 1.data, 2.data
\item indexfile
\item fieldnames
\end{itemize}

\subsubsection{.vmat}

File containing a description of a virtual dataset. The next section goes into more details.

\subsection{The vmat file format}

A .vmat is a text specification of a dataset built from the composition of source data, possibly with operations applied to it. The description is made through a set of sections (all optionnal except SOURCES) enclosed in xml-like tags.

There is a program named vmat (althought it handles many dataset formats) that can be used to perform usefull tasks with datasets. Two of them are relevant here. 

Calling \verb!vmat gendef datasetname 10 20! will first generate a stats.def file in the metadata directory of the dataset containing a define for each field for each supported statistic. Then it will generate files bins10.def and bins20.def containing defines that can be used to bin the fields (with 10 and 20 bins). These defines can then be used (with INCLUDE) in the VPL code.

Use \verb!vmat genvmat datasetname (binned\{num\} | onehot\{num\} | normalized)! to generate automatically a vmat that will do the same operation on all fields.

Here's an example of a .vmat file:

\begin{verbatim}

<SOURCES>
/u/Gaston/some_source.amat /u/Gaston/some_source3.sdb
</SOURCES>

<PREFILTER>
INCLUDE /u/Gaston/some_source.amat.metadata/stats.def
@some_field @some_field.mean >=
</PREFILTER>

# this is a comment

<JOIN>
/u/jkeable/PLearn/UserExp/jkeable/joinVMatrix/slave.amat
[id,val] == [id,val1]
MEAN(claim) :claim.mean
</JOIN>

<PROCESSING>
%0 %1 + :my_sum
# this is another comment
[%2:@claim.mean]
</PROCESSING>

<POSTFILTER>
@claim.mean isnan not
</POSTFILTER>

<PRECOMPUTE> dmat </PRECOMPUTE>

\end{verbatim}

\subsubsection{SOURCES}
The SOURCES section is the only one that is not optionnal. It describes the source dataset(s) (in any format) that the vmat will be built from. In its most complex form, the syntax of the specification is a matrix of NxM dataset strings. The datasets are first concatenated horizontally, then vertically. Thus, all datasets on a same row must have the same length, and the results of all horizontal concatenation, the same width.

\subsubsection{PREFILTER and POSTFILTER}

Those sections are use to perform filtering at different stages of the vmat construction. Prefiltering is made after processing of the SOURCES section, while the postfiltering is made after the JOIN and PROCESSING section are executed. 

The syntax of the filtering operations is VPL, which is a postfix language described further down. After execution of the code, a value must remain on the execution stack. If it non zero, the row will be kept, otherwise, it is rejected.

\subsubsection{JOIN}

This section is used to perform a pseudo-join operation between a master and a slave table. At this time, a standard join operation (cartesian product) is not possible. Instead, slave rows matching a master row are used to compute statistics appended to the master row. The syntax is :

\begin{verbatim}
slave_dataset
[master_field1,...,master_fieldN] == [slave_field1,...,slave_fieldN]
STATISTIC1(slavefield1) :newfield_name1
STATISTIC2(slavefield2) :newfield_name2
...
\end{verbatim}
where STATISTIC is one of : {COUNT | NNONMISSING | NMISSING | SUM | SUMSQUARE | MEAN | VARIANCE | STDDEV | STDERR | MIN | MAX}. 

Note that you can repeat the operation multiple times in a single JOIN section.

\subsubsection{PROCESSING}
Use this section to modify or create new fields in the dataset (e.g : binning operations, onehot conversions, etc..). The processing is specified with VPL, described later.

As soon as you declare a PROCESSING section, all fields resulting from previous operations (sources concatenation, join operations) are not included automatically in the resulting matrix (obviously, you can still refer to them), so it is empty unless you declare them manually. In the small example given above, the operations done are the replacement of the first two fields by their sum, and then the remaining fields are appended in the resulting matrix with the second line. See VPL section for more details.

\subsubsection{PRECOMPUTE}
When you need speed, use this section to create a precomputed version of you vmat. This version will be computed the first time the vmat is loaded. You specify the type of file you want the precomputed version to be saved to (dmat or pmat). See example above.

The synchronization of the precomputed version is ensured through an almost-intelligent mecanism that checks the date of sources recursively. Note : we decided that include files (e.g: statistics) that are out of date issue a warning, but will not force recomputation. This could be changed. 

\subsubsection{DEBUG}
Including this section will force output of preprocessed VPL to stderr.

\subsection{The VPL language}
VPL (vmat processing language) is a home brewed mini-language in postfix notation. As of today, it is used is the \{PRE,POST\}FILT\-ERING and PROCESSING sections of a .vmat file. It supports INCLUDEs instructions and DEFINEs (dumb named string constants). It can handle reals as well as dates (format is: CYYMMDD, where C is 0 (1900-1999) or 1 (2000-2099). The language will not be extensively decribed here. For more info, you can look at PLearnCore/VMatLanguage.*. 

A VPL code snippet is always applied to the row of a VMatrix, and can only refer to data of that row. The result of the execution will be a vector, which is the execution stack at code termination. 

When you use VPL in a PROCESSING section, each field you declare must have its associated fieldname declaration. The compiler will ensure that the size of the result vector and the number of declared fieldnames match. This doesn't apply in the filtering sections since the result is always a single value. 

To declare a fieldname, use a colon with the name immediately after. To batch-declare fieldnames, use eg. :myfield:1:10. This will declare fields myfield1 up to myfield10.

There are two notations to refer to a field value  : the @ symbol followed by the fieldname, or \% followed by the field number.

To batch-copy fields, use the following syntax : [field1:fieldn] (fields can be in @ or \% notation).

Here's a real-life example of a PROCESSING section:

\begin{verbatim}
<PROCESSING>
@lease_indicator 88 == 1 0 ifelse :lease_indicator
@rate_class 1 - 7 onehot :rate_class:0:6
@collision_deductible { 2->1; 4->2; 5->3; 6->4; 7->5; 
			[8 8]->6; MISSING->0; OTHER->0 }
		7 onehot :collision_deductible:0:6
@roadstar_indicator 89 == 1 0 ifelse :roadstar_indicator
<\PROCESSING>
\end{verbatim}

\subsection{Preprogrammed datasets}

\begin{itemize}
\item mnist
\end{itemize}

\section{The metadata directory}

A metadata directory is associated with each dataset.  For the datasets
corresponding to a file ({\tt .amat, .pmat, .vmat}) or directory ({\tt .dmat/}) the
associated metadata directory is obtained by appending {\tt .metadata/} to the
file or directory name.

A metadata directory will typically contain the following cache directories to avoid recomputing costly things

\begin{itemize}
\item \verb!STATSCACHE/! contains cached statistics
\item \verb!MODELCACHE/<classname>/! contains any pertinent cached data computed on this dataset by objects of class \verb!<classname>!
\end{itemize}

In addition, the {\tt .metadata} directory associated with a {\tt .vmat} may contain
\begin{itemize}
\item {\tt precomputed.dmat/} or {\tt precomputed.pmat} if the {\tt .vmat} description specified \verb!<PRECOMPUTE>!
\item {\tt source.index} containing row indexes in the source (resulting from \verb!<PREFILTER>!, \verb!<POSTFILTER>!, \verb!<SHUFFLE>!)
\end{itemize}

\section{Directory and file structure for experiments}

\begin{verbatim}

/home/exp/<project>/<task>/
  - dataset.aliases
  - model.aliases
   
.../modelalias/
  # for normal train/test
  <trainalias>.<testalias>.results
  <trainalias>.psave                   # best model so far
  <trainalias>.epoch<epoch>.psave
  <trainalias>.epoch<epoch>.<testalias>.outputs
  <trainalias>.epoch<epoch>.<testalias>.costs

  # For bootstrap ( <i> is the index of the trainset variant )
  <trainalias>.bs_<bsparams>_<i>.<testalias>.results
  <trainalias>.bs_<bsparams>.testalias.results.summary
  <trainalias>.bs_<bsparams>_<i>.psave   # best model so far
  <trainalias>.bs_<bsparams>_<i>.epoch<epoch>.psave
  <trainalias>.bs_<bsparams>_<i>.epoch<epoch>.<testalias>.outputs
  <trainalias>.bs_<bsparams>_<i>.epoch<epoch>.<testalias>.costs

  # For k-fold  ( <i> is the index of the trainset variant )
  <trainalias>.kf_<k>_<i>.<testalias>.results
  <trainalias>.kf_<k>.testalias.results.summary
  <trainalias>.kf_<k>_<i>.psave   # best model so far
  <trainalias>.kf_<k>_<i>.epoch<epoch>.psave
  <trainalias>.kf_<k>_<i>.epoch<epoch>.<testalias>.outputs
  <trainalias>.kf_<k>_<i>.epoch<epoch>.<testalias>.costs
  
  # For sequential validation  ( <i> is the index of the trainset variant )
  <trainalias>.sv_<svparams>_<i>.<testalias>.results
  <trainalias>.sv_<svparams>.testalias.results.summary
  <trainalias>.sv_<svparams>_<i>.psave   # best model so far
  <trainalias>.sv_<svparams>_<i>.epoch<epoch>.psave
  <trainalias>.sv_<svparams>_<i>.epoch<epoch>.<testalias>.outputs
  <trainalias>.sv_<svparams>_<i>.epoch<epoch>.<testalias>.costs
  
\end{verbatim}

And from the plearn command....:

\begin{verbatim}

The 'plearn' program is designed to strongly encourage the following
directory and file structure for carrying experiments.  Suppose we have a
root "experiment" directory, EXP, that is to contain all the results of all
the experiments on all projects ever carried.

Here is the recommended structure:

EXP/PROJECT/TASK/DATA-REPRESENTATION/MODELALIAS
|___________________________________|
                 |
            SPECDIR (will typically contain 'dataset.aliases', specifying datasets 
                                            and 'model.aliases' specifying learners with learneroptions)

PROJECT is the name of the higher level project, regarding a particular set

TASK should represent a particular learning task for which you wish to
     assess different methods (ex: condprob_of_claim)

DATA-REPRESENTATION is the notion of using different data representations
                    (tosuit underlying learner) for a task. (ex:
                    onehot_inputs_integer_target)

The directory organisation down to DATA-REPRESENTATION is only
indicative. You may name the directories as you please (including the
DATA-REPRESENTATION directory), add or omit hierarchy levels, etc...

The organisation below DATA-REPRESENTATION is however enforced by the
'plearn' program.  DATA-REPRESENTATION *must* contain a 'dataset.aliases'
file defining the datasets to be used (in that representation). Everything
below DATA-REPRESENTATION will use those datasets. 

Because it contains the actual dataset specification in dataset.aliases 
and models specificatrions in model.aliases
we also call EXP/PROJECT/TASK/DATA-REPRESENTATION/
             |___________________________________|
                              |
                           SPECDIR

Ex of dataset.aliases file:
train /home/db/finance/stock3proj/discreterepr_trainset
valid /home/db/finance/stock3proj/discreterepr_testset1
test /home/db/finance/stock3proj/discreterepr_testset2

An alias for 'train' is mandatory. You can give aliases to any number of
other datasets ('valid' and 'test' are customary but you could also use
'test1' 'testsomemore' 'strangetestset' etc...)
The actual dataset specification can be any string understood by getDataSet.


The model.aliases file should contain one line for each
learnertype+learnerparams you are considering. The line starts for an
'alias' for that particular learnertytpe+learnerparams combination. That
alias, which will also be the name of its result directory, must be
followed by a learner classname, and a semi-column separated string of
options of the form optionname=optionvalue, in the format understood by
that learner's setOption method.

Formally, that is: 
<alias> <classname> <option1=...; option2=...; ...>
ex: knn5 KNN inputsize=210; k=5 

The following files will typically be created consequently by the 'plearn' program in the corresponding <alias> directory.

- train.objective   
  Ascii file reporting the cost as it is optimized by the learner (and other possible costs)
- model#.psave        
  Contains the saved model (at 'epoch' #, see below).
- <datasetalias>.results 
  Ascii file reporting the average costs achieved by the model on the 
  specified dataset (also includes stderr, min, max of those costs)
- model#.<datasetalias>.outputs.pmat 
  The individual outputs of the model obtained on the specified dataset
- model#.<datasetalias>.costs.pmat
  The individual costs of the model obtained on the specified dataset

For learners performing iterative optimization (or some form of incremental
learning) and able to save and test intermediate models, the # stands for
the 'epoch' number of the model saved (starting at 0).  It will also
correspond to the data-row number in train.objective and
<datasetalias>.results


\end{verbatim}


\chapter{File formats}

\section{The .plearn and .psave format}

\subsection{Generalities on mixing ascii and binary}

The following characters are in many cases skipped before reading any
element: space, tab, newline, carriage-return, comma and semicolon. They
are essentially ignored. Binary serialized things should always start with
a non-printable ascii character.

\subsection{TVec and TMat}

TVec and TMat will be serialized differently depending on the {\em
implicit\_storage} flag of the PStream they are being written to.

If {\em implicit\_storage} is set, then serialisation won't write the actual
whole structure of the TVec or TMat, but will only save the size information
and elements as a 1D or 2D {\em sequence} (see \ref{ascii_sequence} and
\ref{binary_sequence}), ex:

\begin{verbatim}
4 [ 1.2 3.5 2.8 5.2 ]

3 2 [
0.1    0.2
0.3    0.4
0.5    0.6
]
\end{verbatim}

If {\em implicit\_storage} is false, then the complete structure of the
TVec or TMat with the pointer to its storage (possibly shared with others)
will be written explicitly. This corresponds to true, deep serialization.

Ex:

\begin{verbatim}

TVec( 4 0 
*1->Storage(4 [ 1.2 3.5 2.8 5.2 ]) )

TMat( 3 2 2 0 
*2->Storage(6 [ 0.1 0.2 0.3 0.4 0.5 0.6 ] ) )

\end{verbatim}

For TVec, we have {\em length offset} followed by the storage pointer.
For TMat, we have {\em length width mod offset} followed by the storage pointer.

This allows to keep structure. For example, if we had a submatrix viewing
the second column of the previous TMat, we would have:

\begin{verbatim}
TMat( 3 1 2 1
*2 )
\end{verbatim}

\subsection{Binary plearn format for base types}

To allow mixing of ascii and binary in a file, a non-printable ascii
character is used as a one-byte header to identify any binary portion.  In
Table~\ref{tab:base-types} we give the header codes for all basic types

Note that char is considered to be the same as signed char, and long is
considered to be the same as int, i.e.: 4-bytes long, which is the case on
current architectures.

\begin{table}[h]
\caption{ Binary-header codes for base types }
\label{tab:base-types}
\begin{tabular}{|llcl|} \hline 
Base type      & Byte order    & Header byte & Number of bytes to follow \\ \hline 
bool           & -             & 0x12        & 1 \\
char           & -             & 0x01        & 1 \\
signed char    & -             & 0x01        & 1 \\
unsigned char  & -             & 0x02        & 1 \\
short          & little-endian & 0x03        & 2 \\
short          & big-endian    & 0x04        & 2 \\
unsigned short & little-endian & 0x05        & 2 \\
unsigned short & big-endian    & 0x06        & 2 \\
int            & little-endian & 0x07        & 4 \\
int            & big-endian    & 0x08        & 4 \\
unsigned int   & little-endian & 0x0B        & 4 \\
unsigned int   & big-endian    & 0x0C        & 4 \\
long           & little-endian & 0x07        & 4 \\
long           & big-endian    & 0x08        & 4 \\
unsigned long  & little-endian & 0x0B        & 4 \\
unsigned long  & big-endian    & 0x0C        & 4 \\
float          & little-endian & 0x0E        & 4 \\
float          & big-endian    & 0x0F        & 4 \\
double         & little-endian & 0x10        & 8 \\
double         & big-endian    & 0x11        & 8 \\ \hline 
\end{tabular}
\begin{center}
\end{center}
\end{table}

booleans are represented the same way in binary mode as in ascii mode: with the character 0 (for false) or 1 (for true).

\subsection{Ascii plearn format for a sequence}
\label{ascii_sequence}

We consider both one-dimensional sequences ( array, vector, \ldots) which only have a length,
and two-dimensional sequences which have a length and a width.

Ascii-serialized one-dimensional sequences will have the following format:

{\em length} \verb{[ ... ... ... ]{

with the elements of the sequence separated by a single space.

However, on reading, several variations of this format are recognized:
\begin{itemize}
\item The elements may be separated by any number of blanks (space, tab, newline) and/or commas or semicolons.
\item The {\em length} may be omitted
\end{itemize}

Ascii-serialized two-dimensional sequences will have the following format:

{\em length} {\em width} {\tt [} 
\begin{verbatim}
... ... ...
... ... ... 
]
\end{verbatim}

with the elements of each row separated by a tab, and the rows separated by a newline.

However on reading, blanks, commas and semi-colons between elements are
completely ignored (skipped), so you may format the data as you wish.

2D Sequences are used exclusively for TMats.
Notice that it's also possible to make a 1D sequence of 1D sequences, but that's different from a 2D sequence.




%\begin{itemize}
%\end{itemize}

\subsection{Binary plearn format for a sequence}
\label{binary_sequence}

We consider both one-dimensional sequences ( array, vector, \ldots) which only have a length,
and two-dimensional sequences which have a length and a width.

The following table gives the corresponding header-byte:

\begin{tabular}{|lll|}
Type of sequence & byte-order    & Header byte \\ \hline
one-dimensional  & little-endian & 0x12        \\ 
one-dimensional  & big-endian    & 0x13        \\ 
two-dimensional  & little-endian & 0x14        \\ 
two-dimensional  & big-endian    & 0x15        \\ \hline
\end{tabular}

All that follows is supposed to be in the byte-order implied by the header-byte.

The first header-byte is followed by an {\em element-type} byte giving the nature
of the elements in the sequence.  It can be either the byte identifying a
base-type given in Table~\ref{tab:base-types} (the endianness must match),
or {\tt 0x12} to indicate a sequence of booleans (1 byte per boolean) 
or {\tt 0xFF} to indicate a {\em generic} sequence.

The header bytes are followed by one (for 1D sequences) or two (for 2D)
4-byte int to indicate the length (and possibly width) of the sequence.
So the total header size for sequences is 6 bytes for 1D sequences and 10
bytes for 2D seqences.

This header is followed by a dump of the elements of the sequence (in
row-major mode for 2D).  Notice that a sequence of a base type, may be
saved as a {\em generic} sequence (with the {\em element-type} byte {\tt 0xFF})



\begin{tabular}{|l|l|l|}
Type of sequence         & Header byte & Followed by \\ \hline
Generic on little-endian & 0x12        & size as 4-byte little-endian int, \\
                         &             & then binary serialization of the elements \\ \hline
Generic on big-endian    & 0x13        & size as 4-byte big-endian int, \\ 
                         &             & then binary serialization of the elements \\ \hline
Sequence of a base-type  & 0x14        & size as 4-byte little-endian int, \\ 
on little-endian         &             & base-type given by header byte in previous \\
                         &             & table, followed by binary dump of elements \\ \hline
Sequence of a base-type  & 0x15        & size as 4-byte big-endian int, \\ 
on big-endian            &             & base-type given by header byte in previous \\
                         &             & table, followed by binary dump of elements \\ \hline
\end{tabular}


\chapter*{License}

This document is covered by the license appearing after the title page.

\vspace*{.5cm}

The PLearn software library and tools described in this document are
distributed under the following BSD-type license:

\begin{verbatim}
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:
 
  1. Redistributions of source code must retain the above copyright
     notice, this list of conditions and the following disclaimer.
 
  2. Redistributions in binary form must reproduce the above copyright
     notice, this list of conditions and the following disclaimer in the
     documentation and/or other materials provided with the distribution.
 
  3. The name of the authors may not be used to endorse or promote
     products derived from this software without specific prior written
     permission.
 
 THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
 IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
 NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
 TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
\end{verbatim}

\end{document}

