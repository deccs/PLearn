#! /usr/bin/env python
import os
import getopt
import re
import shutil
import string
import subprocess
import sys
import time
import traceback
from subprocess import Popen,PIPE,STDOUT
from utils import *
from configobj import ConfigObj
from textwrap import dedent
import pdb
from threading import Thread,Lock
from time import sleep
import datetime

try:
    from random import shuffle
except ImportError:
    import whrandom
    def shuffle(list):
        l = len(list)
        for i in range(0,l-1):
            j = whrandom.randint(i+1,l-1)
            list[i], list[j] = list[j], list[i]

STATUS_FINISHED = 0
STATUS_RUNNING = 1
STATUS_WAITING = 2
STATUS_INIT = 3


#original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
class LockedIterator:
    def __init__( self, iterator ):
        self._lock     = Lock()
        self._iterator = iterator

    def __iter__( self ):
        return self

    def get(self):
        try:
            self._lock.acquire()
            return self._iterator.next()
        finally:
            self._lock.release()


    def next( self ):
        try:
            self._lock.acquire()
            return self._iterator.next()
        finally:
            self._lock.release()

class LockedListIter:
    def __init__( self, list ):
        self._lock     = Lock()
        self._list     = list
        self._last     = -1

    def __iter__( self ):
        return self

    def next(self):
        try:
            self._lock.acquire()
            self._last+=1
            if len(self._list)>self._last:
                return
            else:
                return self._list[self._last]
        finally:
            self._lock.release()


    def append( self, a ):
        try:
            self._lock.acquire()
            list.append(a)
        finally:
            self._lock.release()


#original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
class MultiThread:
    def __init__( self, function, argsVector, maxThreads=5, print_when_finished=None, sleep_time = 0):
        self._function     = function
        self._argsIterator = LockedIterator( iter( argsVector ) )
        self._threadPool   = []
        self.print_when_finish = print_when_finished
        self.running = 0
        self.init_len_list = len(argsVector)
        self.sleep_time = sleep_time

        if maxThreads==-1:
            nb_thread=len(argsVector)
        elif maxThreads<=0:
            print "[DBI] you set %d concurrent jobs. Must be higher then 0!!"%(maxThreads)
            sys.exit(1)
        else:
            nb_thread=maxThreads
        if nb_thread>len(argsVector):
            nb_thread=len(argsVector)
        for i in range( nb_thread ):
            self._threadPool.append( Thread( target=self._tailRecurse ) )

    def _tailRecurse( self ):
        for args in self._argsIterator:
            self._function( args )
        self.running-=1
        if self.print_when_finish:
            if callable(self.print_when_finish):
                print self.print_when_finish(),"left running: %d/%d"%(self.running,self.init_len_list)
            else:
                print self.print_when_finish,"left running: %d/%d"%(self.running,self.init_len_list)

    def start( self  ):
        for thread in self._threadPool:
            # necessary to give other threads a chance to run
            time.sleep( self.sleep_time )
            self.running+=1
            thread.start()

    def join( self, timeout=None ):
        for thread in self._threadPool:
            thread.join( timeout )

class DBIBase:

    def __init__(self, commands, **args ):
        #generate a new unique id
        self.unique_id = get_new_sid('')

        # option is not used yet
        self.has_short_duration = 0

        # if all machines are full, run the jobs one by one on the localhost
        self_use_localhost_if_full = 1

        # the( human readable) time format used in log file
        self.time_format = "%Y-%m-%d/%H:%M:%S"

        # Commands to be executed once before the entire batch on the submit node
        self.pre_batch = []
        # Commands to be executed before every task in tasks
        self.pre_tasks = []
        # The main tasks to be dispatched
        self.tasks = []
        # Commands to be executed after each task in tasks
        self.post_tasks = []
        # Commands to be executed once after the entire batch on the submit node
        self.post_batch = []

        # the default directory where to keep all the log files
        self.log_dir = os.path.join( 'LOGS', self.unique_id )
        self.log_file = os.path.join( self.log_dir, 'log' )

        # the default directory where file generated by dbi will be stored
        # It should not take the "" or " " value. Use "." instead.
        self.tmp_dir = 'TMP_DBI'
        #
        if not hasattr(self, 'file_redirect_stdout'):
            self.file_redirect_stdout = True
        if not hasattr(self, 'file_redirect_stderr'):
            self.file_redirect_stderr = True
        if not hasattr(self, 'redirect_stderr_to_stdout'):
            self.redirect_stderr_to_stdout = False

        # Initialize the namespace
        self.test = False
        self.dolog = False
        self.temp_files = []
        self.arch = 0 # TODO, we should put the local arch: 32,64 or 3264 bits
        for key in args.keys():
            self.__dict__[key] = args[key]

        # check if log directory exists, if not create it
        if (not os.path.exists('LOGS')):
            os.mkdir('LOGS')
        if (not os.path.exists(self.log_dir)):
#            if self.dolog or self.file_redirect_stdout or self.file_redirect_stderr:
            os.mkdir(self.log_dir)

        # If some arguments aren't lists, put them in a list
        if not isinstance(commands, list):
            commands = [commands]
        if not isinstance(self.pre_batch, list):
            self.pre_batch = [self.pre_batch]
        if not isinstance(self.pre_tasks, list):
            self.pre_tasks = [self.pre_tasks]
        if not isinstance(self.post_tasks, list):
            self.post_tasks = [self.post_tasks]
        if not isinstance(self.post_batch, list):
            self.post_batch = [self.post_batch]

    def n_avail_machines(self): raise NotImplementedError, "DBIBase.n_avail_machines()"

    def add_commands(self,commands): raise NotImplementedError, "DBIBase.add_commands()"

    def get_redirection(self,stdout_file,stderr_file):
        """Compute the needed redirection based of the objects attribute.
        Return a tuple (stdout,stderr) that can be used with popen.
        """
        output = PIPE
        error = PIPE
        if int(self.file_redirect_stdout):
            output = open(stdout_file, 'w')
        if self.redirect_stderr_to_stdout:
            error = STDOUT
        elif int(self.file_redirect_stderr):
            error = open(stderr_file, 'w')
        return (output,error)

    def exec_pre_batch(self):
        # Execute pre-batch
        if len(self.pre_batch)>0:
            pre_batch_command = ';'.join( self.pre_batch )
            if not self.test:
                (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
                self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
            else:
                print "[DBI] pre_batch_command:",pre_batch_command

    def exec_post_batch(self):
        # Execute post-batch
        if len(self.post_batch)>0:
            post_batch_command = ';'.join( self.post_batch )
            if not self.test:
                (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
                self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
            else:
                print "[DBI] post_batch_command:",post_batch_command

    def clean(self):
        print "[DBI] WARNING the clean function was not overrided by the sub class!"

    def run(self):
        pass

    def wait(self):
        print "[DBI] WARNING the wait function was not overrided by the sub class!"

    def print_jobs_status(self):
        finished=0
        running=0
        waiting=0
        init=0
        unfinished=[]
        for t in self.tasks:
            if t.status==STATUS_INIT:
                init+=1
                unfinished.append(t.id)
            elif t.status==STATUS_RUNNING:
                running+=1
                unfinished.append(t.id)
            elif t.status==STATUS_FINISHED:
                finished+=1
            elif t.status==STATUS_WAITING:
                waiting+=i
                unfinished.append(t.id)
            else:
                print "[DBI] jobs %i have an unknow status: %d",t.id
        print "[DBI] %d jobs. finished: %d, running: %d, waiting: %d, init: %d"%(len(self.tasks),finished, running, waiting, init)
        print "[DBI] jobs unfinished (starting at 1): ",unfinished

class Task:

    def __init__(self, command, tmp_dir, log_dir, time_format, pre_tasks=[], post_tasks=[], dolog = True, id=-1, gen_unique_id = True, args = {}):
        self.add_unique_id = 0
        self.id=id
        # The "python utils.py..." command is not exactly the same for every
        # task in a batch, so it cannot be considered a "pre-command", and
        # has to be actually part of the command.  Since the user-provided
        # pre-command has to be executed afterwards, it also has to be part of
        # the command itself. Therefore, no need for pre- and post-commands in
        # the Task class

        utils_file = os.path.join(tmp_dir, 'utils.py')
        utils_file = os.path.abspath(utils_file)

        for key in args.keys():
            self.__dict__[key] = args[key]
        self.dolog = dolog

        formatted_command = re.sub( '[^a-zA-Z0-9]', '_', command );
        if gen_unique_id:
            self.unique_id = get_new_sid('')#compation intense
            self.log_file = truncate( os.path.join(log_dir, self.unique_id +'_'+ formatted_command), 200) + ".log"
        else:
            self.unique_id = formatted_command[:200]+'_'+str(datetime.datetime.now()).replace(' ','_')
            self.log_file = os.path.join(log_dir, self.unique_id) + ".log"

        if self.add_unique_id:
                command = command + ' unique_id=' + self.unique_id
        #self.before_commands = []
        #self.user_defined_before_commands = []
        #self.user_defined_after_commands = []
        #self.after_commands = []

        self.commands = []
        if len(pre_tasks) > 0:
            self.commands.extend( pre_tasks )

        if self.dolog == True:
            self.commands.append(utils_file + ' set_config_value '+
                string.join([self.log_file,'STATUS',str(STATUS_RUNNING)],' '))
            # set the current date in the field LAUNCH_TIME
            self.commands.append(utils_file +  ' set_current_date '+
                string.join([self.log_file,'LAUNCH_TIME',time_format],' '))


        self.commands.append( command )
        self.commands.extend( post_tasks )
        if self.dolog == True:
            self.commands.append(utils_file + ' set_config_value '+
                string.join([self.log_file,'STATUS',str(STATUS_FINISHED)],' '))
            # set the current date in the field FINISHED_TIME
            self.commands.append(utils_file + ' set_current_date ' +
                string.join([self.log_file,'FINISHED_TIME',time_format],' '))

        #print "self.commands =", self.commands
        self.status=STATUS_INIT
    def get_status(self):
        #TODO: catch exception if value not available
        status = get_config_value(self.log_file,'STATUS')
        return int(status)

    def get_stdout(self):
        try:
            if isinstance(self.p.stdout, file):
                return self.p.stdout
            else:
                return open(self.log_file + '.out','r')
        except:
            pass
        return None

    def get_stderr(self):
        try:
            if isinstance(self.p.stderr, file):
                return self.p.stderr
            else:
                return open(self.log_file + '.err','r')
        except:
            pass
        return None

    def set_scheduled_time(self):
        if self.dolog:
            set_config_value(self.log_file, 'STATUS',str(STATUS_WAITING))
            set_config_value(self.log_file, 'SCHEDULED_TIME',
                             time.strftime(self.time_format, time.localtime(time.time())))

    def get_waiting_time(self):
        # get the string representation
        str_sched = get_config_value(self.log_file,'SCHEDULED_TIME')
        # transform in seconds from the start of epoch
        sched_time = time.mktime(time.strptime(str_sched,ClusterLauncher.time_format))

        # get the string representation
        str_launch = get_config_value(self.log_file,'LAUNCH_TIME')
        # transform in seconds from the start of epoch
        launch_time = time.mktime(time.strptime(str_launch,ClusterLauncher.time_format))

        return launch_time - sched_time

    def get_running_time(self):
        #TODO: handle if job did not finish
        # get the string representation
        str_launch = get_config_value(self.log_file,'LAUNCH_TIME')
        # transform in seconds from the start of epoch
        launch_time = time.mktime(time.strptime(str_launch,ClusterLauncher.time_format))

        # get the string representation
        str_finished = get_config_value(self.log_file,'FINISHED_TIME')
        # transform in seconds from the start of epoch
        finished_time = time.mktime(time.strptime(str_finished,ClusterLauncher.time_format))

        return finished_time - launch_time

class DBICluster(DBIBase):

    def __init__(self, commands, **args ):
        self.duree=None
        self.arch=None
        self.cwait=True
        self.force=False
        self.interruptible=False
        self.cpu=1
        self.threads=[]
        self.started=0
        self.nb_proc=50
        self.mt=None
        self.args=args
        self.mem=None
        self.os=None
        DBIBase.__init__(self, commands, **args)
        self.pre_tasks=["echo '[DBI] executing on host' $HOSTNAME"]+self.pre_tasks
        self.post_tasks=["echo '[DBI] exit status' $?"]+self.post_tasks
        self.add_commands(commands)
        self.nb_proc=int(self.nb_proc)
        self.backend_failed=0
        self.jobs_failed=0

        if not os.path.exists(self.tmp_dir):
            os.mkdir(self.tmp_dir)

    def add_commands(self,commands):
        if not isinstance(commands, list):
            commands=[commands]

        # create the information about the tasks
        id=len(self.tasks)+1
        for command in commands:
            self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                   self.time_format,self.pre_tasks,
                                   self.post_tasks,self.dolog,id,False,
                                   self.args))
            id+=1


    def run_one_job(self, task):
        DBIBase.run(self)
        task.status=STATUS_RUNNING

        remote_command=string.join(task.commands,';')
        filename=os.path.join(self.tmp_dir,task.unique_id)
        filename=os.path.abspath(filename)
        f=open(filename,'w')
        f.write(remote_command+'\n')
        f.close()
        os.chmod(filename, 0750)
        self.temp_files.append(filename)

        command = "cluster"
        if self.arch == "32":
            command += " --typecpu 32bits"
        elif self.arch == "64":
            command += " --typecpu 64bits"
        elif self.arch == "3264":
            command += " --typecpu all"
        if self.duree:
            command += " --duree "+self.duree
        if self.cwait:
            command += " --wait"
        if self.mem:
            command += " --memoire "+self.mem
        if self.force:
            command += " --force"
        if self.interruptible:
            command += " --interruptible"
        if self.cpu!=1:
            command += " --cpu "+self.cpu
        if self.os:
            command += " --os "+self.os
        command += " --execute '"+ filename + "'"

        self.started+=1
        started=self.started# not thread safe!!!
        print "[DBI, %d/%d, %s] %s"%(started,len(self.tasks),time.ctime(),command)
        if self.test:
            task.status=STATUS_FINISHED
            return

        task.launch_time = time.time()
        task.set_scheduled_time()

        (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
        task.p = Popen(command, shell=True,stdout=output,stderr=error)
        task.p_wait_ret=task.p.wait()
        task.dbi_return_status=None
        if output!=PIPE:#TODO what do to if = PIPE?
            fd=open(task.log_file+'.out','r')
            last=""
            for l in fd.readlines():
                last=l
            if last.startswith("[DBI] exit status "):
                task.dbi_return_status=int(last.split()[-1])
#        print "[DBI,%d/%d,%s] Job ended, popen returncode:%d, popen.wait.return:%d, dbi echo return code:%s"%(started,len(self.tasks),time.ctime(),task.p.returncode,task.p_wait_ret,task.dbi_return_status)
        if task.dbi_return_status==None:
            print "[DBI, %d/%d, %s] Trouble with launching/executing '%s'." % (started,len(self.tasks),time.ctime(),command)
            print "    Its execution did not finished. Probable cause is the back-end itself."
            print "    You may want to run the task again."
            print "    popen returncode: %d"     % task.p.returncode
            print "    popen.wait.return: %d"    % task.p_wait_ret
            print "    dbi echo return code: %s" % task.dbi_return_status
            self.backend_failed+=1
        elif task.dbi_return_status!=0:
            self.jobs_failed+=1
        task.status=STATUS_FINISHED

    def run(self):
        print "[DBI] The Log file are under %s"%self.log_dir
        if self.test:
            print "[DBI] Test mode, we only print the command to be executed, we don't execute them"
        # Execute pre-batch
        self.exec_pre_batch()

        # Execute all Tasks (including pre_tasks and post_tasks if any)
        self.mt=MultiThread(self.run_one_job,self.tasks,
                            self.nb_proc,lambda :"[DBI,%s]"%time.ctime(),
                            sleep_time=2)
        self.mt.start()

        # Execute post-batchs
        self.exec_post_batch()

        print "[DBI] The Log file are under %s"%self.log_dir

    def clean(self):
        #TODO: delete all log files for the current batch
        for f in self.temp_files:
            os.remove(f)

    def wait(self):
        if self.mt:
            try:
                self.mt.join()
            except KeyboardInterrupt, e:
                print "[DBI] Catched KeyboardInterrupt"
                self.print_jobs_status()
                raise

        else:
            print "[DBI] WARNING jobs not started!"
        self.print_jobs_status()
        print "[DBI] %d jobs where the back-end failed." % (self.backend_failed)
        print "[DBI] %d jobs returned a failure status." % (self.jobs_failed)

class DBIBqtools(DBIBase):

    def __init__( self, commands, **args ):
        self.nb_proc = 1
        self.clean_up = True
        self.micro = 1
        self.queue = "qwork@ms"
        self.long = False
        self.duree = "12:00:00"
        self.mem = None
        DBIBase.__init__(self, commands, **args)

        self.nb_proc = int(self.nb_proc)
        self.micro = int(self.micro)

### We can't accept the symbols "," as this cause trouble with bqtools
        if self.log_dir.find(',')!=-1 or self.log_file.find(',')!=-1:
            print "[DBI] ERROR: The log file and the log dir should not have the symbol ','"
            print "[DBI] log file=",self.log_file
            print "[DBI] log dir=",self.log_dir
            sys.exit(1)

        # create directory in which all the temp files will be created
        self.temp_dir = 'bqtools_tmp_' + os.path.split(self.log_dir)[1]
        print "[DBI] All bqtools file will be in ",self.temp_dir
        os.mkdir(self.temp_dir)
        os.chdir(self.temp_dir)

        if self.long:
            self.queue = "qlong@ms"
            # Get max job duration from environment variable if it is set.
            max = os.getenv("BQ_MAX_JOB_DURATION")
            if max:
                self.duree = max
            else:
                self.duree = "1200:00:00" #50 days

        # create the information about the tasks
        args['temp_dir'] = self.temp_dir
        self.args=args
        self.add_commands(commands)

    def add_commands(self,commands):
        if not isinstance(commands, list):
            commands=[commands]

        # create the information about the tasks
        for command in commands:
            id=len(self.tasks)+1
            self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                   self.time_format,self.pre_tasks,
                                   self.post_tasks,self.dolog,id,False,
                                   self.args))
            id+=1
    def run(self):
        pre_batch_command = ';'.join( self.pre_batch );
        post_batch_command = ';'.join( self.post_batch );

        # create one (sh) script that will launch the appropriate ~~command~~
        # in the right environment


        launcher = open( 'launcher', 'w' )
        bq_cluster_home = os.getenv( 'BQ_CLUSTER_HOME', '$HOME' )
        bq_shell_cmd = os.getenv( 'BQ_SHELL_CMD', '/bin/sh -c' )
        launcher.write( dedent('''\
                #!/bin/sh

                HOME=%s
                export HOME

                cd ../../../
                (%s '~~task~~')'''
                % (bq_cluster_home, bq_shell_cmd)
                ) )

        if int(self.file_redirect_stdout):
            launcher.write( ' >> ~~logfile~~.out' )
        if int(self.file_redirect_stderr):
            launcher.write( ' 2>> ~~logfile~~.err' )
        launcher.close()

        # create a file containing the list of commands, one per line
        # and another one containing the log_file name associated
        tasks_file = open( 'tasks', 'w' )
        logfiles_file = open( 'logfiles', 'w' )
        for task in self.tasks:
            tasks_file.write( ';'.join(task.commands) + '\n' )
            logfiles_file.write( task.log_file + '\n' )
        tasks_file.close()
        logfiles_file.close()

        # create the bqsubmit.dat, with
        bqsubmit_dat = open( 'bqsubmit.dat', 'w' )
        bqsubmit_dat.write( dedent('''\
                batchName = dbi_%s
                command = sh launcher
                templateFiles = launcher
                submitOptions = -q %s -l walltime=%s
                param1 = (task, logfile) = load tasks, logfiles
                linkFiles = launcher
                concurrentJobs = %d
                preBatch = rm -f _*.BQ
                microJobs = %d
                '''%(self.unique_id[1:12],self.queue,self.duree,self.nb_proc,self.micro)) )
        print self.unique_id
        if self.clean_up:
            bqsubmit_dat.write('postBatch = rm -rf dbi_batch*.BQ ; rm -f logfiles tasks launcher bqsubmit.dat ;')
        bqsubmit_dat.close()

        # Execute pre-batch
        self.exec_pre_batch()

        print "[DBI] All the log will be in the directory: ",self.log_dir
        # Launch bqsubmit
        if not self.test:
            for t in self.tasks:
                t.set_scheduled_time()
            self.p = Popen( 'bqsubmit', shell=True)
            self.p.wait()
        else:
            print "[DBI] Test mode, we generate all the file, but we do not execute bqsubmit"
            if self.dolog:
                print "[DBI] The scheduling time will not be logged when you will submit the generated file"

        # Execute post-batchs
        self.exec_post_batch()

    def wait(self):
        print "[DBI] WARNING cannot wait until all jobs are done for bqtools, use bqwatch or bqstatus"


# Transfor a string so that it is treated by Condor as a single argument
def condor_escape_argument(argstring):
    # Double every single quote and double quote character,
    # surround the result by a pair of single quotes,
    # then surrount everything by a pair of double quotes
    return "\"'" + argstring.replace("'", "''").replace('"','""') + "'\""

class DBICondor(DBIBase):

    def __init__( self, commands, **args ):
        self.getenv = False
        self.nice = False
        # in Meg for initialization for consistency with cluster
        # then in kilo as that is what is needed by condor
        self.mem = 0
        self.req = ''
        self.raw = ''
        self.rank = ''
        self.copy_local_source_file = False
        self.files = ''
        self.file_redirect_stdout = False
        self.file_redirect_stderr = False
        self.redirect_stderr_to_stdout = False
        self.env = ''
        self.os = ''

        DBIBase.__init__(self, commands, **args)
        self.mem=int(self.mem)*1024
        if not os.path.exists(self.log_dir):
            os.mkdir(self.log_dir) # condor log are always generated

        if not os.path.exists(self.tmp_dir):
            os.mkdir(self.tmp_dir)
        self.args = args
        self.add_commands(commands)

    def add_commands(self,commands):
        if not isinstance(commands, list):
            commands=[commands]

        # create the information about the tasks
        id=len(self.tasks)+1
        for command in commands:
            c_split = command.split()
            # c = program name, c2 = arguments
            c = c_split[0]
            if len(c_split) > 1:
                c2 = ' ' + ' '.join(c_split[1:])
            else:
                c2 = ''

            # We use the absolute path so that we don't have corner case as with ./
            shellcommand=False
            # Maybe the command is not in the form: executable_name args,
            # but starts with a character that is interpreted by the shell
            # in a special way. I.e., a sequence of commands, like:
            # 'prog1; prog2 arg1 arg2' (with the quotes).
            # The command might also be a shell built-in command.
            # Feel free to complete this list
            shell_special_chars = ["'", '"', ' ', '$', '`', '(', ';']
            authorized_shell_commands=[ "touch", "echo", "cd" ]
            if c[0] in shell_special_chars or c in authorized_shell_commands:
                shellcommand=True
            elif not self.files:
                # Transform path to get an absolute path.
                c_abs = os.path.abspath(c)
                if os.path.isfile(c_abs):
                    # The file is in the current directory (easy case).
                    c = c_abs
                elif not os.path.isabs(c):
                    # We need to find where the file could be... easiest way to
                    # do it is ask the 'which' shell command.
                    which_out = subprocess.Popen('which %s' % c, shell = True, stdout = PIPE).stdout.readlines()
                    if len(which_out) == 1:
                        c = which_out[0].strip()

            command = "".join([c,c2])

                # We will execute the command on the specified architecture
                # if it is specified. If the executable exist for both
                # architecture we execute on both. Otherwise we execute on the
                # same architecture as the architecture of the launch computer
            self.cplat = get_condor_platform()
            if self.arch == "32":
                self.targetcondorplatform='INTEL'
                newcommand=command
            elif self.arch == "64":
                self.targetcondorplatform='X86_64'
                newcommand=command
            elif self.arch == "3264":
                #the same executable will be executed on all computer
                #So it should be a 32 bits executable
                self.targetcondorplatform='BOTH'
                newcommand=command
            elif c.endswith('.32'):
                self.targetcondorplatform='INTEL'
                newcommand=command
            elif c.endswith('.64'):
                self.targetcondorplatform='X86_64'
                newcommand=command
            elif os.path.exists(c+".32") and os.path.exists(c+".64"):
                self.targetcondorplatform='BOTH'
                #newcommand=c+".32"+c2
                newcommand='if [ $CPUTYPE == \'x86_64\' ]; then'
                newcommand+='  '+c+'.64'+c2
                newcommand+='; else '
                newcommand+=c+".32"+c2+'; fi'
                if not os.access(c+".64", os.X_OK):
                    raise Exception("The command '"+c+".64' does not have execution permission!")
#                newcommand=command
                c+=".32"
            elif self.cplat=="INTEL" and os.path.exists(c+".32"):
                self.targetcondorplatform='INTEL'
                c+=".32"
                newcommand=c+c2
            elif self.cplat=="X86_64" and os.path.exists(c+".64"):
                self.targetcondorplatform='X86_64'
                c+=".64"
                newcommand=c+c2
            else:
                self.targetcondorplatform=self.cplat
                newcommand=command

            if shellcommand:
                pass
            elif not os.path.exists(c):
                raise Exception("The command '"+c+"' does not exist!")
            elif not os.access(c, os.X_OK):
                raise Exception("The command '"+c+"' does not have execution permission!")

            self.tasks.append(Task(newcommand, self.tmp_dir, self.log_dir,
                                   self.time_format, self.pre_tasks,
                                   self.post_tasks,self.dolog,id,False,
                                   self.args))
            id+=1
            #keeps a list of the temporary files created, so that they can be deleted at will

    def run_all_job(self):
        if len(self.tasks)==0:
            return #no task to run
        # create the bqsubmit.dat, with
        condor_datas = []

        #we supose that each task in tasks have the same number of commands
        #it should be true.
        if len(self.tasks[0].commands)>1:
            for task in self.tasks:
                condor_data = os.path.join(self.tmp_dir,self.unique_id +'.'+ task.unique_id + '.data')
                condor_datas.append(condor_data)
                self.temp_files.append(condor_data)
                param_dat = open(condor_data, 'w')

                param_dat.write( dedent('''\
                #!/bin/bash
                %s''' %('\n'.join(task.commands))))
                param_dat.close()


        condor_file = os.path.join(self.log_dir, "submit_file.condor")
        self.temp_files.append(condor_file)
        condor_dat = open( condor_file, 'w' )

        if self.req:
            req = self.req
        else:
            req = "True"
        if self.targetcondorplatform == 'BOTH':
            req+="&&((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
        else :
            req+="&&(Arch == \"%s\")"%(self.targetcondorplatform)
        if self.os:
            req+='&&(OpSyS == "'+self.os+'")'

        source_file=os.getenv("CONDOR_LOCAL_SOURCE")
        condor_home = os.getenv('CONDOR_HOME')
        if source_file and source_file.endswith(".cshrc"):
            launch_file = os.path.join(self.log_dir, 'launch.csh')
        else:
            launch_file = os.path.join(self.log_dir, 'launch.sh')

        if self.mem<=0:
            try:
                self.mem = os.stat(self.tasks[0].commands[0].split()[0]).st_size
            except:
                pass
        condor_dat.write( dedent('''\
                executable     = %s
                universe       = vanilla
                requirements   = %s
                output         = %s/condor.$(Process).out
                error          = %s/condor.$(Process).error
                log            = %s/condor.log
                getenv         = %s
                nice_user      = %s
                ''' % (launch_file,req,
                       self.log_dir,
                       self.log_dir,
                       self.log_dir,str(self.getenv),str(self.nice))))
        if self.mem>0:
            #condor need value in Kb
            condor_dat.write('ImageSize      = %d\n'%(self.mem))

        if self.files: #ON_EXIT_OR_EVICT
            condor_dat.write( dedent('''\
                when_to_transfer_output = ON_EXIT
                should_transfer_files   = Yes
                transfer_input_files    = %s
                '''%(self.files+','+launch_file+','+self.tasks[0].commands[0].split()[0]))) # no directory
        if self.env:
            condor_dat.write('environment    = '+self.env+'\n')
        if self.raw:
            condor_dat.write( self.raw+'\n')
        if self.rank:
            condor_dat.write( dedent('''\
                rank = %s
                ''' %(self.rank)))
        if len(condor_datas)!=0:
            for i in condor_datas:
                condor_dat.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")
        else:
            for task in self.tasks:
                argstring = condor_escape_argument(' ; '.join(task.commands))
                condor_dat.write("arguments      = %s \nqueue\n" % argstring)
        condor_dat.close()

        dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
        overwrite_launch_file=False
        if not os.path.exists(dbi_file):
            print '[DBI] WARNING: Can\'t locate file "dbi.py". Maybe the file "'+launch_file+'" is not up to date!'
        else:
            if os.path.exists(launch_file):
                mtimed=os.stat(dbi_file)[8]
                mtimel=os.stat(launch_file)[8]
                if mtimed>mtimel:
                    print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your needs!'
                    overwrite_launch_file=True

        if self.copy_local_source_file:
            source_file_dest = os.path.join(self.log_dir,
                                            os.path.basename(source_file))
            shutil.copy( source_file, source_file_dest)
            self.temp_files.append(source_file_dest)
            os.chmod(source_file_dest, 0755)
            source_file=source_file_dest

        if not os.path.exists(launch_file) or overwrite_launch_file:
            self.temp_files.append(launch_file)
            launch_dat = open(launch_file,'w')
            if source_file and not source_file.endswith(".cshrc"):
                launch_dat.write(dedent('''\
                    #!/bin/sh
                    '''))
                if condor_home:
                    launch_dat.write('export HOME=%s\n' % condor_home)
                if source_file:
                    launch_dat.write('source ' + source_file + '\n')
                launch_dat.write(dedent('''\
                    echo "Executing on " `/bin/hostname` 1>&2
                    echo "HOSTNAME: ${HOSTNAME}" 1>&2
                    echo "PATH: $PATH" 1>&2
                    echo "PYTHONPATH: $PYTHONPATH" 1>&2
                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
                    #which python 1>&2
                    #echo -n python version: 1>&2
                    #python -V 1>&2
                    #echo -n /usr/bin/python version: 1>&2
                    #/usr/bin/python -V 1>&2
                    echo "Running command: sh -c \\"$@\\"" 1>&2
                    sh -c "$@"
                    '''))
            else:
                launch_dat.write(dedent('''\
                    #! /bin/tcsh
                    \n'''))
                if condor_home:
                    launch_dat.write('setenv HOME %s\n' % condor_home)
                if source_file:
                    launch_dat.write('source ' + source_file + '\n')
                launch_dat.write(dedent('''\
                    echo "Executing on " `/bin/hostname`
                    echo "HOSTNAME: ${HOSTNAME}"
                    echo "PATH: $PATH"
                    echo "PYTHONPATH: $PYTHONPATH"
                    echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
                    #which python
                    #echo -n python version:
                    #python -V
                    #echo -n /usr/bin/python version:
                    #/usr/bin/python -V
                    #echo ${PROGRAM} $@
                    #${PROGRAM} "$@"
                    echo "Running command: $argv"
                    $argv
                    '''))
            launch_dat.close()
            os.chmod(launch_file, 0755)

        utils_file = os.path.join(self.tmp_dir, 'utils.py')
        if not os.path.exists(utils_file):
            shutil.copy( get_plearndir()+
                         '/python_modules/plearn/parallel/utils.py', utils_file)
            self.temp_files.append(utils_file)
            os.chmod(utils_file, 0755)

        configobj_file = os.path.join(self.tmp_dir, 'configobj.py')
        if not os.path.exists('configobj.py'):
            shutil.copy( get_plearndir()+
                         '/python_modules/plearn/parallel/configobj.py',  configobj_file)
            self.temp_files.append(configobj_file)
            os.chmod(configobj_file, 0755)

        # Launch condor
        if self.test == False:
            (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
            print "[DBI] Executing: condor_submit " + condor_file
            for task in self.tasks:
                task.set_scheduled_time()
            self.p = Popen( 'condor_submit '+ condor_file, shell=True)
            self.p.wait()
            if self.p.returncode != 0:
                print "[DBI] condor_submit failed! We can't stard the jobs"
        else:
            print "[DBI] Created condor file: " + condor_file
            if self.dolog:
                print "[DBI] The scheduling time will not be logged when you will submit the condor file"

    def clean(self):
        if len(self.temp_files)>0:
            sleep(20)
            for file_name in self.temp_files:
                try:
                    os.remove(file_name)
                except os.error:
                    pass
                pass


    def run(self):
        print "[DBI] The Log file are under %s"%self.log_dir

        self.exec_pre_batch()

        self.run_all_job()

        self.exec_post_batch()

    def wait(self):
        print "[DBI] WARNING no waiting for all job to finish implemented for condor, use 'condor_q' or 'condor_wait %s/condor.log'"%(self.log_dir)

    def clean(self):
        pass

class DBILocal(DBIBase):

    def __init__( self, commands, **args ):
        self.nb_proc=1
        DBIBase.__init__(self, commands, **args)
        self.args=args
        self.threads=[]
        self.mt = None
        self.started=0
        self.nb_proc=int(self.nb_proc)
        self.add_commands(commands)

    def add_commands(self,commands):
        if not isinstance(commands, list):
            commands=[commands]

        #We copy the variable localy as an optimisation for big list of commands
        #save around 15% with 100 commands
        tmp_dir=self.tmp_dir
        log_dir=self.log_dir
        time_format=self.time_format
        pre_tasks=self.pre_tasks
        post_tasks=self.post_tasks
        dolog=self.dolog
        args=self.args
        id=len(self.tasks)+1
        for command in commands:
            pos = string.find(command,' ')
            if pos>=0:
                c = command[0:pos]
                c2 = command[pos:]
            else:
                c=command
                c2=""

            # We use the absolute path so that we don't have corner case as with ./
            c = os.path.normpath(os.path.join(os.getcwd(), c))
            command = "".join([c,c2])

            # We will execute the command on the specified architecture
            # if it is specified. If the executable exist for both
            # architecture we execute on both. Otherwise we execute on the
            # same architecture as the architecture of the launch computer

            if not os.access(c, os.X_OK):
                raise Exception("The command '"+c+"' does not exist or does not have execution permission!")
            self.tasks.append(Task(command, tmp_dir, log_dir,
                                   time_format, pre_tasks,
                                   post_tasks,dolog,id,False,self.args))
            id+=1
        #keeps a list of the temporary files created, so that they can be deleted at will

    def run_one_job(self,task):
        c = (';'.join(task.commands))
        task.set_scheduled_time()

        if self.test:
            print "[DBI] "+c
            return

        (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')

        self.started+=1
        print "[DBI,%d/%d,%s] %s"%(self.started,len(self.tasks),time.ctime(),c)
        p = Popen(c, shell=True,stdout=output,stderr=error)
        p.wait()
        task.status=STATUS_FINISHED

    def clean(self):
        if len(self.temp_files)>0:
            sleep(20)
            for file_name in self.temp_files:
                try:
                    os.remove(file_name)
                except os.error:
                    pass
                pass

    def run(self):
        if self.test:
            print "[DBI] Test mode, we only print the command to be executed, we don't execute them"
        if not self.file_redirect_stdout and self.nb_proc>1:
            print "[DBI] WARNING: many process but all their stdout are redirected to the parent"
        if not self.file_redirect_stderr and self.nb_proc>1:
            print "[DBI] WARNING: many process but all their stderr are redirected to the parent"
        print "[DBI] The Log file are under %s"%self.log_dir

        # Execute pre-batch
        self.exec_pre_batch()

        # Execute all Tasks (including pre_tasks and post_tasks if any)
        self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,lambda :("[DBI,%s]"%time.ctime()))
        self.mt.start()

        # Execute post-batchs
        self.exec_post_batch()


    def clean(self):
        pass

    def wait(self):
        if self.mt:
            try:
                self.mt.join()
            except KeyboardInterrupt, e:
                print "[DBI] Catched KeyboardInterrupt"
                self.print_jobs_status()
                print "[DBI] The Log file are under %s"%self.log_dir
                raise
        else:
            print "[DBI] WARNING jobs not started!"
        self.print_jobs_status()
        print "[DBI] The Log file are under %s"%self.log_dir

class SshHost:
    def __init__(self, hostname,nice=19,get_avail=True):
        self.hostname= hostname
        self.minupdate=15
        self.lastupd= -1-self.minupdate
        self.working=True
        (self.bogomips,self.ncores,self.loadavg)=(-1.,-1,-1.)
        self.nice=nice
        if get_avail:
            self.getAvailability()

    def getAvailability(self):
        # simple heuristic: mips / load
        t= time.time()
        if t - self.lastupd > self.minupdate: # min. 15 sec. before update
            (self.bogomips,self.ncores,self.loadavg)=self.getAllHostInfo()
            self.lastupd= t
            #print  self.hostname, self.bogomips, self.loadavg, (self.bogomips / (self.loadavg + 0.5))
        return self.bogomips / (self.loadavg + 0.5)

    def getAllHostInfo(self):
        cmd= ["ssh", self.hostname ,"cat /proc/cpuinfo;cat /proc/loadavg"]
        p= Popen(cmd, stdout=PIPE)
        bogomips= -1
        ncores=-1
        loadavg=-1
        returncode = p.returncode
        wait = p.wait()
        if returncode:
            self.working=False
            return (-1.,-1,-1.)
        elif wait!=0:
            self.working=False
            return (-1.,-1,-1.)

        for l in p.stdout:
            if l.startswith('bogomips'):
                s= l.split(' ')
                bogomips+= float(s[-1])
            if l.startswith('processor'):
                s= l.split(' ')
                ncores=int(s[-1])+1

        if l:
            loadavg=float(l[0])
        #(bogomips,ncores,load average)
        return (bogomips,ncores,loadavg)

    def addToLoadavg(self,n):
        self.loadavg+= n
        self.lastupd= time.time()

    def __str__(self):
        return "SshHost("+self.hostname+" <nice: "+str(self.nice)\
               +"bogomips:"+str(self.bogomips)\
               +',ncores:'+str(self.ncores)\
               +',loadavg'+str(self.loadavg)\
               +',avail:'+str(self.getAvailability())\
               +',lastupd:'+str(self.lastupd) + '>)'

    def __repr__(self):
        return str(self)

def get_hostname():
    from socket import gethostname
    myhostname = gethostname()
    pos = string.find(myhostname,'.')
    if pos>=0:
        myhostname = myhostname[0:pos]
    return myhostname

# copied from PLearn/python_modules/plearn/pymake/pymake.py
def get_platform():
    #should we use an env variable called PLATFORM???
    #if not defined, use uname uname -i???
    pymake_osarch = os.getenv('PYMAKE_OSARCH')
    if pymake_osarch:
        return pymake_osarch
    platform = sys.platform
    if platform=='linux2':
        linux_type = os.uname()[4]
        if linux_type == 'ppc':
            platform = 'linux-ppc'
        elif linux_type =='x86_64':
            platform = 'linux-x86_64'
        else:
            platform = 'linux-i386'
    return platform

# copied from PLearn/python_modules/plearn/pymake/pymake.py
def find_all_ssh_hosts():
    hostspath_list = [os.path.join(os.getenv("HOME"),".pymake",get_platform()+'.hosts')]
    if os.path.exists(hostspath_list[0])==0:
        print "[DBI] no host file %s for the ssh backend"%(hostspath_list[0])
        sys.exit(1)
    print "[DBI] using file %s for the list of host"%(hostspath_list[0])
#    from plearn.pymake.pymake import process_hostspath_list
#    (list_of_hosts, nice_values) = process_hostspath_list(hostspath_list,19,get_hostname())
    shuffle(list_of_hosts)
    print list_of_hosts
    print nice_values
    h=[]
    for host in list_of_hosts:
        print "connecting to",host
        s=SshHost(host,nice_values[host],False)
        if s.working:
            h.append(s)
        else:
            print "[DBI] host not working:",s.hostname
        print s
    print h
    return h

def cmp_ssh_hosts(h1, h2):
    return cmp(h2.getAvailability(), h1.getAvailability())

class DBISsh(DBIBase):

    def __init__(self, commands, **args ):
        print "[DBI] WARNING: The SSH DBI is not fully implemented!"
        print "[DBI] Use at your own risk!"
        self.nb_proc=1
        DBIBase.__init__(self, commands, **args)
        self.args=args
        self.add_commands(commands)
        self.hosts= find_all_ssh_hosts()
        print "[DBI] hosts: ",self.hosts

    def add_commands(self,commands):
        if not isinstance(commands, list):
            commands=[commands]

        # create the information about the tasks
        id=len(self.tasks)+1
        for command in commands:
            self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                   self.time_format, self.pre_tasks,
                                   self.post_tasks,self.dolog,id,False,
                                   self.args))
            id+=1

    def getHost(self):
        self.hosts.sort(cmp= cmp_ssh_hosts)
        print "hosts= "
        for h in self.hosts: print h
        self.hosts[0].addToLoadavg(1.0)
        return self.hosts[0]

    def run_one_job(self, task):
        DBIBase.run(self)

        host= self.getHost()

        cwd= os.getcwd()
        command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + "'"
        print "[DBI] "+command

        if self.test:
            return

        task.launch_time = time.time()
        task.set_scheduled_time()

        (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')

        task.p = Popen(command, shell=True,stdout=output,stderr=error)
        task.p.wait()
        task.status=STATUS_FINISHED


    def run_one_job2(self, host):
        DBIBase.run(self)

        cwd= os.getcwd()
        print self._locked_iter
        for task in self._locked_iter:
            print "task",task
            command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + " ; echo $?'"
            print "[DBI, %s] %s"%(time.ctime(),command)

            if self.test:
                return

            task.launch_time = time.time()
            task.set_scheduled_time()

###            (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')

            task.p = Popen(command, shell=True,stdout=PIPE,stderr=PIPE)
            wait = task.p.wait()
            returncode = p.returncode
            if returncode:
                self.working=False

            elif wait!=0:
                self.working=False
                #redo it
            return -1.

            out=task.p.stdout.readlines()
            err=task.p.stderr.readlines()
            self.echo_result=None
            if out:
                self.echo_result=int(out[-1])
                del out[-1]
            print "out",out
            print "err",err
            print "echo result",self.echo_result
            if err:
                task.return_status = int(err[-1])  # last line was an echo $? (because rsh doesn't transmit the status byte correctly)
                del err[-1]
                print "return status", task.return_status
            sleep(1)
            task.status=STATUS_FINISHED

    def run(self):
        print "[DBI] The Log file are under %s"%self.log_dir
        if not self.file_redirect_stdout and self.nb_proc>1:
            print "[DBI] WARNING: many process but all their stdout are redirected to the parent"
        if not self.file_redirect_stderr and self.nb_proc>1:
            print "[DBI] WARNING: many process but all their stderr are redirected to the parent"

        # Execute pre-batch
        self.exec_pre_batch()
        self._locked_iter=LockedListIter(iter(self.tasks))
        if self.test:
            print "[DBI] In testmode, we only print the command that would be executed."
        print "in run",self.hosts
        # Execute all Tasks (including pre_tasks and post_tasks if any)
        self.mt=MultiThread(self.run_one_job2,self.hosts,self.nb_proc,lambda :("[DBI,%s]"%time.ctime()))
        self.mt.start()

        # Execute post-batchs
        self.exec_post_batch()

    def clean(self):
        #TODO: delete all log files for the current batch
        pass

    def wait(self):
        #TODO
        self.mt.join()
        self.print_jobs_status()


# creates an object of type ('DBI' + launch_system) if it exists
def DBI(commands, launch_system, **args):
    """The Distributed Batch Interface is a collection of python classes
    that make it easy to execute commands in parallel using different
    systems like condor, bqtools on Mammouth, the cluster command or localy.
    """
    try:
        jobs = eval('DBI'+launch_system+'(commands,**args)')
    except NameError:
        print 'The launch system ',launch_system, ' does not exists. Available systems are: Cluster, Ssh, bqtools and Condor'
        traceback.print_exc()
        sys.exit(1)
    return jobs

def main():
    if len(sys.argv)!=2:
        print "Usage: %s {Condor|Cluster|Ssh|Local|Bqtools} < joblist"%(sys.argv[0])
        print "Where joblist is a file containing one experiment on each line"
        sys.exit(0)
    DBI([ s[0:-1] for s in sys.stdin.readlines() ], sys.argv[1]).run()
#    jobs.clean()

#    config['LOG_DIRECTORY'] = 'LOGS/'
if __name__ == "__main__":
    main()
