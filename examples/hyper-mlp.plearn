#!plearn
#
# arguments: wd nh nh2 
#
PTester(
# string: Path of this experiment's directory in which to save all experiment results (will be created if it does not already exist)
expdir = "mlp-wd=${wd}-nh=${nh}-nh2=${nh2}";

$DEFINE{trainset}{SubVMatrix(parent= *99->AutoVMatrix(specification="letters all normalize"); length=1000)}

$DEFINE{validset}{SubVMatrix(parent= *99; istart=1000; length=1000)}

$DEFINE{testset}{SubVMatrix(parent= *99; istart=2000; length=1000)}

# provide this PTester'S learner with an expdir, to save its stuff
provide_learner_expdir = 1;

# actually this dataset is not used, only those from the ExplicitSplitter are used.
dataset = *0;

# PP< Splitter >: The splitter to use to generate one or several train/test pairs.
splitter = ExplicitSplitter( splitsets = 1 4 [ *1->${trainset}, *2->${validset}, *3->${testset}, *1 ] );

statnames = ["E[E[test1.NLL]]", "E[STDERROR[test1.NLL]]", "E[E[test2.NLL]]", "E[STDERROR[test2.NLL]]","E[E[test3.NLL]]", "E[STDERROR[test3.NLL]]"];

save_test_outputs = 1;

# PP< PLearner >: The learner to train/test
learner =
 HyperLearner( 
  learner =
   NNet(
          verbosity=1

          # int: dimensionality of output
          noutputs = 26;  
          # int: number of hidden units in first hidden layer (0 means no hidden layer)
          nhidden = ${nh}; 
          nhidden2 = ${nh2}; 

          # bool:     should we include direct input to output connections? (there is already a direct connection 
          # when nhidden = 0 and nhidden2 = 0)
          direct_in_to_out = 0;

          # double: global weight decay for all layers
          weight_decay = ${wd};

          # string: what transfer function to use for ouput layer?
          # one of: tanh, sigmoid, exp, softmax
          # an empty string means no output transfer function
          output_transfer_func = "softmax"  ;

          # Array< string >:     a list of cost functions to use
          # in the form "[ cf1; cf2; cf3; ... ]" where each function is one of:
          #   mse (for regression)
          #   mse_onehot (for classification)
          #   NLL (negative log likelihood -log(p[c]) for classification)
          #   class_error (classification error)
          # The first function of the list will be used as
          # the objective function to optimize
          # (possibly with an added weight decay penalty)
          cost_funcs = [ "NLL"];

          # PP< Optimizer >:     specify the optimizer to use
          optimizer = GradientOptimizer(
                      # double: the initial learning rate
                      start_learning_rate = 1e-3;

                      # double: the learning rate decrease constant
                      decrease_constant = 1e-7;
                      )

          # int: how many samples to use to estimate the avergage gradient before updating the weights
          # 0 is equivalent to specifying training_set->length()
          # NOTE: this overrides the optimizer s 'n_updates' and 'every_iterations'.
          batch_size = 1;

          # long: The initial seed for the random number generator used to initialize this learner's parameters
          # as typically done in the forget() method...
          # With a given seed, forget() should always initialize the parameters to the same values.
          seed = 0  ;

          # int: how many times the optimizer gets to see the whole training set.
          nstages = 100;
          )

 tester = PTester(
   # PP< Splitter >: The splitter to use to generate one or several train/test pairs.
   splitter = 
     ExplicitSplitter( splitsets = 1 2 [ *1, *2 ] );

   statnames = ["E[E[train.NLL]]","E[E[test.NLL]]"];

   save_test_outputs = 0;

   )  # end of tester

 # TVec< string >: learner option names to be reported in results table
 option_fields = ["optimizer.start_learning_rate", "optimizer.decrease_constant", "nstages"];

 # TVec< string >: a list of names of options, used in the strategy, but that do not require 
 # calling build() and forget() each time they are modified. 
 # Note that this is almost always the case of the 'nstages' option 
 # typically modified incrementally in an 'early stopping' fashion... 
 dont_restart_upon_change = ["nstages"]

 # bool: should each strategy step be provided a directory expdir/Step# to report its results
 provide_strategy_expdir = 1

 strategy = [ HyperOptimize(
              # int: an index in the tester's statnames to be used as the objective cost to minimize
              which_cost = 1; # test.NLL

              # bool: should the tester be provided with an expdir for each option combinatin to test
              provide_tester_expdir = 1  ;

              # PP< OptionsOracle >: Oracle giving trials to explore.
              oracle = CartesianProductOracle( 
                option_names = ["optimizer.start_learning_rate","optimizer.decrease_constant"]; 
                option_values = [ [ 3e-5 1e-5 ] [ 1e-7 1e-8] ];
              );  
              # bool: If this is true, a new evaluation will be performed after executing the sub-strategy,
              # using this HyperOptimizer's splitter and which_cost
              # This is useful if the sub_strategy optimizes a different cost, or uses different splitting.
              rerun_after_sub = 0  ;

              # bool: should sub_strategy commands be provided an expdir
              provide_sub_expdir = 0  ;

              # TVec< PP< HyperCommand > >: Optional sub-strategy to optimize other hyper-params (for each combination given by the oracle)
              sub_strategy = [ HyperOptimize(
                               which_cost = 1; # test.NLL
                               provide_tester_expdir = 0 ;
                               oracle = EarlyStoppingOracle(
                                   # string: the name of the option to change
                                   option = "nstages"

                                   # TVec< double >: a numerical range of the form [ start, end ] or [ start, end, step ]
                                   range = [1 20 2]  # [start-nepochs max-nepochs delta-nepochs]

                                   # double: minimum allowed error beyond which we stop
                                   min_value = -3.40282e+38  ;

                                   # double: maximum allowed error beyond which we stop
                                   max_value = 3.40282e+38  ;

                                   # double: maximum allowed degradation from last best objective value
                                   max_degradation = 3.40282e+38  ;

                                   # double: maximum allowed degradation from last best objective, relative to abs(best_objective)
                                   # ex: 0.10 will allow a degradation of 10% the magnitude of best_objective
                                   # Will be ignored if negative
                                   relative_max_degradation = -1  ;

                                   # double: minimum required improvement from previous objective value
                                   min_improvement = -3.40282e+38  ;

                                   # double: minimum required improvement from previous objective value, relative to it.
                                   # ex: 0.01 means we need an improvement of 0.01*abs(previous_objective)  i.e. 1%
                                   # Will be ignored if negative
                                   relative_min_improvement = -1  ;

                                   # int:     ax. nb of steps beyond best found
                                   max_degraded_steps = 10  ; ) # end of oracle
                                 ) # end of HyperOptimize
                               ] # end of sub_strategy

            ) # end of HyperOptimize 
           ] # end of strategy

 # bool: should final learner be saved in expdir/final_learner.psave
 save_final_learner = 1  ;

 # bool: should progress in learning and testing be reported in a ProgressBar.
 report_progress = 1  ;

 # int: Level of verbosity. If 0 should not write anything on cerr. 
 # If >0 may write some info on the steps performed along the way.
 #  The level of details written should depend on this value.
 verbosity = 1  ;

 ) # end of HyperLearner

)

