#!plearn

PTester( 
# string: Path of this experiment's directory in which to save all experiment results (will be created if it does not already exist)
expdir = "conj_mlp";

# VMat: The dataset to use for training/testing (will be split according to what is specified in the testmethod)
dataset = AutoVMatrix(specification="letters all normalize");

# PP< Splitter >: The splitter to use to generate one or several train/test pairs.
splitter = TrainTestSplitter(test_fraction=.10) ;

# PP< Learner >: The learner to train/test
learner = 
NNet( 

          verbosity = 3;

          # int: dimensionality of output 
          # (for classification the output is typically a vector of class scores
          # so this is the number of classes)
          noutputs = 26;  # 26 letters

          # int: number of hidden units in first hidden layer (0 means no hidden layer)
          nhidden = 20;

          # double: global weight decay for all layers
          weight_decay = 0;

          # string: what transfer function to use for ouput layer? 
          # one of: tanh, sigmoid, exp, softmax 
          # an empty string means no output transfer function 
          output_transfer_func = "softmax"  ;

          # Array< string >:     a list of cost functions to use
          # in the form "[ cf1; cf2; cf3; ... ]" where each function is one of: 
          #   mse (for regression)
          #   mse_onehot (for classification)
          #   NLL (negative log likelihood -log(p[c]) for classification) 
          #   class_error (classification error) 
          # The first function of the list will be used as 
          # the objective function to optimize 
          # (possibly with an added weight decay penalty) 
          cost_funcs = [ "NLL", "class_error" ];

          # PP< Optimizer >:     specify the optimizer to use
          optimizer = ConjGradientOptimizer(
                      starting_step_size = 1  ;
                      epsilon = 0.00001  ;
                      line_search_algo = 1  ;
                      find_new_direction_formula = 1  ;
                      sigma = 0.05  ;
                      rho = 0.01  ;
                      fmax = 0  ;
                      stop_epsilon = 0.0001;
                      tau1 = 2  ;
                      restart_coeff = 0.2  ;
                      )

          # int: how many samples to use to estimate the avergage gradient before updating the weights
          # 0 is equivalent to specifying training_set->length() 
          # NOTE: this overrides the optimizer s 'n_updates' and 'every_iterations'.
          batch_size = 0;

          # int: how many times the optimizer gets to see the whole training set.
          nstages = 30;

          # string:    The directory in which to save results 
          expdir = ""  ;

          )
)


