# Train a Neural Network on two predefined datasets (subsets of the UCI Letters dataset).

$DEFINE{expdir}{expdir-dbsplitter+mlp}

$DEFINE{statmask}{[ "NLL*" "class_error*" ] [ "E[*]" ] [ "test1.*" "test2.*" ] [ "E[*]" "STDERROR[*]" ] }

ShellScript( 
  # TVec< string >: The commands to be executed at run time.
  shell_commands = [
    "rm -rf ${expdir}"
  ]
);

PTester( 
  # string: Path of this tester's directory in which to save all tester results.
  # The directory will be created if it does not already exist.
  # If this is an empty string, no directory is created and no output file is generated.
  expdir = "${expdir}"  ;
  # PP< Splitter >: The splitter to use to generate one or several train/test tuples from the dataset.
  splitter =
    DBSplitter( 
      # TVec< string >: Vector with the specifications of the databases to be used.
      databases = [
        "letters train normalize" "letters test normalize"
      ]  ;
    );
  # TVec< string >: A list of global statistics we are interested in.
  # These are strings of the form S1[S2[dataset.cost_name]] where:
  #   - dataset is train or test1 or test2 ... (train being 
  #     the first dataset in a split, test1 the second, ...) 
  #   - cost_name is one of the training or test cost names (depending on dataset) understood 
  #     by the underlying learner (see its getTrainCostNames and getTestCostNames methods) 
  #   - S1 and S2 are a statistic, i.e. one of: E (expectation), V(variance), MIN, MAX, STDDEV, ... 
  #     S2 is computed over the samples of a given dataset split. S1 is over the splits. 
  statnames = [ "" ]
  # TVec< TVec< string > >: A list of lists of masks. If provided, each of the lists is used to compose the statnames_from_mask.
  # If not provided the statnames are those in the 'statnames' list. 
  # For example if statmask = [ ["E[train.*]" "E[test1.*]"] ] and statnames = [ "E[func1]" "E[func2]" 
  # then the resulting statnames list will be:
  # [ "E[train.E[func1]]", "E[train.E[func2]]", "E[test1.E[func1]]", "E[test1.E[func2]]" ].
  statmask = [
    ${statmask} [ "E[train.E[*]]" ]
  ]
  # PP< PLearner >: The learner to train/test.
  learner =
    TestingLearner( 
      # PP< PTester >: The tester used by the TestingLearner.
      tester =
        PTester( 
          # PP< Splitter >: The splitter to use to generate one or several train/test tuples from the dataset.
          splitter =
            RepeatSplitter( 
              # int: How many times we want to repeat.
              n = 3  ;
              # int: If set to 1, the dataset will be shuffled differently at each repetition.
              shuffle = 1  ;
              # PP< Splitter >: The splitter we want to repeat.
              to_repeat =
                TrainTestSplitter( 
                  # int: if set to 1, the trainset will be appended after the test set (thus each split will contain three sets)
                  append_train = 1  ;
                  # double: the fraction of the dataset reserved to the test set
                  test_fraction = 0.33  ;
                );
            );
          # TVec< string >: A list of global statistics we are interested in.
          # These are strings of the form S1[S2[dataset.cost_name]] where:
          #   - dataset is train or test1 or test2 ... (train being 
          #     the first dataset in a split, test1 the second, ...) 
          #   - cost_name is one of the training or test cost names (depending on dataset) understood 
          #     by the underlying learner (see its getTrainCostNames and getTestCostNames methods) 
          #   - S1 and S2 are a statistic, i.e. one of: E (expectation), V(variance), MIN, MAX, STDDEV, ... 
          #     S2 is computed over the samples of a given dataset split. S1 is over the splits. 
          statnames = [ "" ]
          # TVec< TVec< string > >: A list of lists of masks. If provided, each of the lists is used to compose the statnames_from_mask.
          # If not provided the statnames are those in the 'statnames' list. 
          # For example if statmask = [ ["E[train.*]" "E[test1.*]"] ] and statnames = [ "E[func1]" "E[func2]" 
          # then the resulting statnames list will be:
          # [ "E[train.E[func1]]", "E[train.E[func2]]", "E[test1.E[func1]]", "E[test1.E[func2]]" ].
          statmask = [ ${statmask} ]
          # PP< PLearner >: The learner to train/test.
          learner =
            NNet( 
              # int:     number of hidden units in first hidden layer (0 means no hidden layer)
              nhidden = 3  ;
              # int:     number of output units. This gives this learner its outputsize.
              #     It is typically of the same dimensionality as the target for regression problems 
              #     But for classification problems where target is just the class number, noutputs is 
              #     usually of dimensionality number of classes (as we want to output a score or probability 
              #     vector, one per class)
              noutputs = 26  ;
              # string:     what transfer function to use for ouput layer? 
              #     one of: tanh, sigmoid, exp, softplus, softmax, log_softmax 
              #     or interval(<minval>,<maxval>), which stands for
              #     <minval>+(<maxval>-<minval>)*sigmoid(.).
              #     An empty string or "none" means no output transfer function 
              output_transfer_func = "softmax"  ;
              # Array< string >:     a list of cost functions to use
              #     in the form "[ cf1; cf2; cf3; ... ]" where each function is one of: 
              #       mse (for regression)
              #       mse_onehot (for classification)
              #       NLL (negative log likelihood -log(p[c]) for classification) 
              #       class_error (classification error) 
              #       binary_class_error (classification error for a 0-1 binary classifier)
              #       multiclass_error
              #       cross_entropy (for binary classification)
              #       stable_cross_entropy (more accurate backprop and possible regularization, for binary classification)
              #       margin_perceptron_cost (a hard version of the cross_entropy, uses the 'margin' option)
              #       lift_output (not a real cost function, just the output for lift computation)
              #     The first function of the list will be used as 
              #     the objective function to optimize 
              #     (possibly with an added weight decay penalty) 
              cost_funcs =  [ "NLL" "class_error" ]  ;
              # PP< Optimizer >:     specify the optimizer to use
              optimizer =
                GradientOptimizer( 
                  # double:     the initial learning rate
                  start_learning_rate = 1e-3
                );
              # int:     how many samples to use to estimate the avergage gradient before updating the weights
              #     0 is equivalent to specifying training_set->length() 
              batch_size = 1  ;
              # int: The stage until which train() should train this learner and return.
              # The meaning of 'stage' is learner-dependent, but for learners whose 
              # training is incremental (such as involving incremental optimization), 
              # it is typically synonym with the number of 'epochs', i.e. the number 
              # of passages of the optimization process through the whole training set, 
              # since the last fresh initialisation.
              nstages = 10  ;
            );
          # bool: If true, each learner to be trained will have its experiment directory set to Split#k/LearnerExpdir/
          provide_learner_expdir = 1  ;
          # bool: If true, the learners are trained, otherwise only tested (in that case it is advised
          # to load an already trained learner in the 'learner' field)
          train = 1  ;
        );
    )
  # bool: If true, the outputs of the test for split #k will be saved in Split#k/test#i_outputs.pmat
  save_test_outputs = 0  ;
  # bool: If true, each learner to be trained will have its experiment directory set to Split#k/LearnerExpdir/
  provide_learner_expdir = 1  ;
  # bool: If true, the learners are trained, otherwise only tested (in that case it is advised
  # to load an already trained learner in the 'learner' field)
  train = 1  ;
);
