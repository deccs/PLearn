$DEFINE{DENSITY_ESTIMATOR}
{ LocallyMagnifiedDistribution( 
    # int: Output computation mode
    mode = 0;

    # int: If <=0 we use all training points (with an appropriate weight).
    # If >0 we consider only that many neighbors of the test point;
    # (it's equivalent to giving all other data points a weight of 0)
    nneighbors = -1;

    # bool: If true, then we ignore the weighting_kernel, and ascribe weight 1/k to the neighbors
    # where k is the index of the neighbor from 1 to nneighbors
    use_rank_weighting = 0;

    # Ker: The magnifying kernel that will be used to locally weigh the samples
    # If it is left null, and use_rank_weighting is false, then all
    # nneighbors will receive a weight of 1
    weighting_kernel = GaussianKernel(sigma=0.3) ;

    # int: If width_neighbor>0, kernel width is set to be the distance to the width_neighbor'th neighbor times the width_factor (Euclidean distance neighbors for now)
    # If width_neighbor==0, kernel is left as specified upon construction
    width_neighbor = 0;

    # double: Only used if width_neighbor>0 (see width_neighbor)
    width_factor = 1;

    # string: Only used if width_neighbor>0. The name of the option in the weighting kernel that should be used to set or modifiy its width
    width_optionname = "sigma"  ;

    # PP< PDistribution >: The distribution that will be trained with local weights obtained from the magnifying kernel
    localdistr = GaussianDistribution(k=1, gamma=1e-4, use_last_eig=0);

    # string: Defines what will be given in output. This is a string where the
    # characters have the following meaning:
    # - 'l' : log_density
    # - 'd' : density
    # - 'c' : cdf
    # - 's' : survival_fn
    # - 'e' : expectation
    # - 'v' : variance.
    # 
    # If these options are specified in lower case they give the value
    # associated with a given observation. In upper case, a curve is
    # evaluated at regular intervals and produced in output (as a
    # histogram). For 'L', 'D', 'C', 'S', it is the predicted part that
    # varies, while for 'E' and 'V' it is the predictor part (for
    # conditional distributions).
    # The number of curve points is given by the 'n_curve_points' option.
    # Note that the upper case letters only work for scalar variables, in
    # order to produce a one-dimensional curve.
    outputs_def = "d"  ;
    );  
}

ClassifierFromDensity( 
  # int: The number of classes
  nclasses = 2;

  # TVec< PP< PDistribution > >: The array of density estimators, one for each class.
  # You may also specify just one that will be replicated as many times as there are classes.
  estimators = [ ${DENSITY_ESTIMATOR}, ${DENSITY_ESTIMATOR} ]

  # bool: Whether computeOutput yields log-probabilities or probabilities (of classes given inputs)
  output_log_probabilities = 0  ;

  # use_these_priors = [ 0.25, 0.75 ];

  # bool: Whether to normalize the probabilities (if not just compute likelihood * prior for each class)
  normalize_probabilities = 1  ;

  );

